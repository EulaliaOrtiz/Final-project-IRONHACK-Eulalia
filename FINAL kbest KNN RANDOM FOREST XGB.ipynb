{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f53b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Javascript\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e65af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bec8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b74375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88bed74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>Continuous interest rate (after tax)</th>\n",
       "      <th>Operating Expense Rate</th>\n",
       "      <th>Research and development expense rate</th>\n",
       "      <th>Cash flow rate</th>\n",
       "      <th>Interest-bearing debt interest rate</th>\n",
       "      <th>Tax rate (A)</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Cash Flow Per Share</th>\n",
       "      <th>Revenue Per Share (Yuan ¥)</th>\n",
       "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
       "      <th>Operating Profit Growth Rate</th>\n",
       "      <th>After-tax Net Profit Growth Rate</th>\n",
       "      <th>Regular Net Profit Growth Rate</th>\n",
       "      <th>Continuous Net Profit Growth Rate</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Net Value Growth Rate</th>\n",
       "      <th>Total Asset Return Growth Rate Ratio</th>\n",
       "      <th>Cash Reinvestment %</th>\n",
       "      <th>Current Ratio</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Interest Expense Ratio</th>\n",
       "      <th>Total debt/Total net worth</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Contingent liabilities/Net worth</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Inventory and accounts receivable/Net value</th>\n",
       "      <th>Total Asset Turnover</th>\n",
       "      <th>Accounts Receivable Turnover</th>\n",
       "      <th>Average Collection Days</th>\n",
       "      <th>Inventory Turnover Rate (times)</th>\n",
       "      <th>Fixed Assets Turnover Frequency</th>\n",
       "      <th>Net Worth Turnover Rate (times)</th>\n",
       "      <th>Revenue per person</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Allocation rate per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Current Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Quick Assets/Current Liability</th>\n",
       "      <th>Cash/Current Liability</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Inventory/Working Capital</th>\n",
       "      <th>Inventory/Current Liability</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Working Capital/Equity</th>\n",
       "      <th>Current Liabilities/Equity</th>\n",
       "      <th>Long-term Liability to Current Assets</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total income/Total expense</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Current Asset Turnover Rate</th>\n",
       "      <th>Quick Asset Turnover Rate</th>\n",
       "      <th>Working capitcal Turnover Rate</th>\n",
       "      <th>Cash Turnover Rate</th>\n",
       "      <th>Cash Flow to Sales</th>\n",
       "      <th>Fixed Assets to Assets</th>\n",
       "      <th>Current Liability to Liability</th>\n",
       "      <th>Current Liability to Equity</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>Cash Flow to Total Assets</th>\n",
       "      <th>Cash Flow to Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Cash Flow to Equity</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Liability-Assets Flag</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>0.780985</td>\n",
       "      <td>1.256969e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>7.250725e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.169141</td>\n",
       "      <td>0.311664</td>\n",
       "      <td>0.017560</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.138736</td>\n",
       "      <td>0.022102</td>\n",
       "      <td>0.848195</td>\n",
       "      <td>0.688979</td>\n",
       "      <td>0.688979</td>\n",
       "      <td>0.217535</td>\n",
       "      <td>4.980000e+09</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.363725</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.629951</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.207576</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.390284</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.095885</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>0.398036</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>1.820926e-04</td>\n",
       "      <td>1.165007e-04</td>\n",
       "      <td>0.032903</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.672775</td>\n",
       "      <td>0.166673</td>\n",
       "      <td>0.190643</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>1.473360e-04</td>\n",
       "      <td>0.147308</td>\n",
       "      <td>0.334015</td>\n",
       "      <td>0.276920</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.676269</td>\n",
       "      <td>0.721275</td>\n",
       "      <td>0.339077</td>\n",
       "      <td>2.559237e-02</td>\n",
       "      <td>0.903225</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>7.010000e+08</td>\n",
       "      <td>6.550000e+09</td>\n",
       "      <td>0.593831</td>\n",
       "      <td>4.580000e+08</td>\n",
       "      <td>0.671568</td>\n",
       "      <td>0.424206</td>\n",
       "      <td>0.676269</td>\n",
       "      <td>0.339077</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>0.637555</td>\n",
       "      <td>0.458609</td>\n",
       "      <td>0.520382</td>\n",
       "      <td>0.312905</td>\n",
       "      <td>0.118250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>0.781506</td>\n",
       "      <td>2.897851e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.461867</td>\n",
       "      <td>6.470647e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.318137</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>0.093722</td>\n",
       "      <td>0.169918</td>\n",
       "      <td>0.022080</td>\n",
       "      <td>0.848088</td>\n",
       "      <td>0.689693</td>\n",
       "      <td>0.689702</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>6.110000e+09</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.376709</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.635172</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.828824</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.093743</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.397725</td>\n",
       "      <td>0.064468</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>9.360000e+09</td>\n",
       "      <td>7.190000e+08</td>\n",
       "      <td>0.025484</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.391590</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.127236</td>\n",
       "      <td>0.182419</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>1.383910e-03</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>0.341106</td>\n",
       "      <td>0.289642</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.731975</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>2.394682e-02</td>\n",
       "      <td>0.931065</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>1.065198e-04</td>\n",
       "      <td>7.700000e+09</td>\n",
       "      <td>0.593916</td>\n",
       "      <td>2.490000e+09</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.468828</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.120916</td>\n",
       "      <td>0.641100</td>\n",
       "      <td>0.459001</td>\n",
       "      <td>0.567101</td>\n",
       "      <td>0.314163</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>0.780284</td>\n",
       "      <td>2.361297e-04</td>\n",
       "      <td>2.550000e+07</td>\n",
       "      <td>0.458521</td>\n",
       "      <td>7.900790e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.193713</td>\n",
       "      <td>0.180581</td>\n",
       "      <td>0.307102</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.092338</td>\n",
       "      <td>0.142803</td>\n",
       "      <td>0.022760</td>\n",
       "      <td>0.848094</td>\n",
       "      <td>0.689463</td>\n",
       "      <td>0.689470</td>\n",
       "      <td>0.217601</td>\n",
       "      <td>7.280000e+09</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.264184</td>\n",
       "      <td>0.368913</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.629631</td>\n",
       "      <td>0.021248</td>\n",
       "      <td>0.207516</td>\n",
       "      <td>0.792484</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.379093</td>\n",
       "      <td>0.006562</td>\n",
       "      <td>0.092318</td>\n",
       "      <td>0.148036</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>6.500000e+07</td>\n",
       "      <td>2.650000e+09</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.381968</td>\n",
       "      <td>0.141016</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.340201</td>\n",
       "      <td>0.602806</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.006302</td>\n",
       "      <td>5.340000e+09</td>\n",
       "      <td>0.098162</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.277456</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>0.446027</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.334777</td>\n",
       "      <td>3.715116e-03</td>\n",
       "      <td>0.909903</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.021387</td>\n",
       "      <td>1.791094e-03</td>\n",
       "      <td>1.022676e-03</td>\n",
       "      <td>0.594502</td>\n",
       "      <td>7.610000e+08</td>\n",
       "      <td>0.671571</td>\n",
       "      <td>0.276179</td>\n",
       "      <td>0.446027</td>\n",
       "      <td>0.334777</td>\n",
       "      <td>0.117922</td>\n",
       "      <td>0.642765</td>\n",
       "      <td>0.459254</td>\n",
       "      <td>0.538491</td>\n",
       "      <td>0.314515</td>\n",
       "      <td>0.025346</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.781241</td>\n",
       "      <td>1.078888e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.465705</td>\n",
       "      <td>4.490449e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.321674</td>\n",
       "      <td>0.014368</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.148603</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.217568</td>\n",
       "      <td>4.880000e+09</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.263371</td>\n",
       "      <td>0.384077</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.630228</td>\n",
       "      <td>0.009572</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.397925</td>\n",
       "      <td>0.089955</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>7.130000e+09</td>\n",
       "      <td>9.150000e+09</td>\n",
       "      <td>0.028065</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>0.378497</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.725754</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.225815</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>1.010646e-03</td>\n",
       "      <td>0.098715</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.276580</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.729825</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>2.216520e-02</td>\n",
       "      <td>0.906902</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>8.140000e+09</td>\n",
       "      <td>6.050000e+09</td>\n",
       "      <td>0.593889</td>\n",
       "      <td>2.030000e+09</td>\n",
       "      <td>0.671519</td>\n",
       "      <td>0.559144</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>0.120760</td>\n",
       "      <td>0.579039</td>\n",
       "      <td>0.448518</td>\n",
       "      <td>0.604105</td>\n",
       "      <td>0.302382</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>0.781550</td>\n",
       "      <td>7.890000e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.462746</td>\n",
       "      <td>6.860686e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.319162</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.096898</td>\n",
       "      <td>0.168412</td>\n",
       "      <td>0.022096</td>\n",
       "      <td>0.848258</td>\n",
       "      <td>0.689697</td>\n",
       "      <td>0.689697</td>\n",
       "      <td>0.217626</td>\n",
       "      <td>5.510000e+09</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.265218</td>\n",
       "      <td>0.379690</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.636055</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>0.375025</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>0.096927</td>\n",
       "      <td>0.167461</td>\n",
       "      <td>0.400079</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>1.633674e-04</td>\n",
       "      <td>2.935211e-04</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.058111</td>\n",
       "      <td>0.394371</td>\n",
       "      <td>0.023988</td>\n",
       "      <td>0.751822</td>\n",
       "      <td>0.260330</td>\n",
       "      <td>0.358380</td>\n",
       "      <td>0.014161</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>6.804636e-04</td>\n",
       "      <td>0.110195</td>\n",
       "      <td>0.344639</td>\n",
       "      <td>0.287913</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.330726</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.913850</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>6.680000e+09</td>\n",
       "      <td>5.050000e+09</td>\n",
       "      <td>0.593915</td>\n",
       "      <td>8.240000e+08</td>\n",
       "      <td>0.671563</td>\n",
       "      <td>0.309555</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.330726</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.622374</td>\n",
       "      <td>0.454411</td>\n",
       "      <td>0.578469</td>\n",
       "      <td>0.311567</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.539468</td>\n",
       "      <td>0.543230</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.604462</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797409</td>\n",
       "      <td>0.809331</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>1.510213e-04</td>\n",
       "      <td>4.500000e+09</td>\n",
       "      <td>0.463734</td>\n",
       "      <td>1.790179e-04</td>\n",
       "      <td>0.113372</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.216602</td>\n",
       "      <td>0.320966</td>\n",
       "      <td>0.020766</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.172102</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.848205</td>\n",
       "      <td>0.689778</td>\n",
       "      <td>0.689778</td>\n",
       "      <td>0.217635</td>\n",
       "      <td>7.070000e+09</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.264517</td>\n",
       "      <td>0.380155</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.631415</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.124618</td>\n",
       "      <td>0.875382</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.373823</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.098222</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.404804</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>4.030000e+07</td>\n",
       "      <td>1.429781e-04</td>\n",
       "      <td>0.027903</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.392596</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>0.817769</td>\n",
       "      <td>0.312840</td>\n",
       "      <td>0.578455</td>\n",
       "      <td>0.099481</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>5.071548e-03</td>\n",
       "      <td>0.103838</td>\n",
       "      <td>0.346224</td>\n",
       "      <td>0.277543</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.786888</td>\n",
       "      <td>0.736716</td>\n",
       "      <td>0.330914</td>\n",
       "      <td>1.792237e-03</td>\n",
       "      <td>0.925611</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>2.294154e-04</td>\n",
       "      <td>1.244230e-04</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>1.077940e-04</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.400338</td>\n",
       "      <td>0.786888</td>\n",
       "      <td>0.330914</td>\n",
       "      <td>0.112622</td>\n",
       "      <td>0.639806</td>\n",
       "      <td>0.458639</td>\n",
       "      <td>0.587178</td>\n",
       "      <td>0.314063</td>\n",
       "      <td>0.027951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.623620</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.279606</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.566193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0</td>\n",
       "      <td>0.475162</td>\n",
       "      <td>0.538269</td>\n",
       "      <td>0.524172</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.809327</td>\n",
       "      <td>0.303520</td>\n",
       "      <td>0.781586</td>\n",
       "      <td>5.220000e+09</td>\n",
       "      <td>1.440000e+09</td>\n",
       "      <td>0.461978</td>\n",
       "      <td>2.370237e-04</td>\n",
       "      <td>0.371596</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.216697</td>\n",
       "      <td>0.318278</td>\n",
       "      <td>0.023050</td>\n",
       "      <td>0.098608</td>\n",
       "      <td>0.172780</td>\n",
       "      <td>0.022159</td>\n",
       "      <td>0.848245</td>\n",
       "      <td>0.689734</td>\n",
       "      <td>0.689734</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>5.220000e+09</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.264730</td>\n",
       "      <td>0.377389</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>0.631489</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>0.900747</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.372505</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.098572</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.399926</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>9.940000e+09</td>\n",
       "      <td>6.051982e-04</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.016083</td>\n",
       "      <td>0.393625</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.793387</td>\n",
       "      <td>0.335085</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.080337</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>4.727181e-03</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>0.342166</td>\n",
       "      <td>0.277368</td>\n",
       "      <td>0.006730</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.734584</td>\n",
       "      <td>0.329753</td>\n",
       "      <td>2.204673e-03</td>\n",
       "      <td>0.932629</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>1.517299e-04</td>\n",
       "      <td>1.173396e-04</td>\n",
       "      <td>0.593954</td>\n",
       "      <td>7.710000e+09</td>\n",
       "      <td>0.671572</td>\n",
       "      <td>0.096136</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.329753</td>\n",
       "      <td>0.112329</td>\n",
       "      <td>0.642072</td>\n",
       "      <td>0.459058</td>\n",
       "      <td>0.569498</td>\n",
       "      <td>0.314446</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.566018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0</td>\n",
       "      <td>0.472725</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.520638</td>\n",
       "      <td>0.610444</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>0.781546</td>\n",
       "      <td>2.509312e-04</td>\n",
       "      <td>1.039086e-04</td>\n",
       "      <td>0.472189</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.490839</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.210929</td>\n",
       "      <td>0.324857</td>\n",
       "      <td>0.044255</td>\n",
       "      <td>0.100073</td>\n",
       "      <td>0.173232</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.847978</td>\n",
       "      <td>0.689202</td>\n",
       "      <td>0.689202</td>\n",
       "      <td>0.217547</td>\n",
       "      <td>5.990000e+09</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.263858</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.038424</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>0.630612</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.961061</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.100103</td>\n",
       "      <td>0.172287</td>\n",
       "      <td>0.395592</td>\n",
       "      <td>0.106447</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>2.797309e-04</td>\n",
       "      <td>1.024298e-03</td>\n",
       "      <td>0.022419</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.393693</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.866047</td>\n",
       "      <td>0.476747</td>\n",
       "      <td>0.496053</td>\n",
       "      <td>0.412885</td>\n",
       "      <td>0.035531</td>\n",
       "      <td>8.821248e-02</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.358847</td>\n",
       "      <td>0.277022</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.553964</td>\n",
       "      <td>0.737432</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.035446</td>\n",
       "      <td>1.762272e-04</td>\n",
       "      <td>1.749713e-04</td>\n",
       "      <td>0.594025</td>\n",
       "      <td>4.074263e-04</td>\n",
       "      <td>0.671564</td>\n",
       "      <td>0.055509</td>\n",
       "      <td>0.553964</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.631678</td>\n",
       "      <td>0.452465</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.313353</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0</td>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.610441</td>\n",
       "      <td>0.840138</td>\n",
       "      <td>0.275789</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0</td>\n",
       "      <td>0.506264</td>\n",
       "      <td>0.559911</td>\n",
       "      <td>0.554045</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.809399</td>\n",
       "      <td>0.303498</td>\n",
       "      <td>0.781663</td>\n",
       "      <td>1.236154e-04</td>\n",
       "      <td>2.510000e+09</td>\n",
       "      <td>0.476123</td>\n",
       "      <td>2.110211e-04</td>\n",
       "      <td>0.181294</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.228326</td>\n",
       "      <td>0.346573</td>\n",
       "      <td>0.031535</td>\n",
       "      <td>0.111799</td>\n",
       "      <td>0.185584</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>0.854064</td>\n",
       "      <td>0.696113</td>\n",
       "      <td>0.696113</td>\n",
       "      <td>0.218006</td>\n",
       "      <td>7.250000e+09</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.264409</td>\n",
       "      <td>0.401028</td>\n",
       "      <td>0.012782</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.630731</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.086979</td>\n",
       "      <td>0.913021</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.369649</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.111722</td>\n",
       "      <td>0.182498</td>\n",
       "      <td>0.401540</td>\n",
       "      <td>0.109445</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>4.550000e+09</td>\n",
       "      <td>2.330013e-04</td>\n",
       "      <td>0.027258</td>\n",
       "      <td>0.012749</td>\n",
       "      <td>0.396735</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.832340</td>\n",
       "      <td>0.353624</td>\n",
       "      <td>0.564439</td>\n",
       "      <td>0.112238</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>7.133218e-03</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>0.380251</td>\n",
       "      <td>0.277353</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.893241</td>\n",
       "      <td>0.736713</td>\n",
       "      <td>0.329294</td>\n",
       "      <td>3.200000e+09</td>\n",
       "      <td>0.939613</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>2.135940e-04</td>\n",
       "      <td>1.351937e-04</td>\n",
       "      <td>0.593997</td>\n",
       "      <td>1.165392e-04</td>\n",
       "      <td>0.671606</td>\n",
       "      <td>0.246805</td>\n",
       "      <td>0.893241</td>\n",
       "      <td>0.329294</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.684857</td>\n",
       "      <td>0.471313</td>\n",
       "      <td>0.678338</td>\n",
       "      <td>0.320118</td>\n",
       "      <td>0.022916</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.607846</td>\n",
       "      <td>0.841084</td>\n",
       "      <td>0.277547</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493053</td>\n",
       "      <td>0.570105</td>\n",
       "      <td>0.549548</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.998080</td>\n",
       "      <td>0.801987</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.786079</td>\n",
       "      <td>1.431695e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.427721</td>\n",
       "      <td>5.900000e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.227758</td>\n",
       "      <td>0.305793</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.092501</td>\n",
       "      <td>0.182119</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.848053</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.217605</td>\n",
       "      <td>9.350000e+09</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.264186</td>\n",
       "      <td>0.360102</td>\n",
       "      <td>0.051348</td>\n",
       "      <td>0.040897</td>\n",
       "      <td>0.630618</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.985851</td>\n",
       "      <td>0.058476</td>\n",
       "      <td>0.370049</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.092465</td>\n",
       "      <td>0.179911</td>\n",
       "      <td>0.393883</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>1.910000e+07</td>\n",
       "      <td>2.995731e-04</td>\n",
       "      <td>0.009194</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.385767</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.873759</td>\n",
       "      <td>0.527136</td>\n",
       "      <td>0.505010</td>\n",
       "      <td>0.238147</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>6.667354e-02</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.239585</td>\n",
       "      <td>0.276975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737286</td>\n",
       "      <td>0.326690</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.938005</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>7.863781e-03</td>\n",
       "      <td>8.238471e-03</td>\n",
       "      <td>0.598674</td>\n",
       "      <td>9.505992e-03</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.326690</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.659917</td>\n",
       "      <td>0.483285</td>\n",
       "      <td>0.505531</td>\n",
       "      <td>0.316238</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.626680</td>\n",
       "      <td>0.627408</td>\n",
       "      <td>0.841019</td>\n",
       "      <td>0.275114</td>\n",
       "      <td>0.026793</td>\n",
       "      <td>0.565167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.233902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "0             1                                           0.370594          \n",
       "1             1                                           0.464291          \n",
       "2             1                                           0.426071          \n",
       "3             1                                           0.399844          \n",
       "4             1                                           0.465022          \n",
       "...         ...                                                ...          \n",
       "6814          0                                           0.493687          \n",
       "6815          0                                           0.475162          \n",
       "6816          0                                           0.472725          \n",
       "6817          0                                           0.506264          \n",
       "6818          0                                           0.493053          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "0                                    0.424389   \n",
       "1                                    0.538214   \n",
       "2                                    0.499019   \n",
       "3                                    0.451265   \n",
       "4                                    0.538432   \n",
       "...                                       ...   \n",
       "6814                                 0.539468   \n",
       "6815                                 0.538269   \n",
       "6816                                 0.533744   \n",
       "6817                                 0.559911   \n",
       "6818                                 0.570105   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "0                                              0.405750    \n",
       "1                                              0.516730    \n",
       "2                                              0.472295    \n",
       "3                                              0.457733    \n",
       "4                                              0.522298    \n",
       "...                                                 ...    \n",
       "6814                                           0.543230    \n",
       "6815                                           0.524172    \n",
       "6816                                           0.520638    \n",
       "6817                                           0.554045    \n",
       "6818                                           0.549548    \n",
       "\n",
       "       Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "0                    0.601457                      0.601457   \n",
       "1                    0.610235                      0.610235   \n",
       "2                    0.601450                      0.601364   \n",
       "3                    0.583541                      0.583541   \n",
       "4                    0.598783                      0.598783   \n",
       "...                       ...                           ...   \n",
       "6814                 0.604455                      0.604462   \n",
       "6815                 0.598308                      0.598308   \n",
       "6816                 0.610444                      0.610213   \n",
       "6817                 0.607850                      0.607850   \n",
       "6818                 0.627409                      0.627409   \n",
       "\n",
       "       Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "0                   0.998969                    0.796887   \n",
       "1                   0.998946                    0.797380   \n",
       "2                   0.998857                    0.796403   \n",
       "3                   0.998700                    0.796967   \n",
       "4                   0.998973                    0.797366   \n",
       "...                      ...                         ...   \n",
       "6814                0.998992                    0.797409   \n",
       "6815                0.998992                    0.797414   \n",
       "6816                0.998984                    0.797401   \n",
       "6817                0.999074                    0.797500   \n",
       "6818                0.998080                    0.801987   \n",
       "\n",
       "       After-tax net Interest Rate  \\\n",
       "0                         0.808809   \n",
       "1                         0.809301   \n",
       "2                         0.808388   \n",
       "3                         0.808966   \n",
       "4                         0.809304   \n",
       "...                            ...   \n",
       "6814                      0.809331   \n",
       "6815                      0.809327   \n",
       "6816                      0.809317   \n",
       "6817                      0.809399   \n",
       "6818                      0.813800   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  \\\n",
       "0                                         0.302646   \n",
       "1                                         0.303556   \n",
       "2                                         0.302035   \n",
       "3                                         0.303350   \n",
       "4                                         0.303475   \n",
       "...                                            ...   \n",
       "6814                                      0.303510   \n",
       "6815                                      0.303520   \n",
       "6816                                      0.303512   \n",
       "6817                                      0.303498   \n",
       "6818                                      0.313415   \n",
       "\n",
       "       Continuous interest rate (after tax)   Operating Expense Rate  \\\n",
       "0                                  0.780985             1.256969e-04   \n",
       "1                                  0.781506             2.897851e-04   \n",
       "2                                  0.780284             2.361297e-04   \n",
       "3                                  0.781241             1.078888e-04   \n",
       "4                                  0.781550             7.890000e+09   \n",
       "...                                     ...                      ...   \n",
       "6814                               0.781588             1.510213e-04   \n",
       "6815                               0.781586             5.220000e+09   \n",
       "6816                               0.781546             2.509312e-04   \n",
       "6817                               0.781663             1.236154e-04   \n",
       "6818                               0.786079             1.431695e-03   \n",
       "\n",
       "       Research and development expense rate   Cash flow rate  \\\n",
       "0                               0.000000e+00         0.458143   \n",
       "1                               0.000000e+00         0.461867   \n",
       "2                               2.550000e+07         0.458521   \n",
       "3                               0.000000e+00         0.465705   \n",
       "4                               0.000000e+00         0.462746   \n",
       "...                                      ...              ...   \n",
       "6814                            4.500000e+09         0.463734   \n",
       "6815                            1.440000e+09         0.461978   \n",
       "6816                            1.039086e-04         0.472189   \n",
       "6817                            2.510000e+09         0.476123   \n",
       "6818                            0.000000e+00         0.427721   \n",
       "\n",
       "       Interest-bearing debt interest rate   Tax rate (A)  \\\n",
       "0                             7.250725e-04       0.000000   \n",
       "1                             6.470647e-04       0.000000   \n",
       "2                             7.900790e-04       0.000000   \n",
       "3                             4.490449e-04       0.000000   \n",
       "4                             6.860686e-04       0.000000   \n",
       "...                                    ...            ...   \n",
       "6814                          1.790179e-04       0.113372   \n",
       "6815                          2.370237e-04       0.371596   \n",
       "6816                          0.000000e+00       0.490839   \n",
       "6817                          2.110211e-04       0.181294   \n",
       "6818                          5.900000e+08       0.000000   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "0                     0.147950                  0.147950   \n",
       "1                     0.182251                  0.182251   \n",
       "2                     0.177911                  0.177911   \n",
       "3                     0.154187                  0.154187   \n",
       "4                     0.167502                  0.167502   \n",
       "...                        ...                       ...   \n",
       "6814                  0.175045                  0.175045   \n",
       "6815                  0.181324                  0.181324   \n",
       "6816                  0.269521                  0.269521   \n",
       "6817                  0.213392                  0.213392   \n",
       "6818                  0.220766                  0.220766   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "0                     0.147950                                  0.169141   \n",
       "1                     0.182251                                  0.208944   \n",
       "2                     0.193713                                  0.180581   \n",
       "3                     0.154187                                  0.193722   \n",
       "4                     0.167502                                  0.212537   \n",
       "...                        ...                                       ...   \n",
       "6814                  0.175045                                  0.216602   \n",
       "6815                  0.181324                                  0.216697   \n",
       "6816                  0.269521                                  0.210929   \n",
       "6817                  0.213392                                  0.228326   \n",
       "6818                  0.220766                                  0.227758   \n",
       "\n",
       "       Cash Flow Per Share   Revenue Per Share (Yuan ¥)  \\\n",
       "0                 0.311664                     0.017560   \n",
       "1                 0.318137                     0.021144   \n",
       "2                 0.307102                     0.005944   \n",
       "3                 0.321674                     0.014368   \n",
       "4                 0.319162                     0.029690   \n",
       "...                    ...                          ...   \n",
       "6814              0.320966                     0.020766   \n",
       "6815              0.318278                     0.023050   \n",
       "6816              0.324857                     0.044255   \n",
       "6817              0.346573                     0.031535   \n",
       "6818              0.305793                     0.000665   \n",
       "\n",
       "       Operating Profit Per Share (Yuan ¥)  \\\n",
       "0                                 0.095921   \n",
       "1                                 0.093722   \n",
       "2                                 0.092338   \n",
       "3                                 0.077762   \n",
       "4                                 0.096898   \n",
       "...                                    ...   \n",
       "6814                              0.098200   \n",
       "6815                              0.098608   \n",
       "6816                              0.100073   \n",
       "6817                              0.111799   \n",
       "6818                              0.092501   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)  \\\n",
       "0                                      0.138736   \n",
       "1                                      0.169918   \n",
       "2                                      0.142803   \n",
       "3                                      0.148603   \n",
       "4                                      0.168412   \n",
       "...                                         ...   \n",
       "6814                                   0.172102   \n",
       "6815                                   0.172780   \n",
       "6816                                   0.173232   \n",
       "6817                                   0.185584   \n",
       "6818                                   0.182119   \n",
       "\n",
       "       Realized Sales Gross Profit Growth Rate   Operating Profit Growth Rate  \\\n",
       "0                                     0.022102                       0.848195   \n",
       "1                                     0.022080                       0.848088   \n",
       "2                                     0.022760                       0.848094   \n",
       "3                                     0.022046                       0.848005   \n",
       "4                                     0.022096                       0.848258   \n",
       "...                                        ...                            ...   \n",
       "6814                                  0.022374                       0.848205   \n",
       "6815                                  0.022159                       0.848245   \n",
       "6816                                  0.022068                       0.847978   \n",
       "6817                                  0.022350                       0.854064   \n",
       "6818                                  0.025316                       0.848053   \n",
       "\n",
       "       After-tax Net Profit Growth Rate   Regular Net Profit Growth Rate  \\\n",
       "0                              0.688979                         0.688979   \n",
       "1                              0.689693                         0.689702   \n",
       "2                              0.689463                         0.689470   \n",
       "3                              0.689110                         0.689110   \n",
       "4                              0.689697                         0.689697   \n",
       "...                                 ...                              ...   \n",
       "6814                           0.689778                         0.689778   \n",
       "6815                           0.689734                         0.689734   \n",
       "6816                           0.689202                         0.689202   \n",
       "6817                           0.696113                         0.696113   \n",
       "6818                           0.689527                         0.689527   \n",
       "\n",
       "       Continuous Net Profit Growth Rate   Total Asset Growth Rate  \\\n",
       "0                               0.217535              4.980000e+09   \n",
       "1                               0.217620              6.110000e+09   \n",
       "2                               0.217601              7.280000e+09   \n",
       "3                               0.217568              4.880000e+09   \n",
       "4                               0.217626              5.510000e+09   \n",
       "...                                  ...                       ...   \n",
       "6814                            0.217635              7.070000e+09   \n",
       "6815                            0.217631              5.220000e+09   \n",
       "6816                            0.217547              5.990000e+09   \n",
       "6817                            0.218006              7.250000e+09   \n",
       "6818                            0.217605              9.350000e+09   \n",
       "\n",
       "       Net Value Growth Rate   Total Asset Return Growth Rate Ratio  \\\n",
       "0                   0.000327                               0.263100   \n",
       "1                   0.000443                               0.264516   \n",
       "2                   0.000396                               0.264184   \n",
       "3                   0.000382                               0.263371   \n",
       "4                   0.000439                               0.265218   \n",
       "...                      ...                                    ...   \n",
       "6814                0.000450                               0.264517   \n",
       "6815                0.000445                               0.264730   \n",
       "6816                0.000435                               0.263858   \n",
       "6817                0.000529                               0.264409   \n",
       "6818                0.000519                               0.264186   \n",
       "\n",
       "       Cash Reinvestment %   Current Ratio   Quick Ratio  \\\n",
       "0                 0.363725        0.002259      0.001208   \n",
       "1                 0.376709        0.006016      0.004039   \n",
       "2                 0.368913        0.011543      0.005348   \n",
       "3                 0.384077        0.004194      0.002896   \n",
       "4                 0.379690        0.006022      0.003727   \n",
       "...                    ...             ...           ...   \n",
       "6814              0.380155        0.010451      0.005457   \n",
       "6815              0.377389        0.009259      0.006741   \n",
       "6816              0.379392        0.038424      0.035112   \n",
       "6817              0.401028        0.012782      0.007256   \n",
       "6818              0.360102        0.051348      0.040897   \n",
       "\n",
       "       Interest Expense Ratio   Total debt/Total net worth   Debt ratio %  \\\n",
       "0                    0.629951                     0.021266       0.207576   \n",
       "1                    0.635172                     0.012502       0.171176   \n",
       "2                    0.629631                     0.021248       0.207516   \n",
       "3                    0.630228                     0.009572       0.151465   \n",
       "4                    0.636055                     0.005150       0.106509   \n",
       "...                       ...                          ...            ...   \n",
       "6814                 0.631415                     0.006655       0.124618   \n",
       "6815                 0.631489                     0.004623       0.099253   \n",
       "6816                 0.630612                     0.001392       0.038939   \n",
       "6817                 0.630731                     0.003816       0.086979   \n",
       "6818                 0.630618                     0.000461       0.014149   \n",
       "\n",
       "       Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "0              0.792424                               0.005024   \n",
       "1              0.828824                               0.005059   \n",
       "2              0.792484                               0.005100   \n",
       "3              0.848535                               0.005047   \n",
       "4              0.893491                               0.005303   \n",
       "...                 ...                                    ...   \n",
       "6814           0.875382                               0.005150   \n",
       "6815           0.900747                               0.006772   \n",
       "6816           0.961061                               0.009149   \n",
       "6817           0.913021                               0.005529   \n",
       "6818           0.985851                               0.058476   \n",
       "\n",
       "       Borrowing dependency   Contingent liabilities/Net worth  \\\n",
       "0                  0.390284                           0.006479   \n",
       "1                  0.376760                           0.005835   \n",
       "2                  0.379093                           0.006562   \n",
       "3                  0.379743                           0.005366   \n",
       "4                  0.375025                           0.006624   \n",
       "...                     ...                                ...   \n",
       "6814               0.373823                           0.005366   \n",
       "6815               0.372505                           0.008619   \n",
       "6816               0.369637                           0.005366   \n",
       "6817               0.369649                           0.007068   \n",
       "6818               0.370049                           0.006368   \n",
       "\n",
       "       Operating profit/Paid-in capital  \\\n",
       "0                              0.095885   \n",
       "1                              0.093743   \n",
       "2                              0.092318   \n",
       "3                              0.077727   \n",
       "4                              0.096927   \n",
       "...                                 ...   \n",
       "6814                           0.098222   \n",
       "6815                           0.098572   \n",
       "6816                           0.100103   \n",
       "6817                           0.111722   \n",
       "6818                           0.092465   \n",
       "\n",
       "       Net profit before tax/Paid-in capital  \\\n",
       "0                                   0.137757   \n",
       "1                                   0.168962   \n",
       "2                                   0.148036   \n",
       "3                                   0.147561   \n",
       "4                                   0.167461   \n",
       "...                                      ...   \n",
       "6814                                0.171111   \n",
       "6815                                0.171805   \n",
       "6816                                0.172287   \n",
       "6817                                0.182498   \n",
       "6818                                0.179911   \n",
       "\n",
       "       Inventory and accounts receivable/Net value   Total Asset Turnover  \\\n",
       "0                                         0.398036               0.086957   \n",
       "1                                         0.397725               0.064468   \n",
       "2                                         0.406580               0.014993   \n",
       "3                                         0.397925               0.089955   \n",
       "4                                         0.400079               0.175412   \n",
       "...                                            ...                    ...   \n",
       "6814                                      0.404804               0.103448   \n",
       "6815                                      0.399926               0.103448   \n",
       "6816                                      0.395592               0.106447   \n",
       "6817                                      0.401540               0.109445   \n",
       "6818                                      0.393883               0.002999   \n",
       "\n",
       "       Accounts Receivable Turnover   Average Collection Days  \\\n",
       "0                          0.001814                  0.003487   \n",
       "1                          0.001286                  0.004917   \n",
       "2                          0.001495                  0.004227   \n",
       "3                          0.001966                  0.003215   \n",
       "4                          0.001449                  0.004367   \n",
       "...                             ...                       ...   \n",
       "6814                       0.000690                  0.009177   \n",
       "6815                       0.000655                  0.009652   \n",
       "6816                       0.001510                  0.004188   \n",
       "6817                       0.000716                  0.008829   \n",
       "6818                       0.000325                  0.019474   \n",
       "\n",
       "       Inventory Turnover Rate (times)   Fixed Assets Turnover Frequency  \\\n",
       "0                         1.820926e-04                      1.165007e-04   \n",
       "1                         9.360000e+09                      7.190000e+08   \n",
       "2                         6.500000e+07                      2.650000e+09   \n",
       "3                         7.130000e+09                      9.150000e+09   \n",
       "4                         1.633674e-04                      2.935211e-04   \n",
       "...                                ...                               ...   \n",
       "6814                      4.030000e+07                      1.429781e-04   \n",
       "6815                      9.940000e+09                      6.051982e-04   \n",
       "6816                      2.797309e-04                      1.024298e-03   \n",
       "6817                      4.550000e+09                      2.330013e-04   \n",
       "6818                      1.910000e+07                      2.995731e-04   \n",
       "\n",
       "       Net Worth Turnover Rate (times)   Revenue per person  \\\n",
       "0                             0.032903             0.034164   \n",
       "1                             0.025484             0.006889   \n",
       "2                             0.013387             0.028997   \n",
       "3                             0.028065             0.015463   \n",
       "4                             0.040161             0.058111   \n",
       "...                                ...                  ...   \n",
       "6814                          0.027903             0.006348   \n",
       "6815                          0.027419             0.016083   \n",
       "6816                          0.022419             0.022097   \n",
       "6817                          0.027258             0.012749   \n",
       "6818                          0.009194             0.002097   \n",
       "\n",
       "       Operating profit per person   Allocation rate per person  \\\n",
       "0                         0.392913                     0.037135   \n",
       "1                         0.391590                     0.012335   \n",
       "2                         0.381968                     0.141016   \n",
       "3                         0.378497                     0.021320   \n",
       "4                         0.394371                     0.023988   \n",
       "...                            ...                          ...   \n",
       "6814                      0.392596                     0.006312   \n",
       "6815                      0.393625                     0.003401   \n",
       "6816                      0.393693                     0.002774   \n",
       "6817                      0.396735                     0.007489   \n",
       "6818                      0.385767                     0.000963   \n",
       "\n",
       "       Working Capital to Total Assets   Quick Assets/Total Assets  \\\n",
       "0                             0.672775                    0.166673   \n",
       "1                             0.751111                    0.127236   \n",
       "2                             0.829502                    0.340201   \n",
       "3                             0.725754                    0.161575   \n",
       "4                             0.751822                    0.260330   \n",
       "...                                ...                         ...   \n",
       "6814                          0.817769                    0.312840   \n",
       "6815                          0.793387                    0.335085   \n",
       "6816                          0.866047                    0.476747   \n",
       "6817                          0.832340                    0.353624   \n",
       "6818                          0.873759                    0.527136   \n",
       "\n",
       "       Current Assets/Total Assets   Cash/Total Assets  \\\n",
       "0                         0.190643            0.004094   \n",
       "1                         0.182419            0.014948   \n",
       "2                         0.602806            0.000991   \n",
       "3                         0.225815            0.018851   \n",
       "4                         0.358380            0.014161   \n",
       "...                            ...                 ...   \n",
       "6814                      0.578455            0.099481   \n",
       "6815                      0.444043            0.080337   \n",
       "6816                      0.496053            0.412885   \n",
       "6817                      0.564439            0.112238   \n",
       "6818                      0.505010            0.238147   \n",
       "\n",
       "       Quick Assets/Current Liability   Cash/Current Liability  \\\n",
       "0                            0.001997             1.473360e-04   \n",
       "1                            0.004136             1.383910e-03   \n",
       "2                            0.006302             5.340000e+09   \n",
       "3                            0.002961             1.010646e-03   \n",
       "4                            0.004275             6.804636e-04   \n",
       "...                               ...                      ...   \n",
       "6814                         0.005469             5.071548e-03   \n",
       "6815                         0.006790             4.727181e-03   \n",
       "6816                         0.035531             8.821248e-02   \n",
       "6817                         0.007753             7.133218e-03   \n",
       "6818                         0.051481             6.667354e-02   \n",
       "\n",
       "       Current Liability to Assets   Operating Funds to Liability  \\\n",
       "0                         0.147308                       0.334015   \n",
       "1                         0.056963                       0.341106   \n",
       "2                         0.098162                       0.336731   \n",
       "3                         0.098715                       0.348716   \n",
       "4                         0.110195                       0.344639   \n",
       "...                            ...                            ...   \n",
       "6814                      0.103838                       0.346224   \n",
       "6815                      0.089901                       0.342166   \n",
       "6816                      0.024414                       0.358847   \n",
       "6817                      0.083199                       0.380251   \n",
       "6818                      0.018517                       0.239585   \n",
       "\n",
       "       Inventory/Working Capital   Inventory/Current Liability  \\\n",
       "0                       0.276920                      0.001036   \n",
       "1                       0.289642                      0.005210   \n",
       "2                       0.277456                      0.013879   \n",
       "3                       0.276580                      0.003540   \n",
       "4                       0.287913                      0.004869   \n",
       "...                          ...                           ...   \n",
       "6814                    0.277543                      0.013212   \n",
       "6815                    0.277368                      0.006730   \n",
       "6816                    0.277022                      0.007810   \n",
       "6817                    0.277353                      0.013334   \n",
       "6818                    0.276975                      0.000000   \n",
       "\n",
       "       Current Liabilities/Liability   Working Capital/Equity  \\\n",
       "0                           0.676269                 0.721275   \n",
       "1                           0.308589                 0.731975   \n",
       "2                           0.446027                 0.742729   \n",
       "3                           0.615848                 0.729825   \n",
       "4                           0.975007                 0.732000   \n",
       "...                              ...                      ...   \n",
       "6814                        0.786888                 0.736716   \n",
       "6815                        0.849898                 0.734584   \n",
       "6816                        0.553964                 0.737432   \n",
       "6817                        0.893241                 0.736713   \n",
       "6818                        1.000000                 0.737286   \n",
       "\n",
       "       Current Liabilities/Equity   Long-term Liability to Current Assets  \\\n",
       "0                        0.339077                            2.559237e-02   \n",
       "1                        0.329740                            2.394682e-02   \n",
       "2                        0.334777                            3.715116e-03   \n",
       "3                        0.331509                            2.216520e-02   \n",
       "4                        0.330726                            0.000000e+00   \n",
       "...                           ...                                     ...   \n",
       "6814                     0.330914                            1.792237e-03   \n",
       "6815                     0.329753                            2.204673e-03   \n",
       "6816                     0.326921                            0.000000e+00   \n",
       "6817                     0.329294                            3.200000e+09   \n",
       "6818                     0.326690                            0.000000e+00   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total income/Total expense  \\\n",
       "0                               0.903225                     0.002022   \n",
       "1                               0.931065                     0.002226   \n",
       "2                               0.909903                     0.002060   \n",
       "3                               0.906902                     0.001831   \n",
       "4                               0.913850                     0.002224   \n",
       "...                                  ...                          ...   \n",
       "6814                            0.925611                     0.002266   \n",
       "6815                            0.932629                     0.002288   \n",
       "6816                            0.932000                     0.002239   \n",
       "6817                            0.939613                     0.002395   \n",
       "6818                            0.938005                     0.002791   \n",
       "\n",
       "       Total expense/Assets   Current Asset Turnover Rate  \\\n",
       "0                  0.064856                  7.010000e+08   \n",
       "1                  0.025516                  1.065198e-04   \n",
       "2                  0.021387                  1.791094e-03   \n",
       "3                  0.024161                  8.140000e+09   \n",
       "4                  0.026385                  6.680000e+09   \n",
       "...                     ...                           ...   \n",
       "6814               0.019060                  2.294154e-04   \n",
       "6815               0.011118                  1.517299e-04   \n",
       "6816               0.035446                  1.762272e-04   \n",
       "6817               0.016443                  2.135940e-04   \n",
       "6818               0.006089                  7.863781e-03   \n",
       "\n",
       "       Quick Asset Turnover Rate   Working capitcal Turnover Rate  \\\n",
       "0                   6.550000e+09                         0.593831   \n",
       "1                   7.700000e+09                         0.593916   \n",
       "2                   1.022676e-03                         0.594502   \n",
       "3                   6.050000e+09                         0.593889   \n",
       "4                   5.050000e+09                         0.593915   \n",
       "...                          ...                              ...   \n",
       "6814                1.244230e-04                         0.593985   \n",
       "6815                1.173396e-04                         0.593954   \n",
       "6816                1.749713e-04                         0.594025   \n",
       "6817                1.351937e-04                         0.593997   \n",
       "6818                8.238471e-03                         0.598674   \n",
       "\n",
       "       Cash Turnover Rate   Cash Flow to Sales   Fixed Assets to Assets  \\\n",
       "0            4.580000e+08             0.671568                 0.424206   \n",
       "1            2.490000e+09             0.671570                 0.468828   \n",
       "2            7.610000e+08             0.671571                 0.276179   \n",
       "3            2.030000e+09             0.671519                 0.559144   \n",
       "4            8.240000e+08             0.671563                 0.309555   \n",
       "...                   ...                  ...                      ...   \n",
       "6814         1.077940e-04             0.671570                 0.400338   \n",
       "6815         7.710000e+09             0.671572                 0.096136   \n",
       "6816         4.074263e-04             0.671564                 0.055509   \n",
       "6817         1.165392e-04             0.671606                 0.246805   \n",
       "6818         9.505992e-03             0.672096                 0.005016   \n",
       "\n",
       "       Current Liability to Liability   Current Liability to Equity  \\\n",
       "0                            0.676269                      0.339077   \n",
       "1                            0.308589                      0.329740   \n",
       "2                            0.446027                      0.334777   \n",
       "3                            0.615848                      0.331509   \n",
       "4                            0.975007                      0.330726   \n",
       "...                               ...                           ...   \n",
       "6814                         0.786888                      0.330914   \n",
       "6815                         0.849898                      0.329753   \n",
       "6816                         0.553964                      0.326921   \n",
       "6817                         0.893241                      0.329294   \n",
       "6818                         1.000000                      0.326690   \n",
       "\n",
       "       Equity to Long-term Liability   Cash Flow to Total Assets  \\\n",
       "0                           0.126549                    0.637555   \n",
       "1                           0.120916                    0.641100   \n",
       "2                           0.117922                    0.642765   \n",
       "3                           0.120760                    0.579039   \n",
       "4                           0.110933                    0.622374   \n",
       "...                              ...                         ...   \n",
       "6814                        0.112622                    0.639806   \n",
       "6815                        0.112329                    0.642072   \n",
       "6816                        0.110933                    0.631678   \n",
       "6817                        0.110957                    0.684857   \n",
       "6818                        0.110933                    0.659917   \n",
       "\n",
       "       Cash Flow to Liability   CFO to Assets   Cash Flow to Equity  \\\n",
       "0                    0.458609        0.520382              0.312905   \n",
       "1                    0.459001        0.567101              0.314163   \n",
       "2                    0.459254        0.538491              0.314515   \n",
       "3                    0.448518        0.604105              0.302382   \n",
       "4                    0.454411        0.578469              0.311567   \n",
       "...                       ...             ...                   ...   \n",
       "6814                 0.458639        0.587178              0.314063   \n",
       "6815                 0.459058        0.569498              0.314446   \n",
       "6816                 0.452465        0.589341              0.313353   \n",
       "6817                 0.471313        0.678338              0.320118   \n",
       "6818                 0.483285        0.505531              0.316238   \n",
       "\n",
       "       Current Liability to Current Assets   Liability-Assets Flag  \\\n",
       "0                                 0.118250                       0   \n",
       "1                                 0.047775                       0   \n",
       "2                                 0.025346                       0   \n",
       "3                                 0.067250                       0   \n",
       "4                                 0.047725                       0   \n",
       "...                                    ...                     ...   \n",
       "6814                              0.027951                       0   \n",
       "6815                              0.031470                       0   \n",
       "6816                              0.007542                       0   \n",
       "6817                              0.022916                       0   \n",
       "6818                              0.005579                       0   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0                        0.716845                    0.009219   \n",
       "1                        0.795297                    0.008323   \n",
       "2                        0.774670                    0.040003   \n",
       "3                        0.739555                    0.003252   \n",
       "4                        0.795016                    0.003878   \n",
       "...                           ...                         ...   \n",
       "6814                     0.799927                    0.000466   \n",
       "6815                     0.799748                    0.001959   \n",
       "6816                     0.797778                    0.002840   \n",
       "6817                     0.811808                    0.002837   \n",
       "6818                     0.815956                    0.000707   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "0                0.622879                0.601453   \n",
       "1                0.623652                0.610237   \n",
       "2                0.623841                0.601449   \n",
       "3                0.622929                0.583538   \n",
       "4                0.623521                0.598782   \n",
       "...                   ...                     ...   \n",
       "6814             0.623620                0.604455   \n",
       "6815             0.623931                0.598306   \n",
       "6816             0.624156                0.610441   \n",
       "6817             0.623957                0.607846   \n",
       "6818             0.626680                0.627408   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                                0.827890              0.290202   \n",
       "1                                0.839969              0.283846   \n",
       "2                                0.836774              0.290189   \n",
       "3                                0.834697              0.281721   \n",
       "4                                0.839973              0.278514   \n",
       "...                                   ...                   ...   \n",
       "6814                             0.840359              0.279606   \n",
       "6815                             0.840306              0.278132   \n",
       "6816                             0.840138              0.275789   \n",
       "6817                             0.841084              0.277547   \n",
       "6818                             0.841019              0.275114   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "0                                0.026601   \n",
       "1                                0.264577   \n",
       "2                                0.026555   \n",
       "3                                0.026697   \n",
       "4                                0.024752   \n",
       "...                                   ...   \n",
       "6814                             0.027064   \n",
       "6815                             0.027009   \n",
       "6816                             0.026791   \n",
       "6817                             0.026822   \n",
       "6818                             0.026793   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "0                                              0.564050                   1   \n",
       "1                                              0.570175                   1   \n",
       "2                                              0.563706                   1   \n",
       "3                                              0.564663                   1   \n",
       "4                                              0.575617                   1   \n",
       "...                                                 ...                 ...   \n",
       "6814                                           0.566193                   1   \n",
       "6815                                           0.566018                   1   \n",
       "6816                                           0.565158                   1   \n",
       "6817                                           0.565302                   1   \n",
       "6818                                           0.565167                   1   \n",
       "\n",
       "       Equity to Liability  \n",
       "0                 0.016469  \n",
       "1                 0.020794  \n",
       "2                 0.016474  \n",
       "3                 0.023982  \n",
       "4                 0.035490  \n",
       "...                    ...  \n",
       "6814              0.029890  \n",
       "6815              0.038284  \n",
       "6816              0.097649  \n",
       "6817              0.044009  \n",
       "6818              0.233902  \n",
       "\n",
       "[6819 rows x 96 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f357ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([' Current Liability to Liability', ' Net Income Flag'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfa0052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6819, 94)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7baae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30477381",
   "metadata": {},
   "source": [
    "# Separating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c45973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Bankrupt?'], axis = 1)\n",
    "y = df['Bankrupt?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea246009",
   "metadata": {},
   "source": [
    "# Selecting features with the K-best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c977ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "transformer = MinMaxScaler().fit(X1)\n",
    "x_normalized = transformer.transform(X1)\n",
    "X2 = pd.DataFrame(x_normalized, columns= X1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b0d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Research and development expense rate</th>\n",
       "      <th>Interest-bearing debt interest rate</th>\n",
       "      <th>Tax rate (A)</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Net Value Growth Rate</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Total debt/Total net worth</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Contingent liabilities/Net worth</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Total Asset Turnover</th>\n",
       "      <th>Average Collection Days</th>\n",
       "      <th>Fixed Assets Turnover Frequency</th>\n",
       "      <th>Revenue per person</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Current Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Cash/Current Liability</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Current Asset Turnover Rate</th>\n",
       "      <th>Quick Asset Turnover Rate</th>\n",
       "      <th>Cash Turnover Rate</th>\n",
       "      <th>Fixed Assets to Assets</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Liability-Assets Flag</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.323965e-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.169141</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.138736</td>\n",
       "      <td>0.498498</td>\n",
       "      <td>3.504580e-14</td>\n",
       "      <td>1.308510e-13</td>\n",
       "      <td>2.139429e-12</td>\n",
       "      <td>0.207576</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.390284</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.095885</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>3.584136e-13</td>\n",
       "      <td>1.166173e-14</td>\n",
       "      <td>3.877887e-12</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>0.672775</td>\n",
       "      <td>0.166673</td>\n",
       "      <td>0.190643</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>1.526798e-14</td>\n",
       "      <td>0.147308</td>\n",
       "      <td>0.334015</td>\n",
       "      <td>0.676269</td>\n",
       "      <td>0.903225</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>7.010000e-02</td>\n",
       "      <td>6.550000e-01</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>5.098627e-11</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>0.520382</td>\n",
       "      <td>0.118250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>9.388432e-13</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.536007e-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.093722</td>\n",
       "      <td>0.169918</td>\n",
       "      <td>0.611612</td>\n",
       "      <td>4.748554e-14</td>\n",
       "      <td>4.376345e-13</td>\n",
       "      <td>1.257786e-12</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.828824</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.093743</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.064468</td>\n",
       "      <td>5.053246e-13</td>\n",
       "      <td>7.197197e-02</td>\n",
       "      <td>7.819127e-13</td>\n",
       "      <td>0.391590</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.127236</td>\n",
       "      <td>0.182419</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>1.434104e-13</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>0.341106</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.931065</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>1.065198e-14</td>\n",
       "      <td>7.700000e-01</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>5.634953e-11</td>\n",
       "      <td>0.120916</td>\n",
       "      <td>0.567101</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>8.475867e-13</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>7.980596e-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.193713</td>\n",
       "      <td>0.180581</td>\n",
       "      <td>0.092338</td>\n",
       "      <td>0.142803</td>\n",
       "      <td>0.728729</td>\n",
       "      <td>4.248932e-14</td>\n",
       "      <td>5.793673e-13</td>\n",
       "      <td>2.137594e-12</td>\n",
       "      <td>0.207516</td>\n",
       "      <td>0.792484</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.379093</td>\n",
       "      <td>0.006562</td>\n",
       "      <td>0.092318</td>\n",
       "      <td>0.148036</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>4.344141e-13</td>\n",
       "      <td>2.652653e-01</td>\n",
       "      <td>3.291369e-12</td>\n",
       "      <td>0.381968</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.340201</td>\n",
       "      <td>0.602806</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>5.533679e-01</td>\n",
       "      <td>0.098162</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.446027</td>\n",
       "      <td>0.909903</td>\n",
       "      <td>0.021387</td>\n",
       "      <td>1.791094e-13</td>\n",
       "      <td>1.022676e-13</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>3.319462e-11</td>\n",
       "      <td>0.117922</td>\n",
       "      <td>0.538491</td>\n",
       "      <td>0.025346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>4.073610e-12</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.535807e-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.148603</td>\n",
       "      <td>0.488488</td>\n",
       "      <td>4.098884e-14</td>\n",
       "      <td>3.138127e-13</td>\n",
       "      <td>9.630183e-13</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.089955</td>\n",
       "      <td>3.304180e-13</td>\n",
       "      <td>9.159159e-01</td>\n",
       "      <td>1.755219e-12</td>\n",
       "      <td>0.378497</td>\n",
       "      <td>0.725754</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.225815</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>1.047302e-13</td>\n",
       "      <td>0.098715</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.906902</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>8.140000e-01</td>\n",
       "      <td>6.050000e-01</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>6.720480e-11</td>\n",
       "      <td>0.120760</td>\n",
       "      <td>0.604105</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>3.312093e-13</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.929986e-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.096898</td>\n",
       "      <td>0.168412</td>\n",
       "      <td>0.551552</td>\n",
       "      <td>4.704690e-14</td>\n",
       "      <td>4.038404e-13</td>\n",
       "      <td>5.180684e-13</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>0.375025</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>0.096927</td>\n",
       "      <td>0.167461</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>4.488068e-13</td>\n",
       "      <td>2.938149e-14</td>\n",
       "      <td>6.596075e-12</td>\n",
       "      <td>0.394371</td>\n",
       "      <td>0.751822</td>\n",
       "      <td>0.260330</td>\n",
       "      <td>0.358380</td>\n",
       "      <td>0.014161</td>\n",
       "      <td>7.051436e-14</td>\n",
       "      <td>0.110195</td>\n",
       "      <td>0.344639</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.913850</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>6.680000e-01</td>\n",
       "      <td>5.050000e-01</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>3.720612e-11</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.578469</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>3.948639e-13</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROA(C) before interest and depreciation before interest  \\\n",
       "0                                           0.370594          \n",
       "1                                           0.464291          \n",
       "2                                           0.426071          \n",
       "3                                           0.399844          \n",
       "4                                           0.465022          \n",
       "\n",
       "    ROA(A) before interest and % after tax  \\\n",
       "0                                 0.424389   \n",
       "1                                 0.538214   \n",
       "2                                 0.499019   \n",
       "3                                 0.451265   \n",
       "4                                 0.538432   \n",
       "\n",
       "    ROA(B) before interest and depreciation after tax  \\\n",
       "0                                           0.405750    \n",
       "1                                           0.516730    \n",
       "2                                           0.472295    \n",
       "3                                           0.457733    \n",
       "4                                           0.522298    \n",
       "\n",
       "    Research and development expense rate  \\\n",
       "0                                0.000000   \n",
       "1                                0.000000   \n",
       "2                                0.002555   \n",
       "3                                0.000000   \n",
       "4                                0.000000   \n",
       "\n",
       "    Interest-bearing debt interest rate   Tax rate (A)  \\\n",
       "0                          7.323965e-13            0.0   \n",
       "1                          6.536007e-13            0.0   \n",
       "2                          7.980596e-13            0.0   \n",
       "3                          4.535807e-13            0.0   \n",
       "4                          6.929986e-13            0.0   \n",
       "\n",
       "    Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "0                  0.147950                  0.147950   \n",
       "1                  0.182251                  0.182251   \n",
       "2                  0.177911                  0.177911   \n",
       "3                  0.154187                  0.154187   \n",
       "4                  0.167502                  0.167502   \n",
       "\n",
       "    Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "0                  0.147950                                  0.169141   \n",
       "1                  0.182251                                  0.208944   \n",
       "2                  0.193713                                  0.180581   \n",
       "3                  0.154187                                  0.193722   \n",
       "4                  0.167502                                  0.212537   \n",
       "\n",
       "    Operating Profit Per Share (Yuan ¥)  \\\n",
       "0                              0.095921   \n",
       "1                              0.093722   \n",
       "2                              0.092338   \n",
       "3                              0.077762   \n",
       "4                              0.096898   \n",
       "\n",
       "    Per Share Net profit before tax (Yuan ¥)   Total Asset Growth Rate  \\\n",
       "0                                   0.138736                  0.498498   \n",
       "1                                   0.169918                  0.611612   \n",
       "2                                   0.142803                  0.728729   \n",
       "3                                   0.148603                  0.488488   \n",
       "4                                   0.168412                  0.551552   \n",
       "\n",
       "    Net Value Growth Rate   Quick Ratio   Total debt/Total net worth  \\\n",
       "0            3.504580e-14  1.308510e-13                 2.139429e-12   \n",
       "1            4.748554e-14  4.376345e-13                 1.257786e-12   \n",
       "2            4.248932e-14  5.793673e-13                 2.137594e-12   \n",
       "3            4.098884e-14  3.138127e-13                 9.630183e-13   \n",
       "4            4.704690e-14  4.038404e-13                 5.180684e-13   \n",
       "\n",
       "    Debt ratio %   Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "0       0.207576           0.792424                               0.005024   \n",
       "1       0.171176           0.828824                               0.005059   \n",
       "2       0.207516           0.792484                               0.005100   \n",
       "3       0.151465           0.848535                               0.005047   \n",
       "4       0.106509           0.893491                               0.005303   \n",
       "\n",
       "    Borrowing dependency   Contingent liabilities/Net worth  \\\n",
       "0               0.390284                           0.006479   \n",
       "1               0.376760                           0.005835   \n",
       "2               0.379093                           0.006562   \n",
       "3               0.379743                           0.005366   \n",
       "4               0.375025                           0.006624   \n",
       "\n",
       "    Operating profit/Paid-in capital   Net profit before tax/Paid-in capital  \\\n",
       "0                           0.095885                                0.137757   \n",
       "1                           0.093743                                0.168962   \n",
       "2                           0.092318                                0.148036   \n",
       "3                           0.077727                                0.147561   \n",
       "4                           0.096927                                0.167461   \n",
       "\n",
       "    Total Asset Turnover   Average Collection Days  \\\n",
       "0               0.086957              3.584136e-13   \n",
       "1               0.064468              5.053246e-13   \n",
       "2               0.014993              4.344141e-13   \n",
       "3               0.089955              3.304180e-13   \n",
       "4               0.175412              4.488068e-13   \n",
       "\n",
       "    Fixed Assets Turnover Frequency   Revenue per person  \\\n",
       "0                      1.166173e-14         3.877887e-12   \n",
       "1                      7.197197e-02         7.819127e-13   \n",
       "2                      2.652653e-01         3.291369e-12   \n",
       "3                      9.159159e-01         1.755219e-12   \n",
       "4                      2.938149e-14         6.596075e-12   \n",
       "\n",
       "    Operating profit per person   Working Capital to Total Assets  \\\n",
       "0                      0.392913                          0.672775   \n",
       "1                      0.391590                          0.751111   \n",
       "2                      0.381968                          0.829502   \n",
       "3                      0.378497                          0.725754   \n",
       "4                      0.394371                          0.751822   \n",
       "\n",
       "    Quick Assets/Total Assets   Current Assets/Total Assets  \\\n",
       "0                    0.166673                      0.190643   \n",
       "1                    0.127236                      0.182419   \n",
       "2                    0.340201                      0.602806   \n",
       "3                    0.161575                      0.225815   \n",
       "4                    0.260330                      0.358380   \n",
       "\n",
       "    Cash/Total Assets   Cash/Current Liability   Current Liability to Assets  \\\n",
       "0            0.004094             1.526798e-14                      0.147308   \n",
       "1            0.014948             1.434104e-13                      0.056963   \n",
       "2            0.000991             5.533679e-01                      0.098162   \n",
       "3            0.018851             1.047302e-13                      0.098715   \n",
       "4            0.014161             7.051436e-14                      0.110195   \n",
       "\n",
       "    Operating Funds to Liability   Current Liabilities/Liability  \\\n",
       "0                       0.334015                        0.676269   \n",
       "1                       0.341106                        0.308589   \n",
       "2                       0.336731                        0.446027   \n",
       "3                       0.348716                        0.615848   \n",
       "4                       0.344639                        0.975007   \n",
       "\n",
       "    Retained Earnings to Total Assets   Total expense/Assets  \\\n",
       "0                            0.903225               0.064856   \n",
       "1                            0.931065               0.025516   \n",
       "2                            0.909903               0.021387   \n",
       "3                            0.906902               0.024161   \n",
       "4                            0.913850               0.026385   \n",
       "\n",
       "    Current Asset Turnover Rate   Quick Asset Turnover Rate  \\\n",
       "0                  7.010000e-02                6.550000e-01   \n",
       "1                  1.065198e-14                7.700000e-01   \n",
       "2                  1.791094e-13                1.022676e-13   \n",
       "3                  8.140000e-01                6.050000e-01   \n",
       "4                  6.680000e-01                5.050000e-01   \n",
       "\n",
       "    Cash Turnover Rate   Fixed Assets to Assets  \\\n",
       "0               0.0458             5.098627e-11   \n",
       "1               0.2490             5.634953e-11   \n",
       "2               0.0761             3.319462e-11   \n",
       "3               0.2030             6.720480e-11   \n",
       "4               0.0824             3.720612e-11   \n",
       "\n",
       "    Equity to Long-term Liability   CFO to Assets  \\\n",
       "0                        0.126549        0.520382   \n",
       "1                        0.120916        0.567101   \n",
       "2                        0.117922        0.538491   \n",
       "3                        0.120760        0.604105   \n",
       "4                        0.110933        0.578469   \n",
       "\n",
       "    Current Liability to Current Assets   Liability-Assets Flag  \\\n",
       "0                              0.118250                     0.0   \n",
       "1                              0.047775                     0.0   \n",
       "2                              0.025346                     0.0   \n",
       "3                              0.067250                     0.0   \n",
       "4                              0.047725                     0.0   \n",
       "\n",
       "    Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0                     0.716845                9.388432e-13   \n",
       "1                     0.795297                8.475867e-13   \n",
       "2                     0.774670                4.073610e-12   \n",
       "3                     0.739555                3.312093e-13   \n",
       "4                     0.795016                3.948639e-13   \n",
       "\n",
       "    Liability to Equity   Equity to Liability  \n",
       "0              0.290202              0.016469  \n",
       "1              0.283846              0.020794  \n",
       "2              0.290189              0.016474  \n",
       "3              0.281721              0.023982  \n",
       "4              0.278514              0.035490  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "kbest = SelectKBest(chi2, k=50) \n",
    "kbest.fit(X2,y)\n",
    "X_new = kbest.transform(X2) \n",
    "selected_columns = [X2.columns[index] for index, value in enumerate(kbest.get_support().tolist()) if value == True]\n",
    "selected = pd.DataFrame(X_new, columns = selected_columns)\n",
    "selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab69c5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131.996215</td>\n",
       "      <td>Liability-Assets Flag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.067819</td>\n",
       "      <td>Cash/Current Liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.995449</td>\n",
       "      <td>Fixed Assets to Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.951695</td>\n",
       "      <td>Net Value Growth Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.024311</td>\n",
       "      <td>Fixed Assets Turnover Frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.720307</td>\n",
       "      <td>Tax rate (A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.960800</td>\n",
       "      <td>Debt ratio %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.681405</td>\n",
       "      <td>Cash/Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.799248</td>\n",
       "      <td>Revenue per person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.193747</td>\n",
       "      <td>Current Liability to Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.508465</td>\n",
       "      <td>Total assets to GNP price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.041779</td>\n",
       "      <td>Current Liability to Current Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.187952</td>\n",
       "      <td>Quick Assets/Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.207267</td>\n",
       "      <td>ROA(A) before interest and % after tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.483747</td>\n",
       "      <td>ROA(B) before interest and depreciation after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.380804</td>\n",
       "      <td>ROA(C) before interest and depreciation befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.329265</td>\n",
       "      <td>Total expense/Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.315159</td>\n",
       "      <td>Quick Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.610895</td>\n",
       "      <td>Interest-bearing debt interest rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.472230</td>\n",
       "      <td>Equity to Liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.391564</td>\n",
       "      <td>Quick Asset Turnover Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.271949</td>\n",
       "      <td>Total Asset Turnover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.053878</td>\n",
       "      <td>Total Asset Growth Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.651368</td>\n",
       "      <td>Per Share Net profit before tax (Yuan ¥)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.589258</td>\n",
       "      <td>Persistent EPS in the Last Four Seasons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score                                             Column\n",
       "0   131.996215                              Liability-Assets Flag\n",
       "1    30.067819                             Cash/Current Liability\n",
       "2    29.995449                             Fixed Assets to Assets\n",
       "3    25.951695                              Net Value Growth Rate\n",
       "4    22.024311                    Fixed Assets Turnover Frequency\n",
       "5    13.720307                                       Tax rate (A)\n",
       "6    10.960800                                       Debt ratio %\n",
       "7    10.681405                                  Cash/Total Assets\n",
       "8     9.799248                                 Revenue per person\n",
       "9     7.193747                        Current Liability to Assets\n",
       "10    6.508465                          Total assets to GNP price\n",
       "11    6.041779                Current Liability to Current Assets\n",
       "12    5.187952                          Quick Assets/Total Assets\n",
       "13    4.207267             ROA(A) before interest and % after tax\n",
       "14    3.483747   ROA(B) before interest and depreciation after...\n",
       "15    3.380804   ROA(C) before interest and depreciation befor...\n",
       "16    3.329265                               Total expense/Assets\n",
       "17    3.315159                                        Quick Ratio\n",
       "18    2.610895                Interest-bearing debt interest rate\n",
       "19    2.472230                                Equity to Liability\n",
       "20    2.391564                          Quick Asset Turnover Rate\n",
       "21    2.271949                               Total Asset Turnover\n",
       "22    2.053878                            Total Asset Growth Rate\n",
       "23    1.651368           Per Share Net profit before tax (Yuan ¥)\n",
       "24    1.589258            Persistent EPS in the Last Four Seasons"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = [elem for elem in zip(kbest.scores_, X2.columns.tolist())]\n",
    "ml.sort(reverse=True)\n",
    "scores = pd.DataFrame(data = ml, columns = ['score','Column'])\n",
    "scores.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362916b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533912b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb67523",
   "metadata": {},
   "source": [
    "# Balancing the dataset: upsampling and downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d4b6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.concat([selected, y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad462f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_1['Bankrupt?'].astype('int')\n",
    "X = df_1.drop(['Bankrupt?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e16158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4672631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5455, 51)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN = pd.concat([X_train, y_train], axis=1)\n",
    "TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57cbec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Research and development expense rate</th>\n",
       "      <th>Interest-bearing debt interest rate</th>\n",
       "      <th>Tax rate (A)</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Net Value Growth Rate</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Total debt/Total net worth</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Contingent liabilities/Net worth</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Total Asset Turnover</th>\n",
       "      <th>Average Collection Days</th>\n",
       "      <th>Fixed Assets Turnover Frequency</th>\n",
       "      <th>Revenue per person</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Current Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Cash/Current Liability</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Current Asset Turnover Rate</th>\n",
       "      <th>Quick Asset Turnover Rate</th>\n",
       "      <th>Cash Turnover Rate</th>\n",
       "      <th>Fixed Assets to Assets</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Liability-Assets Flag</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Equity to Liability</th>\n",
       "      <th>Bankrupt?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>0.519768</td>\n",
       "      <td>0.538051</td>\n",
       "      <td>0.558060</td>\n",
       "      <td>5.020040e-01</td>\n",
       "      <td>3.525605e-13</td>\n",
       "      <td>0.558761</td>\n",
       "      <td>0.174877</td>\n",
       "      <td>0.174877</td>\n",
       "      <td>0.174877</td>\n",
       "      <td>0.215940</td>\n",
       "      <td>0.102598</td>\n",
       "      <td>0.172629</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>4.796698e-14</td>\n",
       "      <td>5.440186e-13</td>\n",
       "      <td>4.509131e-13</td>\n",
       "      <td>0.097197</td>\n",
       "      <td>0.902803</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.373542</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.102562</td>\n",
       "      <td>0.171631</td>\n",
       "      <td>0.088456</td>\n",
       "      <td>1.143255e-12</td>\n",
       "      <td>1.204035e-14</td>\n",
       "      <td>2.093992e-12</td>\n",
       "      <td>0.398408</td>\n",
       "      <td>0.792863</td>\n",
       "      <td>0.474841</td>\n",
       "      <td>0.459619</td>\n",
       "      <td>0.100335</td>\n",
       "      <td>5.774934e-13</td>\n",
       "      <td>0.095271</td>\n",
       "      <td>0.353342</td>\n",
       "      <td>0.919886</td>\n",
       "      <td>0.937487</td>\n",
       "      <td>0.041187</td>\n",
       "      <td>2.165894e-14</td>\n",
       "      <td>2.304338e-14</td>\n",
       "      <td>1.292516e-14</td>\n",
       "      <td>3.910017e-11</td>\n",
       "      <td>0.112116</td>\n",
       "      <td>0.605097</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>3.234024e-14</td>\n",
       "      <td>0.278029</td>\n",
       "      <td>0.039145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.535807e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.148603</td>\n",
       "      <td>0.488488</td>\n",
       "      <td>4.098884e-14</td>\n",
       "      <td>3.138127e-13</td>\n",
       "      <td>9.630183e-13</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.089955</td>\n",
       "      <td>3.304180e-13</td>\n",
       "      <td>9.159159e-01</td>\n",
       "      <td>1.755219e-12</td>\n",
       "      <td>0.378497</td>\n",
       "      <td>0.725754</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.225815</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>1.047302e-13</td>\n",
       "      <td>0.098715</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.906902</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>8.140000e-01</td>\n",
       "      <td>6.050000e-01</td>\n",
       "      <td>2.030000e-01</td>\n",
       "      <td>6.720480e-11</td>\n",
       "      <td>0.120760</td>\n",
       "      <td>0.604105</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>3.312093e-13</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641</th>\n",
       "      <td>0.337640</td>\n",
       "      <td>0.254307</td>\n",
       "      <td>0.378446</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.788158e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>0.164792</td>\n",
       "      <td>0.074098</td>\n",
       "      <td>0.091738</td>\n",
       "      <td>0.025726</td>\n",
       "      <td>2.401558e-14</td>\n",
       "      <td>3.344947e-13</td>\n",
       "      <td>8.564138e-12</td>\n",
       "      <td>0.268706</td>\n",
       "      <td>0.731294</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.402534</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>0.090634</td>\n",
       "      <td>0.244378</td>\n",
       "      <td>7.586012e-13</td>\n",
       "      <td>9.730727e-14</td>\n",
       "      <td>4.838189e-12</td>\n",
       "      <td>0.378983</td>\n",
       "      <td>0.740426</td>\n",
       "      <td>0.465562</td>\n",
       "      <td>0.739236</td>\n",
       "      <td>0.108758</td>\n",
       "      <td>2.493248e-13</td>\n",
       "      <td>0.239915</td>\n",
       "      <td>0.350564</td>\n",
       "      <td>0.858173</td>\n",
       "      <td>0.852516</td>\n",
       "      <td>0.136224</td>\n",
       "      <td>7.290000e-01</td>\n",
       "      <td>4.620000e-02</td>\n",
       "      <td>3.330000e-01</td>\n",
       "      <td>2.253425e-11</td>\n",
       "      <td>0.144985</td>\n",
       "      <td>0.649732</td>\n",
       "      <td>0.050780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.557733</td>\n",
       "      <td>8.620669e-14</td>\n",
       "      <td>0.336515</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.469458</td>\n",
       "      <td>0.539250</td>\n",
       "      <td>0.526902</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.465393e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.216224</td>\n",
       "      <td>0.096898</td>\n",
       "      <td>0.170144</td>\n",
       "      <td>0.746747</td>\n",
       "      <td>4.784662e-14</td>\n",
       "      <td>2.331798e-13</td>\n",
       "      <td>7.414608e-13</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>0.867944</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.377480</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.096894</td>\n",
       "      <td>0.169165</td>\n",
       "      <td>0.041979</td>\n",
       "      <td>5.643873e-13</td>\n",
       "      <td>3.253253e-01</td>\n",
       "      <td>1.061334e-12</td>\n",
       "      <td>0.393207</td>\n",
       "      <td>0.742227</td>\n",
       "      <td>0.077033</td>\n",
       "      <td>0.138243</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>1.165108e-13</td>\n",
       "      <td>0.053358</td>\n",
       "      <td>0.344074</td>\n",
       "      <td>0.375513</td>\n",
       "      <td>0.931610</td>\n",
       "      <td>0.007297</td>\n",
       "      <td>1.394958e-14</td>\n",
       "      <td>8.040000e-01</td>\n",
       "      <td>3.140000e-01</td>\n",
       "      <td>8.540841e-11</td>\n",
       "      <td>0.125439</td>\n",
       "      <td>0.579259</td>\n",
       "      <td>0.058460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799720</td>\n",
       "      <td>7.364524e-13</td>\n",
       "      <td>0.280124</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.475469</td>\n",
       "      <td>0.456074</td>\n",
       "      <td>3.246493e-01</td>\n",
       "      <td>7.889678e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146559</td>\n",
       "      <td>0.146559</td>\n",
       "      <td>0.146559</td>\n",
       "      <td>0.180108</td>\n",
       "      <td>0.085498</td>\n",
       "      <td>0.142577</td>\n",
       "      <td>0.546547</td>\n",
       "      <td>3.690467e-14</td>\n",
       "      <td>3.055669e-13</td>\n",
       "      <td>3.258409e-12</td>\n",
       "      <td>0.231702</td>\n",
       "      <td>0.768298</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.404609</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.085463</td>\n",
       "      <td>0.141603</td>\n",
       "      <td>0.094453</td>\n",
       "      <td>1.109380e-12</td>\n",
       "      <td>5.641550e-14</td>\n",
       "      <td>8.273936e-12</td>\n",
       "      <td>0.370378</td>\n",
       "      <td>0.708529</td>\n",
       "      <td>0.311743</td>\n",
       "      <td>0.483895</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>1.409178e-13</td>\n",
       "      <td>0.197270</td>\n",
       "      <td>0.345557</td>\n",
       "      <td>0.815745</td>\n",
       "      <td>0.910702</td>\n",
       "      <td>0.037014</td>\n",
       "      <td>1.877357e-14</td>\n",
       "      <td>1.225404e-14</td>\n",
       "      <td>5.420000e-01</td>\n",
       "      <td>1.134380e-11</td>\n",
       "      <td>0.125988</td>\n",
       "      <td>0.601032</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749555</td>\n",
       "      <td>1.789358e-12</td>\n",
       "      <td>0.298268</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>0.360796</td>\n",
       "      <td>0.400403</td>\n",
       "      <td>0.398094</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.142128e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.166210</td>\n",
       "      <td>0.084114</td>\n",
       "      <td>0.133313</td>\n",
       "      <td>0.467467</td>\n",
       "      <td>3.722028e-14</td>\n",
       "      <td>2.094563e-13</td>\n",
       "      <td>1.060724e-12</td>\n",
       "      <td>0.158660</td>\n",
       "      <td>0.841340</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.381373</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.084135</td>\n",
       "      <td>0.132268</td>\n",
       "      <td>0.077961</td>\n",
       "      <td>4.701326e-13</td>\n",
       "      <td>7.117117e-01</td>\n",
       "      <td>2.399755e-12</td>\n",
       "      <td>0.380916</td>\n",
       "      <td>0.717092</td>\n",
       "      <td>0.106695</td>\n",
       "      <td>0.156383</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>2.569948e-02</td>\n",
       "      <td>0.087106</td>\n",
       "      <td>0.336488</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.880488</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>6.230000e-01</td>\n",
       "      <td>4.450000e-01</td>\n",
       "      <td>5.020000e-03</td>\n",
       "      <td>7.912751e-11</td>\n",
       "      <td>0.126967</td>\n",
       "      <td>0.543242</td>\n",
       "      <td>0.084685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.701107</td>\n",
       "      <td>1.071476e-12</td>\n",
       "      <td>0.282426</td>\n",
       "      <td>0.022732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.536007e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.093722</td>\n",
       "      <td>0.169918</td>\n",
       "      <td>0.611612</td>\n",
       "      <td>4.748554e-14</td>\n",
       "      <td>4.376345e-13</td>\n",
       "      <td>1.257786e-12</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.828824</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.093743</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.064468</td>\n",
       "      <td>5.053246e-13</td>\n",
       "      <td>7.197197e-02</td>\n",
       "      <td>7.819127e-13</td>\n",
       "      <td>0.391590</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.127236</td>\n",
       "      <td>0.182419</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>1.434104e-13</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>0.341106</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.931065</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>1.065198e-14</td>\n",
       "      <td>7.700000e-01</td>\n",
       "      <td>2.490000e-01</td>\n",
       "      <td>5.634953e-11</td>\n",
       "      <td>0.120916</td>\n",
       "      <td>0.567101</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>8.475867e-13</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>0.464047</td>\n",
       "      <td>0.527257</td>\n",
       "      <td>0.517533</td>\n",
       "      <td>1.090505e-14</td>\n",
       "      <td>3.505401e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.209133</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.165775</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>4.730099e-14</td>\n",
       "      <td>5.117114e-13</td>\n",
       "      <td>1.359253e-12</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.379319</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0.164822</td>\n",
       "      <td>0.112444</td>\n",
       "      <td>1.383280e-12</td>\n",
       "      <td>2.832134e-14</td>\n",
       "      <td>1.243391e-12</td>\n",
       "      <td>0.392517</td>\n",
       "      <td>0.783541</td>\n",
       "      <td>0.463405</td>\n",
       "      <td>0.663927</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>1.549267e-13</td>\n",
       "      <td>0.168455</td>\n",
       "      <td>0.341540</td>\n",
       "      <td>0.909853</td>\n",
       "      <td>0.932147</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>2.485980e-14</td>\n",
       "      <td>1.758731e-14</td>\n",
       "      <td>4.840000e-01</td>\n",
       "      <td>2.237006e-11</td>\n",
       "      <td>0.113409</td>\n",
       "      <td>0.569659</td>\n",
       "      <td>0.039624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791303</td>\n",
       "      <td>1.153670e-13</td>\n",
       "      <td>0.284577</td>\n",
       "      <td>0.020014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.495475</td>\n",
       "      <td>0.493763</td>\n",
       "      <td>1.002004e-01</td>\n",
       "      <td>4.586317e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147529</td>\n",
       "      <td>0.147529</td>\n",
       "      <td>0.147529</td>\n",
       "      <td>0.201191</td>\n",
       "      <td>0.088592</td>\n",
       "      <td>0.160654</td>\n",
       "      <td>0.526527</td>\n",
       "      <td>4.190357e-14</td>\n",
       "      <td>2.852904e-13</td>\n",
       "      <td>1.832368e-12</td>\n",
       "      <td>0.197539</td>\n",
       "      <td>0.802461</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.382879</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.088565</td>\n",
       "      <td>0.159679</td>\n",
       "      <td>0.151424</td>\n",
       "      <td>5.629832e-13</td>\n",
       "      <td>2.347491e-14</td>\n",
       "      <td>1.383769e-12</td>\n",
       "      <td>0.389736</td>\n",
       "      <td>0.706854</td>\n",
       "      <td>0.292874</td>\n",
       "      <td>0.413661</td>\n",
       "      <td>0.060502</td>\n",
       "      <td>1.872532e-13</td>\n",
       "      <td>0.177582</td>\n",
       "      <td>0.345573</td>\n",
       "      <td>0.859651</td>\n",
       "      <td>0.898535</td>\n",
       "      <td>0.060988</td>\n",
       "      <td>9.170000e-01</td>\n",
       "      <td>6.670000e-01</td>\n",
       "      <td>3.980000e-01</td>\n",
       "      <td>4.132963e-11</td>\n",
       "      <td>0.117993</td>\n",
       "      <td>0.595849</td>\n",
       "      <td>0.066816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.768935</td>\n",
       "      <td>5.741823e-14</td>\n",
       "      <td>0.287988</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0.466728</td>\n",
       "      <td>0.490078</td>\n",
       "      <td>0.501097</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.303461e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.194006</td>\n",
       "      <td>0.084439</td>\n",
       "      <td>0.155457</td>\n",
       "      <td>0.673674</td>\n",
       "      <td>4.426795e-14</td>\n",
       "      <td>1.570753e-13</td>\n",
       "      <td>2.568966e-12</td>\n",
       "      <td>0.218672</td>\n",
       "      <td>0.781328</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.393737</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.085650</td>\n",
       "      <td>0.156104</td>\n",
       "      <td>0.089955</td>\n",
       "      <td>6.058102e-13</td>\n",
       "      <td>6.886887e-01</td>\n",
       "      <td>2.509123e-12</td>\n",
       "      <td>0.384150</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.239571</td>\n",
       "      <td>0.007454</td>\n",
       "      <td>2.587370e-14</td>\n",
       "      <td>0.158285</td>\n",
       "      <td>0.340148</td>\n",
       "      <td>0.690745</td>\n",
       "      <td>0.910031</td>\n",
       "      <td>0.041586</td>\n",
       "      <td>1.007911e-14</td>\n",
       "      <td>8.590000e-01</td>\n",
       "      <td>9.090000e-02</td>\n",
       "      <td>8.313099e-11</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.101788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767057</td>\n",
       "      <td>8.242671e-13</td>\n",
       "      <td>0.293298</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "1868                                           0.519768          \n",
       "3                                              0.399844          \n",
       "6641                                           0.337640          \n",
       "233                                            0.469458          \n",
       "1445                                           0.411300          \n",
       "...                                                 ...          \n",
       "1179                                           0.360796          \n",
       "1                                              0.464291          \n",
       "948                                            0.464047          \n",
       "2908                                           0.446497          \n",
       "2022                                           0.466728          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "1868                                 0.538051   \n",
       "3                                    0.451265   \n",
       "6641                                 0.254307   \n",
       "233                                  0.539250   \n",
       "1445                                 0.475469   \n",
       "...                                       ...   \n",
       "1179                                 0.400403   \n",
       "1                                    0.538214   \n",
       "948                                  0.527257   \n",
       "2908                                 0.495475   \n",
       "2022                                 0.490078   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "1868                                           0.558060    \n",
       "3                                              0.457733    \n",
       "6641                                           0.378446    \n",
       "233                                            0.526902    \n",
       "1445                                           0.456074    \n",
       "...                                                 ...    \n",
       "1179                                           0.398094    \n",
       "1                                              0.516730    \n",
       "948                                            0.517533    \n",
       "2908                                           0.493763    \n",
       "2022                                           0.501097    \n",
       "\n",
       "       Research and development expense rate  \\\n",
       "1868                            5.020040e-01   \n",
       "3                               0.000000e+00   \n",
       "6641                            0.000000e+00   \n",
       "233                             0.000000e+00   \n",
       "1445                            3.246493e-01   \n",
       "...                                      ...   \n",
       "1179                            0.000000e+00   \n",
       "1                               0.000000e+00   \n",
       "948                             1.090505e-14   \n",
       "2908                            1.002004e-01   \n",
       "2022                            0.000000e+00   \n",
       "\n",
       "       Interest-bearing debt interest rate   Tax rate (A)  \\\n",
       "1868                          3.525605e-13       0.558761   \n",
       "3                             4.535807e-13       0.000000   \n",
       "6641                          2.788158e-13       0.000000   \n",
       "233                           7.465393e-13       0.000000   \n",
       "1445                          7.889678e-13       0.000000   \n",
       "...                                    ...            ...   \n",
       "1179                          7.142128e-13       0.000000   \n",
       "1                             6.536007e-13       0.000000   \n",
       "948                           3.505401e-13       0.000000   \n",
       "2908                          4.586317e-13       0.000000   \n",
       "2022                          4.303461e-13       0.000000   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "1868                  0.174877                  0.174877   \n",
       "3                     0.154187                  0.154187   \n",
       "6641                  0.131010                  0.131010   \n",
       "233                   0.177953                  0.177953   \n",
       "1445                  0.146559                  0.146559   \n",
       "...                        ...                       ...   \n",
       "1179                  0.158400                  0.158400   \n",
       "1                     0.182251                  0.182251   \n",
       "948                   0.181703                  0.181703   \n",
       "2908                  0.147529                  0.147529   \n",
       "2022                  0.152206                  0.152206   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "1868                  0.174877                                  0.215940   \n",
       "3                     0.154187                                  0.193722   \n",
       "6641                  0.131010                                  0.164792   \n",
       "233                   0.177953                                  0.216224   \n",
       "1445                  0.146559                                  0.180108   \n",
       "...                        ...                                       ...   \n",
       "1179                  0.158400                                  0.166210   \n",
       "1                     0.182251                                  0.208944   \n",
       "948                   0.181703                                  0.209133   \n",
       "2908                  0.147529                                  0.201191   \n",
       "2022                  0.152206                                  0.194006   \n",
       "\n",
       "       Operating Profit Per Share (Yuan ¥)  \\\n",
       "1868                              0.102598   \n",
       "3                                 0.077762   \n",
       "6641                              0.074098   \n",
       "233                               0.096898   \n",
       "1445                              0.085498   \n",
       "...                                    ...   \n",
       "1179                              0.084114   \n",
       "1                                 0.093722   \n",
       "948                               0.097875   \n",
       "2908                              0.088592   \n",
       "2022                              0.084439   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)   Total Asset Growth Rate  \\\n",
       "1868                                   0.172629                  0.738739   \n",
       "3                                      0.148603                  0.488488   \n",
       "6641                                   0.091738                  0.025726   \n",
       "233                                    0.170144                  0.746747   \n",
       "1445                                   0.142577                  0.546547   \n",
       "...                                         ...                       ...   \n",
       "1179                                   0.133313                  0.467467   \n",
       "1                                      0.169918                  0.611612   \n",
       "948                                    0.165775                  0.729730   \n",
       "2908                                   0.160654                  0.526527   \n",
       "2022                                   0.155457                  0.673674   \n",
       "\n",
       "       Net Value Growth Rate   Quick Ratio   Total debt/Total net worth  \\\n",
       "1868            4.796698e-14  5.440186e-13                 4.509131e-13   \n",
       "3               4.098884e-14  3.138127e-13                 9.630183e-13   \n",
       "6641            2.401558e-14  3.344947e-13                 8.564138e-12   \n",
       "233             4.784662e-14  2.331798e-13                 7.414608e-13   \n",
       "1445            3.690467e-14  3.055669e-13                 3.258409e-12   \n",
       "...                      ...           ...                          ...   \n",
       "1179            3.722028e-14  2.094563e-13                 1.060724e-12   \n",
       "1               4.748554e-14  4.376345e-13                 1.257786e-12   \n",
       "948             4.730099e-14  5.117114e-13                 1.359253e-12   \n",
       "2908            4.190357e-14  2.852904e-13                 1.832368e-12   \n",
       "2022            4.426795e-14  1.570753e-13                 2.568966e-12   \n",
       "\n",
       "       Debt ratio %   Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "1868       0.097197           0.902803                               0.005325   \n",
       "3          0.151465           0.848535                               0.005047   \n",
       "6641       0.268706           0.731294                               0.005033   \n",
       "233        0.132056           0.867944                               0.005046   \n",
       "1445       0.231702           0.768298                               0.005720   \n",
       "...             ...                ...                                    ...   \n",
       "1179       0.158660           0.841340                               0.005025   \n",
       "1          0.171176           0.828824                               0.005059   \n",
       "948        0.176800           0.823200                               0.005386   \n",
       "2908       0.197539           0.802461                               0.005059   \n",
       "2022       0.218672           0.781328                               0.004911   \n",
       "\n",
       "       Borrowing dependency   Contingent liabilities/Net worth  \\\n",
       "1868               0.373542                           0.005366   \n",
       "3                  0.379743                           0.005366   \n",
       "6641               0.402534                           0.007171   \n",
       "233                0.377480                           0.005423   \n",
       "1445               0.404609                           0.006264   \n",
       "...                     ...                                ...   \n",
       "1179               0.381373                           0.005701   \n",
       "1                  0.376760                           0.005835   \n",
       "948                0.379319                           0.006585   \n",
       "2908               0.382879                           0.007247   \n",
       "2022               0.393737                           0.005366   \n",
       "\n",
       "       Operating profit/Paid-in capital  \\\n",
       "1868                           0.102562   \n",
       "3                              0.077727   \n",
       "6641                           0.074055   \n",
       "233                            0.096894   \n",
       "1445                           0.085463   \n",
       "...                                 ...   \n",
       "1179                           0.084135   \n",
       "1                              0.093743   \n",
       "948                            0.097872   \n",
       "2908                           0.088565   \n",
       "2022                           0.085650   \n",
       "\n",
       "       Net profit before tax/Paid-in capital   Total Asset Turnover  \\\n",
       "1868                                0.171631               0.088456   \n",
       "3                                   0.147561               0.089955   \n",
       "6641                                0.090634               0.244378   \n",
       "233                                 0.169165               0.041979   \n",
       "1445                                0.141603               0.094453   \n",
       "...                                      ...                    ...   \n",
       "1179                                0.132268               0.077961   \n",
       "1                                   0.168962               0.064468   \n",
       "948                                 0.164822               0.112444   \n",
       "2908                                0.159679               0.151424   \n",
       "2022                                0.156104               0.089955   \n",
       "\n",
       "       Average Collection Days   Fixed Assets Turnover Frequency  \\\n",
       "1868              1.143255e-12                      1.204035e-14   \n",
       "3                 3.304180e-13                      9.159159e-01   \n",
       "6641              7.586012e-13                      9.730727e-14   \n",
       "233               5.643873e-13                      3.253253e-01   \n",
       "1445              1.109380e-12                      5.641550e-14   \n",
       "...                        ...                               ...   \n",
       "1179              4.701326e-13                      7.117117e-01   \n",
       "1                 5.053246e-13                      7.197197e-02   \n",
       "948               1.383280e-12                      2.832134e-14   \n",
       "2908              5.629832e-13                      2.347491e-14   \n",
       "2022              6.058102e-13                      6.886887e-01   \n",
       "\n",
       "       Revenue per person   Operating profit per person  \\\n",
       "1868         2.093992e-12                      0.398408   \n",
       "3            1.755219e-12                      0.378497   \n",
       "6641         4.838189e-12                      0.378983   \n",
       "233          1.061334e-12                      0.393207   \n",
       "1445         8.273936e-12                      0.370378   \n",
       "...                   ...                           ...   \n",
       "1179         2.399755e-12                      0.380916   \n",
       "1            7.819127e-13                      0.391590   \n",
       "948          1.243391e-12                      0.392517   \n",
       "2908         1.383769e-12                      0.389736   \n",
       "2022         2.509123e-12                      0.384150   \n",
       "\n",
       "       Working Capital to Total Assets   Quick Assets/Total Assets  \\\n",
       "1868                          0.792863                    0.474841   \n",
       "3                             0.725754                    0.161575   \n",
       "6641                          0.740426                    0.465562   \n",
       "233                           0.742227                    0.077033   \n",
       "1445                          0.708529                    0.311743   \n",
       "...                                ...                         ...   \n",
       "1179                          0.717092                    0.106695   \n",
       "1                             0.751111                    0.127236   \n",
       "948                           0.783541                    0.463405   \n",
       "2908                          0.706854                    0.292874   \n",
       "2022                          0.676385                    0.195478   \n",
       "\n",
       "       Current Assets/Total Assets   Cash/Total Assets  \\\n",
       "1868                      0.459619            0.100335   \n",
       "3                         0.225815            0.018851   \n",
       "6641                      0.739236            0.108758   \n",
       "233                       0.138243            0.011382   \n",
       "1445                      0.483895            0.050565   \n",
       "...                            ...                 ...   \n",
       "1179                      0.156383            0.000409   \n",
       "1                         0.182419            0.014948   \n",
       "948                       0.663927            0.047491   \n",
       "2908                      0.413661            0.060502   \n",
       "2022                      0.239571            0.007454   \n",
       "\n",
       "       Cash/Current Liability   Current Liability to Assets  \\\n",
       "1868             5.774934e-13                      0.095271   \n",
       "3                1.047302e-13                      0.098715   \n",
       "6641             2.493248e-13                      0.239915   \n",
       "233              1.165108e-13                      0.053358   \n",
       "1445             1.409178e-13                      0.197270   \n",
       "...                       ...                           ...   \n",
       "1179             2.569948e-02                      0.087106   \n",
       "1                1.434104e-13                      0.056963   \n",
       "948              1.549267e-13                      0.168455   \n",
       "2908             1.872532e-13                      0.177582   \n",
       "2022             2.587370e-14                      0.158285   \n",
       "\n",
       "       Operating Funds to Liability   Current Liabilities/Liability  \\\n",
       "1868                       0.353342                        0.919886   \n",
       "3                          0.348716                        0.615848   \n",
       "6641                       0.350564                        0.858173   \n",
       "233                        0.344074                        0.375513   \n",
       "1445                       0.345557                        0.815745   \n",
       "...                             ...                             ...   \n",
       "1179                       0.336488                        0.517257   \n",
       "1                          0.341106                        0.308589   \n",
       "948                        0.341540                        0.909853   \n",
       "2908                       0.345573                        0.859651   \n",
       "2022                       0.340148                        0.690745   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total expense/Assets  \\\n",
       "1868                            0.937487               0.041187   \n",
       "3                               0.906902               0.024161   \n",
       "6641                            0.852516               0.136224   \n",
       "233                             0.931610               0.007297   \n",
       "1445                            0.910702               0.037014   \n",
       "...                                  ...                    ...   \n",
       "1179                            0.880488               0.062863   \n",
       "1                               0.931065               0.025516   \n",
       "948                             0.932147               0.039506   \n",
       "2908                            0.898535               0.060988   \n",
       "2022                            0.910031               0.041586   \n",
       "\n",
       "       Current Asset Turnover Rate   Quick Asset Turnover Rate  \\\n",
       "1868                  2.165894e-14                2.304338e-14   \n",
       "3                     8.140000e-01                6.050000e-01   \n",
       "6641                  7.290000e-01                4.620000e-02   \n",
       "233                   1.394958e-14                8.040000e-01   \n",
       "1445                  1.877357e-14                1.225404e-14   \n",
       "...                            ...                         ...   \n",
       "1179                  6.230000e-01                4.450000e-01   \n",
       "1                     1.065198e-14                7.700000e-01   \n",
       "948                   2.485980e-14                1.758731e-14   \n",
       "2908                  9.170000e-01                6.670000e-01   \n",
       "2022                  1.007911e-14                8.590000e-01   \n",
       "\n",
       "       Cash Turnover Rate   Fixed Assets to Assets  \\\n",
       "1868         1.292516e-14             3.910017e-11   \n",
       "3            2.030000e-01             6.720480e-11   \n",
       "6641         3.330000e-01             2.253425e-11   \n",
       "233          3.140000e-01             8.540841e-11   \n",
       "1445         5.420000e-01             1.134380e-11   \n",
       "...                   ...                      ...   \n",
       "1179         5.020000e-03             7.912751e-11   \n",
       "1            2.490000e-01             5.634953e-11   \n",
       "948          4.840000e-01             2.237006e-11   \n",
       "2908         3.980000e-01             4.132963e-11   \n",
       "2022         9.090000e-02             8.313099e-11   \n",
       "\n",
       "       Equity to Long-term Liability   CFO to Assets  \\\n",
       "1868                        0.112116        0.605097   \n",
       "3                           0.120760        0.604105   \n",
       "6641                        0.144985        0.649732   \n",
       "233                         0.125439        0.579259   \n",
       "1445                        0.125988        0.601032   \n",
       "...                              ...             ...   \n",
       "1179                        0.126967        0.543242   \n",
       "1                           0.120916        0.567101   \n",
       "948                         0.113409        0.569659   \n",
       "2908                        0.117993        0.595849   \n",
       "2022                        0.134579        0.561082   \n",
       "\n",
       "       Current Liability to Current Assets   Liability-Assets Flag  \\\n",
       "1868                              0.032234                     0.0   \n",
       "3                                 0.067250                     0.0   \n",
       "6641                              0.050780                     0.0   \n",
       "233                               0.058460                     0.0   \n",
       "1445                              0.063574                     0.0   \n",
       "...                                    ...                     ...   \n",
       "1179                              0.084685                     0.0   \n",
       "1                                 0.047775                     0.0   \n",
       "948                               0.039624                     0.0   \n",
       "2908                              0.066816                     0.0   \n",
       "2022                              0.101788                     0.0   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price  \\\n",
       "1868                     0.798835                3.234024e-14   \n",
       "3                        0.739555                3.312093e-13   \n",
       "6641                     0.557733                8.620669e-14   \n",
       "233                      0.799720                7.364524e-13   \n",
       "1445                     0.749555                1.789358e-12   \n",
       "...                           ...                         ...   \n",
       "1179                     0.701107                1.071476e-12   \n",
       "1                        0.795297                8.475867e-13   \n",
       "948                      0.791303                1.153670e-13   \n",
       "2908                     0.768935                5.741823e-14   \n",
       "2022                     0.767057                8.242671e-13   \n",
       "\n",
       "       Liability to Equity   Equity to Liability  Bankrupt?  \n",
       "1868              0.278029              0.039145          1  \n",
       "3                 0.281721              0.023982          1  \n",
       "6641              0.336515              0.011797          1  \n",
       "233               0.280124              0.028018          1  \n",
       "1445              0.298268              0.014336          1  \n",
       "...                    ...                   ...        ...  \n",
       "1179              0.282426              0.022732          1  \n",
       "1                 0.283846              0.020794          1  \n",
       "948               0.284577              0.020014          1  \n",
       "2908              0.287988              0.017506          1  \n",
       "2022              0.293298              0.015432          1  \n",
       "\n",
       "[176 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TRAIN[TRAIN[\"Bankrupt?\"]==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478991e",
   "metadata": {},
   "source": [
    "## Upsampling the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0f997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_indexes = np.random.choice(TRAIN[TRAIN[\"Bankrupt?\"]==1].index, size = 400, replace=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb4bc5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Research and development expense rate</th>\n",
       "      <th>Interest-bearing debt interest rate</th>\n",
       "      <th>Tax rate (A)</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Net Value Growth Rate</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Total debt/Total net worth</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Contingent liabilities/Net worth</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Total Asset Turnover</th>\n",
       "      <th>Average Collection Days</th>\n",
       "      <th>Fixed Assets Turnover Frequency</th>\n",
       "      <th>Revenue per person</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Current Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Cash/Current Liability</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Current Asset Turnover Rate</th>\n",
       "      <th>Quick Asset Turnover Rate</th>\n",
       "      <th>Cash Turnover Rate</th>\n",
       "      <th>Fixed Assets to Assets</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Liability-Assets Flag</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Equity to Liability</th>\n",
       "      <th>Bankrupt?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>0.464047</td>\n",
       "      <td>0.527257</td>\n",
       "      <td>0.517533</td>\n",
       "      <td>1.090505e-14</td>\n",
       "      <td>3.505401e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.209133</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.165775</td>\n",
       "      <td>7.297297e-01</td>\n",
       "      <td>4.730099e-14</td>\n",
       "      <td>5.117114e-13</td>\n",
       "      <td>1.359253e-12</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.379319</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0.164822</td>\n",
       "      <td>0.112444</td>\n",
       "      <td>1.383280e-12</td>\n",
       "      <td>2.832134e-14</td>\n",
       "      <td>1.243391e-12</td>\n",
       "      <td>0.392517</td>\n",
       "      <td>0.783541</td>\n",
       "      <td>0.463405</td>\n",
       "      <td>0.663927</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>1.549267e-13</td>\n",
       "      <td>0.168455</td>\n",
       "      <td>0.341540</td>\n",
       "      <td>0.909853</td>\n",
       "      <td>0.932147</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>2.485980e-14</td>\n",
       "      <td>1.758731e-14</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>2.237006e-11</td>\n",
       "      <td>0.113409</td>\n",
       "      <td>0.569659</td>\n",
       "      <td>0.039624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791303</td>\n",
       "      <td>1.153670e-13</td>\n",
       "      <td>0.284577</td>\n",
       "      <td>0.020014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>0.407400</td>\n",
       "      <td>0.442652</td>\n",
       "      <td>0.438086</td>\n",
       "      <td>2.106155e-14</td>\n",
       "      <td>5.071214e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152122</td>\n",
       "      <td>0.152122</td>\n",
       "      <td>0.152122</td>\n",
       "      <td>0.178501</td>\n",
       "      <td>0.075564</td>\n",
       "      <td>0.146193</td>\n",
       "      <td>4.064064e-01</td>\n",
       "      <td>3.751716e-14</td>\n",
       "      <td>6.270171e-13</td>\n",
       "      <td>1.647507e-12</td>\n",
       "      <td>0.190344</td>\n",
       "      <td>0.809656</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.387515</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.075586</td>\n",
       "      <td>0.145185</td>\n",
       "      <td>0.110945</td>\n",
       "      <td>1.119384e-12</td>\n",
       "      <td>1.416067e-14</td>\n",
       "      <td>1.329085e-12</td>\n",
       "      <td>0.385122</td>\n",
       "      <td>0.787303</td>\n",
       "      <td>0.499944</td>\n",
       "      <td>0.490401</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>2.017186e-13</td>\n",
       "      <td>0.110956</td>\n",
       "      <td>0.340664</td>\n",
       "      <td>0.552091</td>\n",
       "      <td>0.908825</td>\n",
       "      <td>0.082677</td>\n",
       "      <td>1.403254e-14</td>\n",
       "      <td>1.493000e-14</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>5.586839e-11</td>\n",
       "      <td>0.132919</td>\n",
       "      <td>0.564678</td>\n",
       "      <td>0.035221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727838</td>\n",
       "      <td>7.963314e-14</td>\n",
       "      <td>0.286656</td>\n",
       "      <td>0.018316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>0.483157</td>\n",
       "      <td>0.548354</td>\n",
       "      <td>0.537127</td>\n",
       "      <td>4.869739e-02</td>\n",
       "      <td>5.576315e-13</td>\n",
       "      <td>0.024395</td>\n",
       "      <td>0.170452</td>\n",
       "      <td>0.170452</td>\n",
       "      <td>0.170452</td>\n",
       "      <td>0.217926</td>\n",
       "      <td>0.100969</td>\n",
       "      <td>0.173834</td>\n",
       "      <td>8.828829e-01</td>\n",
       "      <td>7.619512e-14</td>\n",
       "      <td>4.264149e-13</td>\n",
       "      <td>6.513699e-13</td>\n",
       "      <td>0.122623</td>\n",
       "      <td>0.877377</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.374804</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.100941</td>\n",
       "      <td>0.171759</td>\n",
       "      <td>0.076462</td>\n",
       "      <td>1.098410e-12</td>\n",
       "      <td>1.075302e-14</td>\n",
       "      <td>1.302410e-12</td>\n",
       "      <td>0.395683</td>\n",
       "      <td>0.779916</td>\n",
       "      <td>0.225259</td>\n",
       "      <td>0.383837</td>\n",
       "      <td>0.047353</td>\n",
       "      <td>2.999732e-13</td>\n",
       "      <td>0.086516</td>\n",
       "      <td>0.346903</td>\n",
       "      <td>0.663970</td>\n",
       "      <td>0.930413</td>\n",
       "      <td>0.035048</td>\n",
       "      <td>2.265917e-14</td>\n",
       "      <td>1.345656e-14</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>4.696361e-11</td>\n",
       "      <td>0.116105</td>\n",
       "      <td>0.589539</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.802265</td>\n",
       "      <td>7.819361e-14</td>\n",
       "      <td>0.279475</td>\n",
       "      <td>0.030427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>0.126456</td>\n",
       "      <td>0.106629</td>\n",
       "      <td>0.112426</td>\n",
       "      <td>3.196393e-01</td>\n",
       "      <td>3.303361e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131052</td>\n",
       "      <td>0.131052</td>\n",
       "      <td>0.131052</td>\n",
       "      <td>0.132268</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.107705</td>\n",
       "      <td>2.412412e-01</td>\n",
       "      <td>2.432852e-14</td>\n",
       "      <td>2.863042e-13</td>\n",
       "      <td>4.184547e-12</td>\n",
       "      <td>0.243704</td>\n",
       "      <td>0.756296</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.401447</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.060506</td>\n",
       "      <td>0.106636</td>\n",
       "      <td>0.271364</td>\n",
       "      <td>5.118188e-13</td>\n",
       "      <td>1.235840e-13</td>\n",
       "      <td>5.422706e-12</td>\n",
       "      <td>0.353802</td>\n",
       "      <td>0.753911</td>\n",
       "      <td>0.391281</td>\n",
       "      <td>0.794697</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>6.163143e-14</td>\n",
       "      <td>0.241827</td>\n",
       "      <td>0.337234</td>\n",
       "      <td>0.953991</td>\n",
       "      <td>0.808267</td>\n",
       "      <td>0.223712</td>\n",
       "      <td>6.760000e-01</td>\n",
       "      <td>3.150000e-02</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>1.961944e-11</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.537968</td>\n",
       "      <td>0.047624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420995</td>\n",
       "      <td>5.470571e-14</td>\n",
       "      <td>0.304944</td>\n",
       "      <td>0.013430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.469458</td>\n",
       "      <td>0.539250</td>\n",
       "      <td>0.526902</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.465393e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.216224</td>\n",
       "      <td>0.096898</td>\n",
       "      <td>0.170144</td>\n",
       "      <td>7.467467e-01</td>\n",
       "      <td>4.784662e-14</td>\n",
       "      <td>2.331798e-13</td>\n",
       "      <td>7.414608e-13</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>0.867944</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.377480</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.096894</td>\n",
       "      <td>0.169165</td>\n",
       "      <td>0.041979</td>\n",
       "      <td>5.643873e-13</td>\n",
       "      <td>3.253253e-01</td>\n",
       "      <td>1.061334e-12</td>\n",
       "      <td>0.393207</td>\n",
       "      <td>0.742227</td>\n",
       "      <td>0.077033</td>\n",
       "      <td>0.138243</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>1.165108e-13</td>\n",
       "      <td>0.053358</td>\n",
       "      <td>0.344074</td>\n",
       "      <td>0.375513</td>\n",
       "      <td>0.931610</td>\n",
       "      <td>0.007297</td>\n",
       "      <td>1.394958e-14</td>\n",
       "      <td>8.040000e-01</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>8.540841e-11</td>\n",
       "      <td>0.125439</td>\n",
       "      <td>0.579259</td>\n",
       "      <td>0.058460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799720</td>\n",
       "      <td>7.364524e-13</td>\n",
       "      <td>0.280124</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0.435139</td>\n",
       "      <td>0.488661</td>\n",
       "      <td>0.479844</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.010902e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181282</td>\n",
       "      <td>0.181282</td>\n",
       "      <td>0.181282</td>\n",
       "      <td>0.172450</td>\n",
       "      <td>0.085416</td>\n",
       "      <td>0.131732</td>\n",
       "      <td>4.704705e-01</td>\n",
       "      <td>3.904438e-14</td>\n",
       "      <td>2.156068e-14</td>\n",
       "      <td>2.338510e-12</td>\n",
       "      <td>0.213079</td>\n",
       "      <td>0.786921</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>0.395591</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.085397</td>\n",
       "      <td>0.130692</td>\n",
       "      <td>0.026987</td>\n",
       "      <td>1.652529e-13</td>\n",
       "      <td>2.279338e-14</td>\n",
       "      <td>1.168568e-11</td>\n",
       "      <td>0.341376</td>\n",
       "      <td>0.807214</td>\n",
       "      <td>0.052647</td>\n",
       "      <td>0.918246</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>1.139916e-14</td>\n",
       "      <td>0.220007</td>\n",
       "      <td>0.346224</td>\n",
       "      <td>0.990887</td>\n",
       "      <td>0.923354</td>\n",
       "      <td>0.027422</td>\n",
       "      <td>1.281281e-13</td>\n",
       "      <td>7.460000e-01</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>8.362393e-12</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.602711</td>\n",
       "      <td>0.037485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.760822</td>\n",
       "      <td>1.464233e-12</td>\n",
       "      <td>0.291637</td>\n",
       "      <td>0.015941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0.384975</td>\n",
       "      <td>0.427061</td>\n",
       "      <td>0.457091</td>\n",
       "      <td>4.508329e-14</td>\n",
       "      <td>1.097079e-12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129957</td>\n",
       "      <td>0.129957</td>\n",
       "      <td>0.129957</td>\n",
       "      <td>0.187199</td>\n",
       "      <td>0.079472</td>\n",
       "      <td>0.135723</td>\n",
       "      <td>5.265265e-01</td>\n",
       "      <td>2.706467e-14</td>\n",
       "      <td>4.393243e-14</td>\n",
       "      <td>1.179328e-11</td>\n",
       "      <td>0.276113</td>\n",
       "      <td>0.723887</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.441969</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.079478</td>\n",
       "      <td>0.134688</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>2.757082e-12</td>\n",
       "      <td>1.441441e-01</td>\n",
       "      <td>4.574773e-13</td>\n",
       "      <td>0.376111</td>\n",
       "      <td>0.653619</td>\n",
       "      <td>0.126777</td>\n",
       "      <td>0.298176</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>1.181347e-01</td>\n",
       "      <td>0.201803</td>\n",
       "      <td>0.339230</td>\n",
       "      <td>0.700065</td>\n",
       "      <td>0.869388</td>\n",
       "      <td>0.059675</td>\n",
       "      <td>6.692571e-14</td>\n",
       "      <td>2.919316e-14</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>8.329596e-11</td>\n",
       "      <td>0.217743</td>\n",
       "      <td>0.552096</td>\n",
       "      <td>0.104796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.718183</td>\n",
       "      <td>8.542125e-14</td>\n",
       "      <td>0.359793</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4787</th>\n",
       "      <td>0.455809</td>\n",
       "      <td>0.493404</td>\n",
       "      <td>0.504684</td>\n",
       "      <td>4.198397e-02</td>\n",
       "      <td>3.838768e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192280</td>\n",
       "      <td>0.192280</td>\n",
       "      <td>0.192280</td>\n",
       "      <td>0.176988</td>\n",
       "      <td>0.090954</td>\n",
       "      <td>0.140468</td>\n",
       "      <td>4.864865e-01</td>\n",
       "      <td>4.220313e-14</td>\n",
       "      <td>4.001230e-13</td>\n",
       "      <td>1.669617e-12</td>\n",
       "      <td>0.191251</td>\n",
       "      <td>0.808749</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.386359</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.090902</td>\n",
       "      <td>0.139454</td>\n",
       "      <td>0.109445</td>\n",
       "      <td>4.359938e-13</td>\n",
       "      <td>9.919920e-01</td>\n",
       "      <td>1.845247e-12</td>\n",
       "      <td>0.391036</td>\n",
       "      <td>0.769235</td>\n",
       "      <td>0.212418</td>\n",
       "      <td>0.352064</td>\n",
       "      <td>0.061008</td>\n",
       "      <td>3.768647e-13</td>\n",
       "      <td>0.088734</td>\n",
       "      <td>0.346740</td>\n",
       "      <td>0.436592</td>\n",
       "      <td>0.921548</td>\n",
       "      <td>0.026816</td>\n",
       "      <td>1.072630e-14</td>\n",
       "      <td>6.540000e-01</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>7.421753e-11</td>\n",
       "      <td>0.141099</td>\n",
       "      <td>0.602053</td>\n",
       "      <td>0.039093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.766274</td>\n",
       "      <td>2.151085e-12</td>\n",
       "      <td>0.286815</td>\n",
       "      <td>0.018210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.506362</td>\n",
       "      <td>0.565689</td>\n",
       "      <td>0.561914</td>\n",
       "      <td>8.507014e-03</td>\n",
       "      <td>4.747950e-13</td>\n",
       "      <td>0.037656</td>\n",
       "      <td>0.159622</td>\n",
       "      <td>0.159622</td>\n",
       "      <td>0.159622</td>\n",
       "      <td>0.225867</td>\n",
       "      <td>0.114974</td>\n",
       "      <td>0.179483</td>\n",
       "      <td>1.219153e-14</td>\n",
       "      <td>6.356815e-14</td>\n",
       "      <td>5.505071e-13</td>\n",
       "      <td>2.627589e-12</td>\n",
       "      <td>0.219972</td>\n",
       "      <td>0.780028</td>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.384154</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.114987</td>\n",
       "      <td>0.178395</td>\n",
       "      <td>0.167916</td>\n",
       "      <td>1.045314e-12</td>\n",
       "      <td>2.483796e-14</td>\n",
       "      <td>5.279328e-12</td>\n",
       "      <td>0.407567</td>\n",
       "      <td>0.758209</td>\n",
       "      <td>0.591436</td>\n",
       "      <td>0.668394</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>4.853275e-14</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>0.340298</td>\n",
       "      <td>0.863506</td>\n",
       "      <td>0.922503</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>1.981049e-14</td>\n",
       "      <td>1.798903e-14</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>2.899699e-11</td>\n",
       "      <td>0.122219</td>\n",
       "      <td>0.562116</td>\n",
       "      <td>0.046350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.818788</td>\n",
       "      <td>5.291934e-13</td>\n",
       "      <td>0.293720</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>0.404036</td>\n",
       "      <td>0.223615</td>\n",
       "      <td>0.430055</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.677135e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.173301</td>\n",
       "      <td>0.061559</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>4.124124e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.266055e-14</td>\n",
       "      <td>7.740384e-12</td>\n",
       "      <td>0.338483</td>\n",
       "      <td>0.661517</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.295894</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>0.061581</td>\n",
       "      <td>0.089465</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>2.074657e-13</td>\n",
       "      <td>8.558559e-01</td>\n",
       "      <td>1.969953e-12</td>\n",
       "      <td>0.369225</td>\n",
       "      <td>0.643091</td>\n",
       "      <td>0.129527</td>\n",
       "      <td>0.124091</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>5.399063e-14</td>\n",
       "      <td>0.160152</td>\n",
       "      <td>0.335961</td>\n",
       "      <td>0.449042</td>\n",
       "      <td>0.843977</td>\n",
       "      <td>0.126337</td>\n",
       "      <td>3.130000e-01</td>\n",
       "      <td>3.890000e-01</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>7.836924e-11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514097</td>\n",
       "      <td>0.193950</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572881</td>\n",
       "      <td>8.817468e-13</td>\n",
       "      <td>0.218785</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "948                                            0.464047          \n",
       "4264                                           0.407400          \n",
       "3114                                           0.483157          \n",
       "4918                                           0.126456          \n",
       "233                                            0.469458          \n",
       "...                                                 ...          \n",
       "2044                                           0.435139          \n",
       "2004                                           0.384975          \n",
       "4787                                           0.455809          \n",
       "284                                            0.506362          \n",
       "2470                                           0.404036          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "948                                  0.527257   \n",
       "4264                                 0.442652   \n",
       "3114                                 0.548354   \n",
       "4918                                 0.106629   \n",
       "233                                  0.539250   \n",
       "...                                       ...   \n",
       "2044                                 0.488661   \n",
       "2004                                 0.427061   \n",
       "4787                                 0.493404   \n",
       "284                                  0.565689   \n",
       "2470                                 0.223615   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "948                                            0.517533    \n",
       "4264                                           0.438086    \n",
       "3114                                           0.537127    \n",
       "4918                                           0.112426    \n",
       "233                                            0.526902    \n",
       "...                                                 ...    \n",
       "2044                                           0.479844    \n",
       "2004                                           0.457091    \n",
       "4787                                           0.504684    \n",
       "284                                            0.561914    \n",
       "2470                                           0.430055    \n",
       "\n",
       "       Research and development expense rate  \\\n",
       "948                             1.090505e-14   \n",
       "4264                            2.106155e-14   \n",
       "3114                            4.869739e-02   \n",
       "4918                            3.196393e-01   \n",
       "233                             0.000000e+00   \n",
       "...                                      ...   \n",
       "2044                            0.000000e+00   \n",
       "2004                            4.508329e-14   \n",
       "4787                            4.198397e-02   \n",
       "284                             8.507014e-03   \n",
       "2470                            0.000000e+00   \n",
       "\n",
       "       Interest-bearing debt interest rate   Tax rate (A)  \\\n",
       "948                           3.505401e-13       0.000000   \n",
       "4264                          5.071214e-13       0.000000   \n",
       "3114                          5.576315e-13       0.024395   \n",
       "4918                          3.303361e-13       0.000000   \n",
       "233                           7.465393e-13       0.000000   \n",
       "...                                    ...            ...   \n",
       "2044                          8.010902e-13       0.000000   \n",
       "2004                          1.097079e-12       0.000000   \n",
       "4787                          3.838768e-13       0.000000   \n",
       "284                           4.747950e-13       0.037656   \n",
       "2470                          3.677135e-13       0.000000   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "948                   0.181703                  0.181703   \n",
       "4264                  0.152122                  0.152122   \n",
       "3114                  0.170452                  0.170452   \n",
       "4918                  0.131052                  0.131052   \n",
       "233                   0.177953                  0.177953   \n",
       "...                        ...                       ...   \n",
       "2044                  0.181282                  0.181282   \n",
       "2004                  0.129957                  0.129957   \n",
       "4787                  0.192280                  0.192280   \n",
       "284                   0.159622                  0.159622   \n",
       "2470                  0.116599                  0.116599   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "948                   0.181703                                  0.209133   \n",
       "4264                  0.152122                                  0.178501   \n",
       "3114                  0.170452                                  0.217926   \n",
       "4918                  0.131052                                  0.132268   \n",
       "233                   0.177953                                  0.216224   \n",
       "...                        ...                                       ...   \n",
       "2044                  0.181282                                  0.172450   \n",
       "2004                  0.129957                                  0.187199   \n",
       "4787                  0.192280                                  0.176988   \n",
       "284                   0.159622                                  0.225867   \n",
       "2470                  0.116599                                  0.173301   \n",
       "\n",
       "       Operating Profit Per Share (Yuan ¥)  \\\n",
       "948                               0.097875   \n",
       "4264                              0.075564   \n",
       "3114                              0.100969   \n",
       "4918                              0.060500   \n",
       "233                               0.096898   \n",
       "...                                    ...   \n",
       "2044                              0.085416   \n",
       "2004                              0.079472   \n",
       "4787                              0.090954   \n",
       "284                               0.114974   \n",
       "2470                              0.061559   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)   Total Asset Growth Rate  \\\n",
       "948                                    0.165775              7.297297e-01   \n",
       "4264                                   0.146193              4.064064e-01   \n",
       "3114                                   0.173834              8.828829e-01   \n",
       "4918                                   0.107705              2.412412e-01   \n",
       "233                                    0.170144              7.467467e-01   \n",
       "...                                         ...                       ...   \n",
       "2044                                   0.131732              4.704705e-01   \n",
       "2004                                   0.135723              5.265265e-01   \n",
       "4787                                   0.140468              4.864865e-01   \n",
       "284                                    0.179483              1.219153e-14   \n",
       "2470                                   0.085561              4.124124e-01   \n",
       "\n",
       "       Net Value Growth Rate   Quick Ratio   Total debt/Total net worth  \\\n",
       "948             4.730099e-14  5.117114e-13                 1.359253e-12   \n",
       "4264            3.751716e-14  6.270171e-13                 1.647507e-12   \n",
       "3114            7.619512e-14  4.264149e-13                 6.513699e-13   \n",
       "4918            2.432852e-14  2.863042e-13                 4.184547e-12   \n",
       "233             4.784662e-14  2.331798e-13                 7.414608e-13   \n",
       "...                      ...           ...                          ...   \n",
       "2044            3.904438e-14  2.156068e-14                 2.338510e-12   \n",
       "2004            2.706467e-14  4.393243e-14                 1.179328e-11   \n",
       "4787            4.220313e-14  4.001230e-13                 1.669617e-12   \n",
       "284             6.356815e-14  5.505071e-13                 2.627589e-12   \n",
       "2470            1.000000e+00  8.266055e-14                 7.740384e-12   \n",
       "\n",
       "       Debt ratio %   Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "948        0.176800           0.823200                               0.005386   \n",
       "4264       0.190344           0.809656                               0.005098   \n",
       "3114       0.122623           0.877377                               0.005216   \n",
       "4918       0.243704           0.756296                               0.005031   \n",
       "233        0.132056           0.867944                               0.005046   \n",
       "...             ...                ...                                    ...   \n",
       "2044       0.213079           0.786921                               0.005874   \n",
       "2004       0.276113           0.723887                               0.004853   \n",
       "4787       0.191251           0.808749                               0.005047   \n",
       "284        0.219972           0.780028                               0.005139   \n",
       "2470       0.338483           0.661517                               0.004877   \n",
       "\n",
       "       Borrowing dependency   Contingent liabilities/Net worth  \\\n",
       "948                0.379319                           0.006585   \n",
       "4264               0.387515                           0.005366   \n",
       "3114               0.374804                           0.005366   \n",
       "4918               0.401447                           0.005366   \n",
       "233                0.377480                           0.005423   \n",
       "...                     ...                                ...   \n",
       "2044               0.395591                           0.005446   \n",
       "2004               0.441969                           0.005366   \n",
       "4787               0.386359                           0.005366   \n",
       "284                0.384154                           0.005366   \n",
       "2470               0.295894                           0.004445   \n",
       "\n",
       "       Operating profit/Paid-in capital  \\\n",
       "948                            0.097872   \n",
       "4264                           0.075586   \n",
       "3114                           0.100941   \n",
       "4918                           0.060506   \n",
       "233                            0.096894   \n",
       "...                                 ...   \n",
       "2044                           0.085397   \n",
       "2004                           0.079478   \n",
       "4787                           0.090902   \n",
       "284                            0.114987   \n",
       "2470                           0.061581   \n",
       "\n",
       "       Net profit before tax/Paid-in capital   Total Asset Turnover  \\\n",
       "948                                 0.164822               0.112444   \n",
       "4264                                0.145185               0.110945   \n",
       "3114                                0.171759               0.076462   \n",
       "4918                                0.106636               0.271364   \n",
       "233                                 0.169165               0.041979   \n",
       "...                                      ...                    ...   \n",
       "2044                                0.130692               0.026987   \n",
       "2004                                0.134688               0.016492   \n",
       "4787                                0.139454               0.109445   \n",
       "284                                 0.178395               0.167916   \n",
       "2470                                0.089465               0.098951   \n",
       "\n",
       "       Average Collection Days   Fixed Assets Turnover Frequency  \\\n",
       "948               1.383280e-12                      2.832134e-14   \n",
       "4264              1.119384e-12                      1.416067e-14   \n",
       "3114              1.098410e-12                      1.075302e-14   \n",
       "4918              5.118188e-13                      1.235840e-13   \n",
       "233               5.643873e-13                      3.253253e-01   \n",
       "...                        ...                               ...   \n",
       "2044              1.652529e-13                      2.279338e-14   \n",
       "2004              2.757082e-12                      1.441441e-01   \n",
       "4787              4.359938e-13                      9.919920e-01   \n",
       "284               1.045314e-12                      2.483796e-14   \n",
       "2470              2.074657e-13                      8.558559e-01   \n",
       "\n",
       "       Revenue per person   Operating profit per person  \\\n",
       "948          1.243391e-12                      0.392517   \n",
       "4264         1.329085e-12                      0.385122   \n",
       "3114         1.302410e-12                      0.395683   \n",
       "4918         5.422706e-12                      0.353802   \n",
       "233          1.061334e-12                      0.393207   \n",
       "...                   ...                           ...   \n",
       "2044         1.168568e-11                      0.341376   \n",
       "2004         4.574773e-13                      0.376111   \n",
       "4787         1.845247e-12                      0.391036   \n",
       "284          5.279328e-12                      0.407567   \n",
       "2470         1.969953e-12                      0.369225   \n",
       "\n",
       "       Working Capital to Total Assets   Quick Assets/Total Assets  \\\n",
       "948                           0.783541                    0.463405   \n",
       "4264                          0.787303                    0.499944   \n",
       "3114                          0.779916                    0.225259   \n",
       "4918                          0.753911                    0.391281   \n",
       "233                           0.742227                    0.077033   \n",
       "...                                ...                         ...   \n",
       "2044                          0.807214                    0.052647   \n",
       "2004                          0.653619                    0.126777   \n",
       "4787                          0.769235                    0.212418   \n",
       "284                           0.758209                    0.591436   \n",
       "2470                          0.643091                    0.129527   \n",
       "\n",
       "       Current Assets/Total Assets   Cash/Total Assets  \\\n",
       "948                       0.663927            0.047491   \n",
       "4264                      0.490401            0.040788   \n",
       "3114                      0.383837            0.047353   \n",
       "4918                      0.794697            0.027098   \n",
       "233                       0.138243            0.011382   \n",
       "...                            ...                 ...   \n",
       "2044                      0.918246            0.004561   \n",
       "2004                      0.298176            0.000433   \n",
       "4787                      0.352064            0.061008   \n",
       "284                       0.668394            0.017499   \n",
       "2470                      0.124091            0.015737   \n",
       "\n",
       "       Cash/Current Liability   Current Liability to Assets  \\\n",
       "948              1.549267e-13                      0.168455   \n",
       "4264             2.017186e-13                      0.110956   \n",
       "3114             2.999732e-13                      0.086516   \n",
       "4918             6.163143e-14                      0.241827   \n",
       "233              1.165108e-13                      0.053358   \n",
       "...                       ...                           ...   \n",
       "2044             1.139916e-14                      0.220007   \n",
       "2004             1.181347e-01                      0.201803   \n",
       "4787             3.768647e-13                      0.088734   \n",
       "284              4.853275e-14                      0.198227   \n",
       "2470             5.399063e-14                      0.160152   \n",
       "\n",
       "       Operating Funds to Liability   Current Liabilities/Liability  \\\n",
       "948                        0.341540                        0.909853   \n",
       "4264                       0.340664                        0.552091   \n",
       "3114                       0.346903                        0.663970   \n",
       "4918                       0.337234                        0.953991   \n",
       "233                        0.344074                        0.375513   \n",
       "...                             ...                             ...   \n",
       "2044                       0.346224                        0.990887   \n",
       "2004                       0.339230                        0.700065   \n",
       "4787                       0.346740                        0.436592   \n",
       "284                        0.340298                        0.863506   \n",
       "2470                       0.335961                        0.449042   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total expense/Assets  \\\n",
       "948                             0.932147               0.039506   \n",
       "4264                            0.908825               0.082677   \n",
       "3114                            0.930413               0.035048   \n",
       "4918                            0.808267               0.223712   \n",
       "233                             0.931610               0.007297   \n",
       "...                                  ...                    ...   \n",
       "2044                            0.923354               0.027422   \n",
       "2004                            0.869388               0.059675   \n",
       "4787                            0.921548               0.026816   \n",
       "284                             0.922503               0.012282   \n",
       "2470                            0.843977               0.126337   \n",
       "\n",
       "       Current Asset Turnover Rate   Quick Asset Turnover Rate  \\\n",
       "948                   2.485980e-14                1.758731e-14   \n",
       "4264                  1.403254e-14                1.493000e-14   \n",
       "3114                  2.265917e-14                1.345656e-14   \n",
       "4918                  6.760000e-01                3.150000e-02   \n",
       "233                   1.394958e-14                8.040000e-01   \n",
       "...                            ...                         ...   \n",
       "2044                  1.281281e-13                7.460000e-01   \n",
       "2004                  6.692571e-14                2.919316e-14   \n",
       "4787                  1.072630e-14                6.540000e-01   \n",
       "284                   1.981049e-14                1.798903e-14   \n",
       "2470                  3.130000e-01                3.890000e-01   \n",
       "\n",
       "       Cash Turnover Rate   Fixed Assets to Assets  \\\n",
       "948                0.4840             2.237006e-11   \n",
       "4264               0.0331             5.586839e-11   \n",
       "3114               0.0760             4.696361e-11   \n",
       "4918               0.0725             1.961944e-11   \n",
       "233                0.3140             8.540841e-11   \n",
       "...                   ...                      ...   \n",
       "2044               0.1660             8.362393e-12   \n",
       "2004               0.0252             8.329596e-11   \n",
       "4787               0.5390             7.421753e-11   \n",
       "284                0.1430             2.899699e-11   \n",
       "2470               0.1450             7.836924e-11   \n",
       "\n",
       "       Equity to Long-term Liability   CFO to Assets  \\\n",
       "948                         0.113409        0.569659   \n",
       "4264                        0.132919        0.564678   \n",
       "3114                        0.116105        0.589539   \n",
       "4918                        0.110933        0.537968   \n",
       "233                         0.125439        0.579259   \n",
       "...                              ...             ...   \n",
       "2044                        0.110933        0.602711   \n",
       "2004                        0.217743        0.552096   \n",
       "4787                        0.141099        0.602053   \n",
       "284                         0.122219        0.562116   \n",
       "2470                        0.000000        0.514097   \n",
       "\n",
       "       Current Liability to Current Assets   Liability-Assets Flag  \\\n",
       "948                               0.039624                     0.0   \n",
       "4264                              0.035221                     0.0   \n",
       "3114                              0.034991                     0.0   \n",
       "4918                              0.047624                     0.0   \n",
       "233                               0.058460                     0.0   \n",
       "...                                    ...                     ...   \n",
       "2044                              0.037485                     0.0   \n",
       "2004                              0.104796                     0.0   \n",
       "4787                              0.039093                     0.0   \n",
       "284                               0.046350                     0.0   \n",
       "2470                              0.193950                     1.0   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price  \\\n",
       "948                      0.791303                1.153670e-13   \n",
       "4264                     0.727838                7.963314e-14   \n",
       "3114                     0.802265                7.819361e-14   \n",
       "4918                     0.420995                5.470571e-14   \n",
       "233                      0.799720                7.364524e-13   \n",
       "...                           ...                         ...   \n",
       "2044                     0.760822                1.464233e-12   \n",
       "2004                     0.718183                8.542125e-14   \n",
       "4787                     0.766274                2.151085e-12   \n",
       "284                      0.818788                5.291934e-13   \n",
       "2470                     0.572881                8.817468e-13   \n",
       "\n",
       "       Liability to Equity   Equity to Liability  Bankrupt?  \n",
       "948               0.284577              0.020014          1  \n",
       "4264              0.286656              0.018316          1  \n",
       "3114              0.279475              0.030427          1  \n",
       "4918              0.304944              0.013430          1  \n",
       "233               0.280124              0.028018          1  \n",
       "...                    ...                   ...        ...  \n",
       "2044              0.291637              0.015941          1  \n",
       "2004              0.359793              0.011369          1  \n",
       "4787              0.286815              0.018210          1  \n",
       "284               0.293720              0.015316          1  \n",
       "2470              0.218785              0.008500          1  \n",
       "\n",
       "[400 rows x 51 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample = TRAIN.loc[upsample_indexes,:]\n",
    "upsample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d104f7",
   "metadata": {},
   "source": [
    "## Clustering to downsize the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96fd3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_bankrupcies =  TRAIN[(TRAIN['Bankrupt?'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534fadfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 2 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5583.759110779421\n",
      "Iteration 1, inertia 3533.564763808211\n",
      "Iteration 2, inertia 3403.981645226996\n",
      "Iteration 3, inertia 3311.0982929387155\n",
      "Iteration 4, inertia 3282.698322643895\n",
      "Iteration 5, inertia 3258.780896448564\n",
      "Iteration 6, inertia 3194.28465493438\n",
      "Iteration 7, inertia 3162.829626415985\n",
      "Iteration 8, inertia 3162.542560612377\n",
      "Iteration 9, inertia 3162.5301412124004\n",
      "Converged at iteration 9: center shift 6.080647703506121e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4611.518259104693\n",
      "Iteration 1, inertia 3487.0147313947396\n",
      "Iteration 2, inertia 3407.478636593823\n",
      "Iteration 3, inertia 3356.4609292919827\n",
      "Iteration 4, inertia 3321.9165634189108\n",
      "Iteration 5, inertia 3283.3583700879376\n",
      "Iteration 6, inertia 3243.5684886335516\n",
      "Iteration 7, inertia 3177.214238400233\n",
      "Iteration 8, inertia 3162.708771668781\n",
      "Iteration 9, inertia 3162.5386374509735\n",
      "Iteration 10, inertia 3162.5301412124004\n",
      "Converged at iteration 10: center shift 6.080647703506302e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5131.139738585605\n",
      "Iteration 1, inertia 3217.1069812080996\n",
      "Iteration 2, inertia 3164.6379068202355\n",
      "Iteration 3, inertia 3162.6076093717684\n",
      "Iteration 4, inertia 3162.532155089156\n",
      "Converged at iteration 4: center shift 9.064975129794005e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6846.219812624432\n",
      "Iteration 1, inertia 3452.1054317676703\n",
      "Iteration 2, inertia 3383.255141944191\n",
      "Iteration 3, inertia 3338.693676191401\n",
      "Iteration 4, inertia 3308.916441350363\n",
      "Iteration 5, inertia 3279.2688773632294\n",
      "Iteration 6, inertia 3217.9217919386438\n",
      "Iteration 7, inertia 3163.500175284106\n",
      "Iteration 8, inertia 3162.549684977041\n",
      "Iteration 9, inertia 3162.5348144372824\n",
      "Iteration 10, inertia 3162.5289323578236\n",
      "Converged at iteration 10: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6100.966623521688\n",
      "Iteration 1, inertia 3262.3879315867825\n",
      "Iteration 2, inertia 3165.56251146614\n",
      "Iteration 3, inertia 3162.5949795464467\n",
      "Iteration 4, inertia 3162.532155089156\n",
      "Converged at iteration 4: center shift 9.064975129794227e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6483.033704540532\n",
      "Iteration 1, inertia 3230.6707903229017\n",
      "Iteration 2, inertia 3164.9659131697354\n",
      "Iteration 3, inertia 3162.6416978977445\n",
      "Iteration 4, inertia 3162.532788980535\n",
      "Iteration 5, inertia 3162.5289323578236\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5093.937799482225\n",
      "Iteration 1, inertia 3526.6979813234043\n",
      "Iteration 2, inertia 3489.6292839468765\n",
      "Iteration 3, inertia 3483.3985392503587\n",
      "Iteration 4, inertia 3482.1042386721206\n",
      "Iteration 5, inertia 3480.8593134072585\n",
      "Iteration 6, inertia 3479.3306462347787\n",
      "Iteration 7, inertia 3477.8418866201428\n",
      "Iteration 8, inertia 3477.6151119318592\n",
      "Iteration 9, inertia 3477.5876177045593\n",
      "Iteration 10, inertia 3477.5787182766285\n",
      "Iteration 11, inertia 3477.5747060197587\n",
      "Iteration 12, inertia 3477.5714427409403\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5548.2715756274965\n",
      "Iteration 1, inertia 3338.7270444470514\n",
      "Iteration 2, inertia 3178.7495541528715\n",
      "Iteration 3, inertia 3162.781723104445\n",
      "Iteration 4, inertia 3162.5325026140704\n",
      "Iteration 5, inertia 3162.5289323578236\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5587.870828138681\n",
      "Iteration 1, inertia 3578.1679119281366\n",
      "Iteration 2, inertia 3492.632818460779\n",
      "Iteration 3, inertia 3399.7192452997374\n",
      "Iteration 4, inertia 3250.165854754679\n",
      "Iteration 5, inertia 3165.517357098311\n",
      "Iteration 6, inertia 3162.6014417331385\n",
      "Iteration 7, inertia 3162.537837748287\n",
      "Iteration 8, inertia 3162.5322029116805\n",
      "Iteration 9, inertia 3162.5289323578236\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5899.416002031669\n",
      "Iteration 1, inertia 3479.717115568052\n",
      "Iteration 2, inertia 3472.9490199523048\n",
      "Iteration 3, inertia 3471.363056208041\n",
      "Iteration 4, inertia 3470.5097696879598\n",
      "Iteration 5, inertia 3470.1785040413265\n",
      "Iteration 6, inertia 3470.089963858123\n",
      "Iteration 7, inertia 3470.0389977776967\n",
      "Iteration 8, inertia 3470.010705789675\n",
      "Iteration 9, inertia 3469.9684693207437\n",
      "Iteration 10, inertia 3469.962775888208\n",
      "Iteration 11, inertia 3469.9521017913867\n",
      "Iteration 12, inertia 3469.9398839979312\n",
      "Iteration 13, inertia 3469.928981940816\n",
      "Iteration 14, inertia 3469.8959811097043\n",
      "Iteration 15, inertia 3469.853284091702\n",
      "Iteration 16, inertia 3469.808857078679\n",
      "Iteration 17, inertia 3469.780749129086\n",
      "Iteration 18, inertia 3469.7774956216035\n",
      "Iteration 19, inertia 3469.773049729527\n",
      "Iteration 20, inertia 3469.7674974282327\n",
      "Iteration 21, inertia 3469.7608573942052\n",
      "Iteration 22, inertia 3469.7570685992177\n",
      "Iteration 23, inertia 3469.7494053046507\n",
      "Iteration 24, inertia 3469.734121683319\n",
      "Iteration 25, inertia 3469.72502711812\n",
      "Converged at iteration 25: center shift 8.013609028079521e-07 within tolerance 1.436774592781648e-06.\n",
      "Training a K-Means model with 3 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4360.945152146891\n",
      "Iteration 1, inertia 3242.901986210149\n",
      "Iteration 2, inertia 3178.786881291436\n",
      "Iteration 3, inertia 3157.112090301227\n",
      "Iteration 4, inertia 3136.311768072605\n",
      "Iteration 5, inertia 3113.4473378343546\n",
      "Iteration 6, inertia 3086.537956559069\n",
      "Iteration 7, inertia 3054.4058552474676\n",
      "Iteration 8, inertia 3021.2623641033206\n",
      "Iteration 9, inertia 2985.951605759749\n",
      "Iteration 10, inertia 2913.557461892977\n",
      "Iteration 11, inertia 2868.4025741517917\n",
      "Iteration 12, inertia 2866.5922470688797\n",
      "Iteration 13, inertia 2866.2354880407465\n",
      "Iteration 14, inertia 2866.1828596444093\n",
      "Iteration 15, inertia 2866.16022599571\n",
      "Iteration 16, inertia 2866.1539949389194\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4502.786373624235\n",
      "Iteration 1, inertia 3037.2337470731445\n",
      "Iteration 2, inertia 2991.3829419808376\n",
      "Iteration 3, inertia 2969.998664637297\n",
      "Iteration 4, inertia 2955.8181745920224\n",
      "Iteration 5, inertia 2940.18436587582\n",
      "Iteration 6, inertia 2928.9312942200427\n",
      "Iteration 7, inertia 2927.2552263143075\n",
      "Iteration 8, inertia 2927.172770739015\n",
      "Iteration 9, inertia 2927.164377935458\n",
      "Converged at iteration 9: center shift 1.3399510472448665e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4042.0879532539207\n",
      "Iteration 1, inertia 3009.2914074197315\n",
      "Iteration 2, inertia 2883.521825467388\n",
      "Iteration 3, inertia 2864.837433832802\n",
      "Iteration 4, inertia 2864.0233219295064\n",
      "Iteration 5, inertia 2863.996407447979\n",
      "Converged at iteration 5: center shift 1.2578318596792564e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4036.352169196559\n",
      "Iteration 1, inertia 2944.4261389098824\n",
      "Iteration 2, inertia 2897.475604016927\n",
      "Iteration 3, inertia 2878.633347257326\n",
      "Iteration 4, inertia 2863.4173435603916\n",
      "Iteration 5, inertia 2853.9028581273924\n",
      "Iteration 6, inertia 2848.6189384728323\n",
      "Iteration 7, inertia 2846.776555125025\n",
      "Iteration 8, inertia 2845.471795246017\n",
      "Iteration 9, inertia 2844.781424670749\n",
      "Iteration 10, inertia 2844.0295999100963\n",
      "Iteration 11, inertia 2843.23853337309\n",
      "Iteration 12, inertia 2842.5051304386943\n",
      "Iteration 13, inertia 2841.8972400743496\n",
      "Iteration 14, inertia 2841.3967698265833\n",
      "Iteration 15, inertia 2840.74292033508\n",
      "Iteration 16, inertia 2840.112537838657\n",
      "Iteration 17, inertia 2839.4049135616615\n",
      "Iteration 18, inertia 2838.890929110134\n",
      "Iteration 19, inertia 2838.4803449684996\n",
      "Iteration 20, inertia 2837.960338574152\n",
      "Iteration 21, inertia 2837.6204626729254\n",
      "Iteration 22, inertia 2837.3170536112443\n",
      "Iteration 23, inertia 2837.0344933109463\n",
      "Iteration 24, inertia 2836.87834367272\n",
      "Iteration 25, inertia 2836.8335228799792\n",
      "Iteration 26, inertia 2836.8240463385605\n",
      "Iteration 27, inertia 2836.8217665280395\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4012.94228219063\n",
      "Iteration 1, inertia 2869.9909350058565\n",
      "Iteration 2, inertia 2841.359942942615\n",
      "Iteration 3, inertia 2837.6173665322067\n",
      "Iteration 4, inertia 2837.068805980106\n",
      "Iteration 5, inertia 2837.0016944576078\n",
      "Iteration 6, inertia 2836.946621613818\n",
      "Iteration 7, inertia 2836.861768511205\n",
      "Iteration 8, inertia 2836.8322441407136\n",
      "Converged at iteration 8: center shift 7.076189425466053e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4792.49991825099\n",
      "Iteration 1, inertia 3156.6745140457047\n",
      "Iteration 2, inertia 3078.549646462163\n",
      "Iteration 3, inertia 3032.5232217756156\n",
      "Iteration 4, inertia 2990.222266282201\n",
      "Iteration 5, inertia 2914.4579433342055\n",
      "Iteration 6, inertia 2868.559989826499\n",
      "Iteration 7, inertia 2866.595717206953\n",
      "Iteration 8, inertia 2866.2354880407465\n",
      "Iteration 9, inertia 2866.1828596444093\n",
      "Iteration 10, inertia 2866.16022599571\n",
      "Iteration 11, inertia 2866.1539949389194\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4361.917941739874\n",
      "Iteration 1, inertia 3061.590429077545\n",
      "Iteration 2, inertia 3024.597387795079\n",
      "Iteration 3, inertia 2964.7242661655964\n",
      "Iteration 4, inertia 2928.4740298040347\n",
      "Iteration 5, inertia 2917.458765704956\n",
      "Iteration 6, inertia 2905.0830217580615\n",
      "Iteration 7, inertia 2889.301067597241\n",
      "Iteration 8, inertia 2873.58594864518\n",
      "Iteration 9, inertia 2860.951093088865\n",
      "Iteration 10, inertia 2853.140490910617\n",
      "Iteration 11, inertia 2848.8928065924993\n",
      "Iteration 12, inertia 2847.1656777937383\n",
      "Iteration 13, inertia 2846.156915861478\n",
      "Iteration 14, inertia 2845.372850939508\n",
      "Iteration 15, inertia 2844.7489194145433\n",
      "Iteration 16, inertia 2843.897045118421\n",
      "Iteration 17, inertia 2843.144412945183\n",
      "Iteration 18, inertia 2842.3760448088688\n",
      "Iteration 19, inertia 2841.8215722404575\n",
      "Iteration 20, inertia 2841.3466989708477\n",
      "Iteration 21, inertia 2840.7130305967835\n",
      "Iteration 22, inertia 2840.0916224272337\n",
      "Iteration 23, inertia 2839.3877181389735\n",
      "Iteration 24, inertia 2838.864115061917\n",
      "Iteration 25, inertia 2838.4803449685005\n",
      "Iteration 26, inertia 2837.960338574152\n",
      "Iteration 27, inertia 2837.6204626729254\n",
      "Iteration 28, inertia 2837.3170536112443\n",
      "Iteration 29, inertia 2837.0344933109463\n",
      "Iteration 30, inertia 2836.87834367272\n",
      "Iteration 31, inertia 2836.8335228799792\n",
      "Iteration 32, inertia 2836.8240463385605\n",
      "Iteration 33, inertia 2836.8217665280395\n",
      "Converged at iteration 33: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4886.80016345397\n",
      "Iteration 1, inertia 3052.853138340789\n",
      "Iteration 2, inertia 2930.4809838564897\n",
      "Iteration 3, inertia 2866.966577694293\n",
      "Iteration 4, inertia 2864.1662348783584\n",
      "Iteration 5, inertia 2864.016632777515\n",
      "Iteration 6, inertia 2864.0019826662897\n",
      "Converged at iteration 6: center shift 2.8852511051149583e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4271.21303482935\n",
      "Iteration 1, inertia 2990.677962889572\n",
      "Iteration 2, inertia 2883.132732676497\n",
      "Iteration 3, inertia 2864.0200749205837\n",
      "Iteration 4, inertia 2857.453131266865\n",
      "Iteration 5, inertia 2855.139212767595\n",
      "Iteration 6, inertia 2853.882740513476\n",
      "Iteration 7, inertia 2853.2374549054475\n",
      "Iteration 8, inertia 2852.916379010995\n",
      "Iteration 9, inertia 2852.6364920582328\n",
      "Iteration 10, inertia 2852.4117578043833\n",
      "Iteration 11, inertia 2852.028004605777\n",
      "Iteration 12, inertia 2851.7143023020485\n",
      "Iteration 13, inertia 2851.469061571659\n",
      "Iteration 14, inertia 2851.2492323400884\n",
      "Iteration 15, inertia 2851.041693899972\n",
      "Iteration 16, inertia 2850.8832175590137\n",
      "Iteration 17, inertia 2850.7974748225092\n",
      "Iteration 18, inertia 2850.7454653363393\n",
      "Iteration 19, inertia 2850.6307122181965\n",
      "Iteration 20, inertia 2850.5561729130186\n",
      "Iteration 21, inertia 2850.501206965712\n",
      "Iteration 22, inertia 2850.442368745204\n",
      "Iteration 23, inertia 2850.4122473044918\n",
      "Iteration 24, inertia 2850.3801123969247\n",
      "Iteration 25, inertia 2850.364684760199\n",
      "Iteration 26, inertia 2850.3537052023476\n",
      "Iteration 27, inertia 2850.33179630757\n",
      "Iteration 28, inertia 2850.302657578942\n",
      "Iteration 29, inertia 2850.272387537454\n",
      "Iteration 30, inertia 2850.2213831793747\n",
      "Iteration 31, inertia 2850.173412706081\n",
      "Iteration 32, inertia 2850.1202323060215\n",
      "Iteration 33, inertia 2850.0399665988743\n",
      "Iteration 34, inertia 2849.9698390424983\n",
      "Iteration 35, inertia 2849.9373176562303\n",
      "Iteration 36, inertia 2849.8949867833653\n",
      "Iteration 37, inertia 2849.850052658723\n",
      "Iteration 38, inertia 2849.7838708359277\n",
      "Iteration 39, inertia 2849.753305891661\n",
      "Iteration 40, inertia 2849.6570262850364\n",
      "Iteration 41, inertia 2849.497463636758\n",
      "Iteration 42, inertia 2849.371825458752\n",
      "Iteration 43, inertia 2849.242751330587\n",
      "Iteration 44, inertia 2849.037428827264\n",
      "Iteration 45, inertia 2848.7278901510635\n",
      "Iteration 46, inertia 2848.4825557447357\n",
      "Iteration 47, inertia 2848.039488069114\n",
      "Iteration 48, inertia 2847.545150942887\n",
      "Iteration 49, inertia 2847.2042002871726\n",
      "Iteration 50, inertia 2846.8121777392457\n",
      "Iteration 51, inertia 2846.324420580577\n",
      "Iteration 52, inertia 2845.5172517225255\n",
      "Iteration 53, inertia 2844.737090218948\n",
      "Iteration 54, inertia 2843.756876557088\n",
      "Iteration 55, inertia 2842.9774829819953\n",
      "Iteration 56, inertia 2842.2347632348824\n",
      "Iteration 57, inertia 2841.7418517689653\n",
      "Iteration 58, inertia 2841.240235921552\n",
      "Iteration 59, inertia 2840.6342379688663\n",
      "Iteration 60, inertia 2839.9443428903032\n",
      "Iteration 61, inertia 2839.196072955017\n",
      "Iteration 62, inertia 2838.8096623291754\n",
      "Iteration 63, inertia 2838.3996887815692\n",
      "Iteration 64, inertia 2837.9239485553176\n",
      "Iteration 65, inertia 2837.6031286944963\n",
      "Iteration 66, inertia 2837.2973049557595\n",
      "Iteration 67, inertia 2837.005940306114\n",
      "Iteration 68, inertia 2836.87834367272\n",
      "Iteration 69, inertia 2836.8335228799792\n",
      "Iteration 70, inertia 2836.8240463385605\n",
      "Iteration 71, inertia 2836.8217665280395\n",
      "Converged at iteration 71: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4293.771184458321\n",
      "Iteration 1, inertia 3230.992070517155\n",
      "Iteration 2, inertia 3134.117452495153\n",
      "Iteration 3, inertia 3095.1070637953653\n",
      "Iteration 4, inertia 3050.5818631264283\n",
      "Iteration 5, inertia 2993.9621800790273\n",
      "Iteration 6, inertia 2953.8780341031065\n",
      "Iteration 7, inertia 2917.6520878451997\n",
      "Iteration 8, inertia 2851.0499117163417\n",
      "Iteration 9, inertia 2837.0714072537467\n",
      "Iteration 10, inertia 2836.8671131394412\n",
      "Iteration 11, inertia 2836.8277028092584\n",
      "Iteration 12, inertia 2836.8198610569016\n",
      "Converged at iteration 12: strict convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 4 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3655.5261031762066\n",
      "Iteration 1, inertia 2719.6468945162396\n",
      "Iteration 2, inertia 2655.612004990521\n",
      "Iteration 3, inertia 2640.012745129216\n",
      "Iteration 4, inertia 2628.801274973046\n",
      "Iteration 5, inertia 2617.5338787647593\n",
      "Iteration 6, inertia 2609.254578513591\n",
      "Iteration 7, inertia 2602.3099785624804\n",
      "Iteration 8, inertia 2596.5764941179764\n",
      "Iteration 9, inertia 2590.847181425355\n",
      "Iteration 10, inertia 2585.9438968402596\n",
      "Iteration 11, inertia 2582.4347117261564\n",
      "Iteration 12, inertia 2580.814274543623\n",
      "Iteration 13, inertia 2580.1466195025396\n",
      "Iteration 14, inertia 2579.880497934027\n",
      "Iteration 15, inertia 2579.7663374711897\n",
      "Iteration 16, inertia 2579.7022517123796\n",
      "Iteration 17, inertia 2579.673500430376\n",
      "Iteration 18, inertia 2579.6677890366204\n",
      "Iteration 19, inertia 2579.661529128022\n",
      "Iteration 20, inertia 2579.6596793871563\n",
      "Converged at iteration 20: center shift 1.290934375708767e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4247.054382320409\n",
      "Iteration 1, inertia 2870.3124304170324\n",
      "Iteration 2, inertia 2786.784121717853\n",
      "Iteration 3, inertia 2733.203984347989\n",
      "Iteration 4, inertia 2628.720647007617\n",
      "Iteration 5, inertia 2585.0150842080907\n",
      "Iteration 6, inertia 2580.44623209723\n",
      "Iteration 7, inertia 2579.9334296325455\n",
      "Iteration 8, inertia 2579.860864327781\n",
      "Iteration 9, inertia 2579.8422681502752\n",
      "Iteration 10, inertia 2579.839762145161\n",
      "Converged at iteration 10: center shift 5.224469000803139e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3688.089149920709\n",
      "Iteration 1, inertia 2698.878752146252\n",
      "Iteration 2, inertia 2640.476730736671\n",
      "Iteration 3, inertia 2630.425690872748\n",
      "Iteration 4, inertia 2627.760642173388\n",
      "Iteration 5, inertia 2626.0138862815893\n",
      "Iteration 6, inertia 2624.383408611292\n",
      "Iteration 7, inertia 2623.0696297312234\n",
      "Iteration 8, inertia 2621.366651542129\n",
      "Iteration 9, inertia 2618.89667064788\n",
      "Iteration 10, inertia 2614.561466338778\n",
      "Iteration 11, inertia 2608.5901674320003\n",
      "Iteration 12, inertia 2601.584162784345\n",
      "Iteration 13, inertia 2594.994565141985\n",
      "Iteration 14, inertia 2589.345426642081\n",
      "Iteration 15, inertia 2584.6630446570334\n",
      "Iteration 16, inertia 2581.7401951596153\n",
      "Iteration 17, inertia 2580.5555112484917\n",
      "Iteration 18, inertia 2580.0923009187927\n",
      "Iteration 19, inertia 2579.8684805851954\n",
      "Iteration 20, inertia 2579.7663374711897\n",
      "Iteration 21, inertia 2579.7022517123796\n",
      "Iteration 22, inertia 2579.673500430376\n",
      "Iteration 23, inertia 2579.6677890366204\n",
      "Iteration 24, inertia 2579.661529128022\n",
      "Iteration 25, inertia 2579.6596793871563\n",
      "Converged at iteration 25: center shift 1.290934375708783e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3445.1911566858307\n",
      "Iteration 1, inertia 2670.6996329977887\n",
      "Iteration 2, inertia 2625.598938207861\n",
      "Iteration 3, inertia 2594.2427724235968\n",
      "Iteration 4, inertia 2580.5114944086226\n",
      "Iteration 5, inertia 2579.7098308457407\n",
      "Iteration 6, inertia 2579.6701547977214\n",
      "Iteration 7, inertia 2579.650162148408\n",
      "Converged at iteration 7: center shift 9.787863109649022e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3991.6019960530457\n",
      "Iteration 1, inertia 2773.871708113777\n",
      "Iteration 2, inertia 2701.412415539991\n",
      "Iteration 3, inertia 2646.6881914061673\n",
      "Iteration 4, inertia 2593.011368394803\n",
      "Iteration 5, inertia 2585.727437208789\n",
      "Iteration 6, inertia 2585.3146049575316\n",
      "Iteration 7, inertia 2585.147277632443\n",
      "Iteration 8, inertia 2585.1240716045645\n",
      "Iteration 9, inertia 2585.1200836882867\n",
      "Converged at iteration 9: center shift 1.1031423656272376e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4068.643675861366\n",
      "Iteration 1, inertia 2740.562349414682\n",
      "Iteration 2, inertia 2618.1352524595636\n",
      "Iteration 3, inertia 2596.7231210074606\n",
      "Iteration 4, inertia 2590.0364571263485\n",
      "Iteration 5, inertia 2586.2459176162\n",
      "Iteration 6, inertia 2584.603871167688\n",
      "Iteration 7, inertia 2583.2621557558828\n",
      "Iteration 8, inertia 2582.1351343745378\n",
      "Iteration 9, inertia 2581.2202362516064\n",
      "Iteration 10, inertia 2580.4969382329095\n",
      "Iteration 11, inertia 2580.039461671715\n",
      "Iteration 12, inertia 2579.9302428567753\n",
      "Iteration 13, inertia 2579.8924968273045\n",
      "Iteration 14, inertia 2579.872631411867\n",
      "Iteration 15, inertia 2579.849457001519\n",
      "Iteration 16, inertia 2579.8388829804308\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4716.1345017304975\n",
      "Iteration 1, inertia 2887.8926804801727\n",
      "Iteration 2, inertia 2705.321289743986\n",
      "Iteration 3, inertia 2671.8012226360097\n",
      "Iteration 4, inertia 2657.154170602414\n",
      "Iteration 5, inertia 2642.344925795526\n",
      "Iteration 6, inertia 2629.518421113954\n",
      "Iteration 7, inertia 2626.544825949665\n",
      "Iteration 8, inertia 2626.241234252975\n",
      "Iteration 9, inertia 2626.2026021035904\n",
      "Iteration 10, inertia 2626.1994647889037\n",
      "Converged at iteration 10: center shift 1.2848011882789335e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4279.8243391455835\n",
      "Iteration 1, inertia 2811.168149022854\n",
      "Iteration 2, inertia 2707.512794785977\n",
      "Iteration 3, inertia 2697.7157100839195\n",
      "Iteration 4, inertia 2692.46465413691\n",
      "Iteration 5, inertia 2689.9282866872727\n",
      "Iteration 6, inertia 2687.559787249241\n",
      "Iteration 7, inertia 2685.398543679692\n",
      "Iteration 8, inertia 2683.6886700674772\n",
      "Iteration 9, inertia 2682.3570681728634\n",
      "Iteration 10, inertia 2680.915415533183\n",
      "Iteration 11, inertia 2679.4618310482833\n",
      "Iteration 12, inertia 2678.199829412208\n",
      "Iteration 13, inertia 2677.096827826506\n",
      "Iteration 14, inertia 2675.7326400682296\n",
      "Iteration 15, inertia 2674.4695336484338\n",
      "Iteration 16, inertia 2673.200277044369\n",
      "Iteration 17, inertia 2671.963998034464\n",
      "Iteration 18, inertia 2670.2271746118213\n",
      "Iteration 19, inertia 2668.1742365638524\n",
      "Iteration 20, inertia 2665.877604947996\n",
      "Iteration 21, inertia 2663.154126648398\n",
      "Iteration 22, inertia 2659.344070514308\n",
      "Iteration 23, inertia 2652.3405138178455\n",
      "Iteration 24, inertia 2637.8615578431636\n",
      "Iteration 25, inertia 2623.973482938068\n",
      "Iteration 26, inertia 2610.9728216663884\n",
      "Iteration 27, inertia 2599.8375889754916\n",
      "Iteration 28, inertia 2587.851435353401\n",
      "Iteration 29, inertia 2581.021767807272\n",
      "Iteration 30, inertia 2580.050455697267\n",
      "Iteration 31, inertia 2579.874034445749\n",
      "Iteration 32, inertia 2579.8448258741437\n",
      "Iteration 33, inertia 2579.8392669303503\n",
      "Converged at iteration 33: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4054.171718682316\n",
      "Iteration 1, inertia 2681.0355742414536\n",
      "Iteration 2, inertia 2604.6424754612417\n",
      "Iteration 3, inertia 2594.555733216022\n",
      "Iteration 4, inertia 2591.440547798008\n",
      "Iteration 5, inertia 2588.3101303048848\n",
      "Iteration 6, inertia 2586.065116268693\n",
      "Iteration 7, inertia 2584.502038834767\n",
      "Iteration 8, inertia 2583.069165513929\n",
      "Iteration 9, inertia 2582.0624050527695\n",
      "Iteration 10, inertia 2581.1639938614553\n",
      "Iteration 11, inertia 2580.4815819082232\n",
      "Iteration 12, inertia 2580.0412916751766\n",
      "Iteration 13, inertia 2579.931919034049\n",
      "Iteration 14, inertia 2579.8924968273045\n",
      "Iteration 15, inertia 2579.872631411867\n",
      "Iteration 16, inertia 2579.849457001519\n",
      "Iteration 17, inertia 2579.8388829804303\n",
      "Converged at iteration 17: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4215.630058425546\n",
      "Iteration 1, inertia 2949.6334382601967\n",
      "Iteration 2, inertia 2874.7492717906202\n",
      "Iteration 3, inertia 2844.368428153375\n",
      "Iteration 4, inertia 2793.758151847751\n",
      "Iteration 5, inertia 2724.656915241288\n",
      "Iteration 6, inertia 2689.820606673304\n",
      "Iteration 7, inertia 2683.5203470044335\n",
      "Iteration 8, inertia 2682.361051648041\n",
      "Iteration 9, inertia 2681.8078765264913\n",
      "Iteration 10, inertia 2681.6663763410475\n",
      "Iteration 11, inertia 2681.607324667931\n",
      "Iteration 12, inertia 2681.5918120425977\n",
      "Iteration 13, inertia 2681.5898599970355\n",
      "Converged at iteration 13: strict convergence.\n",
      "Training a K-Means model with 5 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3345.5145828727827\n",
      "Iteration 1, inertia 2523.9944086798378\n",
      "Iteration 2, inertia 2455.672356530813\n",
      "Iteration 3, inertia 2441.8397298367954\n",
      "Iteration 4, inertia 2431.172373816612\n",
      "Iteration 5, inertia 2419.5766225839375\n",
      "Iteration 6, inertia 2407.5995274467787\n",
      "Iteration 7, inertia 2398.0852721851197\n",
      "Iteration 8, inertia 2390.810109794238\n",
      "Iteration 9, inertia 2384.4209008313724\n",
      "Iteration 10, inertia 2379.4901591205335\n",
      "Iteration 11, inertia 2376.634584671237\n",
      "Iteration 12, inertia 2375.533606248264\n",
      "Iteration 13, inertia 2375.1817053685145\n",
      "Iteration 14, inertia 2374.9853429623777\n",
      "Iteration 15, inertia 2374.8920403890206\n",
      "Iteration 16, inertia 2374.8493382082675\n",
      "Iteration 17, inertia 2374.8299523572236\n",
      "Iteration 18, inertia 2374.8155873893134\n",
      "Converged at iteration 18: center shift 5.528421092051193e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3827.526155808725\n",
      "Iteration 1, inertia 2624.6834505328784\n",
      "Iteration 2, inertia 2521.3668522906564\n",
      "Iteration 3, inertia 2468.6998014468254\n",
      "Iteration 4, inertia 2444.971242254934\n",
      "Iteration 5, inertia 2432.1100711642503\n",
      "Iteration 6, inertia 2420.816670894237\n",
      "Iteration 7, inertia 2402.757552665441\n",
      "Iteration 8, inertia 2378.5793141212966\n",
      "Iteration 9, inertia 2361.4109679240596\n",
      "Iteration 10, inertia 2356.137111397352\n",
      "Iteration 11, inertia 2352.1321539004553\n",
      "Iteration 12, inertia 2346.671737824414\n",
      "Iteration 13, inertia 2339.8413227612264\n",
      "Iteration 14, inertia 2335.066186639181\n",
      "Iteration 15, inertia 2333.967945205999\n",
      "Iteration 16, inertia 2333.704253950342\n",
      "Iteration 17, inertia 2333.670851619946\n",
      "Iteration 18, inertia 2333.6663605117237\n",
      "Iteration 19, inertia 2333.6614845303757\n",
      "Iteration 20, inertia 2333.6597162203852\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3608.455547861904\n",
      "Iteration 1, inertia 2538.0958370860785\n",
      "Iteration 2, inertia 2431.123014627876\n",
      "Iteration 3, inertia 2370.1577389428035\n",
      "Iteration 4, inertia 2336.0546082113497\n",
      "Iteration 5, inertia 2333.79249711365\n",
      "Iteration 6, inertia 2333.6314436560283\n",
      "Iteration 7, inertia 2333.620876936684\n",
      "Converged at iteration 7: center shift 1.036223752377195e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3546.053060852206\n",
      "Iteration 1, inertia 2604.2920860996046\n",
      "Iteration 2, inertia 2535.0878080499374\n",
      "Iteration 3, inertia 2520.3534551933767\n",
      "Iteration 4, inertia 2495.558329579305\n",
      "Iteration 5, inertia 2447.965070991515\n",
      "Iteration 6, inertia 2384.470115493541\n",
      "Iteration 7, inertia 2336.268088544437\n",
      "Iteration 8, inertia 2333.8252018257344\n",
      "Iteration 9, inertia 2333.628665752648\n",
      "Iteration 10, inertia 2333.617327065157\n",
      "Converged at iteration 10: center shift 1.0362387083890718e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3860.980959538557\n",
      "Iteration 1, inertia 2429.1038189254805\n",
      "Iteration 2, inertia 2386.2518445092983\n",
      "Iteration 3, inertia 2379.017029726671\n",
      "Iteration 4, inertia 2374.168569902039\n",
      "Iteration 5, inertia 2369.5412269716808\n",
      "Iteration 6, inertia 2364.8534487545994\n",
      "Iteration 7, inertia 2360.4450783339144\n",
      "Iteration 8, inertia 2356.596022512057\n",
      "Iteration 9, inertia 2352.344042512638\n",
      "Iteration 10, inertia 2348.8626674824554\n",
      "Iteration 11, inertia 2346.6461496011993\n",
      "Iteration 12, inertia 2344.836662492775\n",
      "Iteration 13, inertia 2342.843246200424\n",
      "Iteration 14, inertia 2341.2803551955967\n",
      "Iteration 15, inertia 2339.846287093545\n",
      "Iteration 16, inertia 2338.7281014986693\n",
      "Iteration 17, inertia 2337.1990138280767\n",
      "Iteration 18, inertia 2335.2719084385126\n",
      "Iteration 19, inertia 2334.207078467806\n",
      "Iteration 20, inertia 2333.4727729849105\n",
      "Iteration 21, inertia 2332.6975215985553\n",
      "Iteration 22, inertia 2332.0928951343344\n",
      "Iteration 23, inertia 2331.71648254566\n",
      "Iteration 24, inertia 2331.4766689563758\n",
      "Iteration 25, inertia 2331.222597259704\n",
      "Iteration 26, inertia 2331.051284537438\n",
      "Iteration 27, inertia 2330.9089388611724\n",
      "Iteration 28, inertia 2330.8814956672372\n",
      "Iteration 29, inertia 2330.8698336237394\n",
      "Iteration 30, inertia 2330.8602016374316\n",
      "Iteration 31, inertia 2330.8581124317498\n",
      "Converged at iteration 31: center shift 4.770811103865608e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3425.8418997767867\n",
      "Iteration 1, inertia 2430.924233883384\n",
      "Iteration 2, inertia 2352.7954770142696\n",
      "Iteration 3, inertia 2342.803922394485\n",
      "Iteration 4, inertia 2337.9161315722104\n",
      "Iteration 5, inertia 2335.3808095245954\n",
      "Iteration 6, inertia 2334.346630296548\n",
      "Iteration 7, inertia 2334.003969029579\n",
      "Iteration 8, inertia 2333.87921473988\n",
      "Iteration 9, inertia 2333.814959370314\n",
      "Iteration 10, inertia 2333.7860941291337\n",
      "Iteration 11, inertia 2333.746603269698\n",
      "Iteration 12, inertia 2333.726928515412\n",
      "Iteration 13, inertia 2333.7157320252363\n",
      "Iteration 14, inertia 2333.6836237721855\n",
      "Iteration 15, inertia 2333.6725192210934\n",
      "Iteration 16, inertia 2333.670014965764\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3548.0861384010695\n",
      "Iteration 1, inertia 2586.2489421330442\n",
      "Iteration 2, inertia 2479.9851804744253\n",
      "Iteration 3, inertia 2442.352781079589\n",
      "Iteration 4, inertia 2403.9378003752818\n",
      "Iteration 5, inertia 2372.0745679038596\n",
      "Iteration 6, inertia 2356.5861277626727\n",
      "Iteration 7, inertia 2349.201678577753\n",
      "Iteration 8, inertia 2345.839024406384\n",
      "Iteration 9, inertia 2343.975326342683\n",
      "Iteration 10, inertia 2342.1405235139378\n",
      "Iteration 11, inertia 2340.5598724405727\n",
      "Iteration 12, inertia 2339.3837636891626\n",
      "Iteration 13, inertia 2337.991724666433\n",
      "Iteration 14, inertia 2336.024892904386\n",
      "Iteration 15, inertia 2334.7743856234947\n",
      "Iteration 16, inertia 2333.998896954385\n",
      "Iteration 17, inertia 2333.103908787726\n",
      "Iteration 18, inertia 2332.355687794738\n",
      "Iteration 19, inertia 2331.8592593054636\n",
      "Iteration 20, inertia 2331.554934492347\n",
      "Iteration 21, inertia 2331.3016920569753\n",
      "Iteration 22, inertia 2331.1202890897566\n",
      "Iteration 23, inertia 2330.9331119428125\n",
      "Iteration 24, inertia 2330.889688041017\n",
      "Iteration 25, inertia 2330.8698336237394\n",
      "Iteration 26, inertia 2330.8602016374316\n",
      "Iteration 27, inertia 2330.8581124317498\n",
      "Converged at iteration 27: center shift 4.770811103865784e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3716.5578700681435\n",
      "Iteration 1, inertia 2599.1678838821863\n",
      "Iteration 2, inertia 2496.59739787326\n",
      "Iteration 3, inertia 2465.156124260412\n",
      "Iteration 4, inertia 2457.3041008337586\n",
      "Iteration 5, inertia 2454.2663909344706\n",
      "Iteration 6, inertia 2452.2806574335264\n",
      "Iteration 7, inertia 2450.5586890309783\n",
      "Iteration 8, inertia 2448.263587172677\n",
      "Iteration 9, inertia 2444.278634682615\n",
      "Iteration 10, inertia 2434.630732823303\n",
      "Iteration 11, inertia 2401.964257627713\n",
      "Iteration 12, inertia 2369.477299537828\n",
      "Iteration 13, inertia 2366.1966916400984\n",
      "Iteration 14, inertia 2365.9144818492273\n",
      "Iteration 15, inertia 2365.849049462954\n",
      "Iteration 16, inertia 2365.845632109532\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3531.300041745852\n",
      "Iteration 1, inertia 2688.4084225141874\n",
      "Iteration 2, inertia 2594.6704681209376\n",
      "Iteration 3, inertia 2561.0748693369474\n",
      "Iteration 4, inertia 2535.6664241661206\n",
      "Iteration 5, inertia 2522.7333280150533\n",
      "Iteration 6, inertia 2514.9406156974806\n",
      "Iteration 7, inertia 2509.04069777391\n",
      "Iteration 8, inertia 2501.262079504078\n",
      "Iteration 9, inertia 2486.2769331438744\n",
      "Iteration 10, inertia 2457.1097088752904\n",
      "Iteration 11, inertia 2436.017081398733\n",
      "Iteration 12, inertia 2433.14856462221\n",
      "Iteration 13, inertia 2429.7950084430568\n",
      "Iteration 14, inertia 2422.2975458537926\n",
      "Iteration 15, inertia 2408.130905725472\n",
      "Iteration 16, inertia 2392.1766750569154\n",
      "Iteration 17, inertia 2380.2543898374997\n",
      "Iteration 18, inertia 2371.088065395702\n",
      "Iteration 19, inertia 2364.0154589931217\n",
      "Iteration 20, inertia 2358.124722613964\n",
      "Iteration 21, inertia 2352.3833188836948\n",
      "Iteration 22, inertia 2347.8900686580305\n",
      "Iteration 23, inertia 2344.588575599578\n",
      "Iteration 24, inertia 2340.6570234826054\n",
      "Iteration 25, inertia 2337.5793259753095\n",
      "Iteration 26, inertia 2335.44230898737\n",
      "Iteration 27, inertia 2334.401498074414\n",
      "Iteration 28, inertia 2334.003969029579\n",
      "Iteration 29, inertia 2333.87921473988\n",
      "Iteration 30, inertia 2333.814959370314\n",
      "Iteration 31, inertia 2333.7860941291337\n",
      "Iteration 32, inertia 2333.746603269698\n",
      "Iteration 33, inertia 2333.726928515412\n",
      "Iteration 34, inertia 2333.7157320252363\n",
      "Iteration 35, inertia 2333.6836237721855\n",
      "Iteration 36, inertia 2333.6725192210934\n",
      "Iteration 37, inertia 2333.670014965764\n",
      "Converged at iteration 37: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3708.6051920004884\n",
      "Iteration 1, inertia 2551.248111611845\n",
      "Iteration 2, inertia 2409.5803869032975\n",
      "Iteration 3, inertia 2389.4980972927196\n",
      "Iteration 4, inertia 2384.8407464098377\n",
      "Iteration 5, inertia 2380.249844606226\n",
      "Iteration 6, inertia 2375.908365202251\n",
      "Iteration 7, inertia 2371.1144929541288\n",
      "Iteration 8, inertia 2366.500816561099\n",
      "Iteration 9, inertia 2361.4712946233485\n",
      "Iteration 10, inertia 2357.3447963255176\n",
      "Iteration 11, inertia 2353.070430282765\n",
      "Iteration 12, inertia 2349.152509196685\n",
      "Iteration 13, inertia 2346.8458206460323\n",
      "Iteration 14, inertia 2345.2338455359222\n",
      "Iteration 15, inertia 2343.4094151366226\n",
      "Iteration 16, inertia 2341.50937070414\n",
      "Iteration 17, inertia 2340.115009184029\n",
      "Iteration 18, inertia 2339.0241450843932\n",
      "Iteration 19, inertia 2337.51831722733\n",
      "Iteration 20, inertia 2335.5665719579065\n",
      "Iteration 21, inertia 2334.408462535494\n",
      "Iteration 22, inertia 2333.6696056777464\n",
      "Iteration 23, inertia 2332.8927199268796\n",
      "Iteration 24, inertia 2332.1999460358575\n",
      "Iteration 25, inertia 2331.776872253044\n",
      "Iteration 26, inertia 2331.522594795876\n",
      "Iteration 27, inertia 2331.285902265075\n",
      "Iteration 28, inertia 2331.102099744675\n",
      "Iteration 29, inertia 2330.92214564521\n",
      "Iteration 30, inertia 2330.8881412744668\n",
      "Iteration 31, inertia 2330.8727139208563\n",
      "Iteration 32, inertia 2330.864083440868\n",
      "Iteration 33, inertia 2330.8581124317493\n",
      "Converged at iteration 33: center shift 4.770811103865907e-07 within tolerance 1.436774592781648e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 6 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2940.885463448231\n",
      "Iteration 1, inertia 2248.7890060252284\n",
      "Iteration 2, inertia 2197.5880826158636\n",
      "Iteration 3, inertia 2186.7697992397807\n",
      "Iteration 4, inertia 2177.5916787140577\n",
      "Iteration 5, inertia 2168.5198387339756\n",
      "Iteration 6, inertia 2158.573622881011\n",
      "Iteration 7, inertia 2150.303816682201\n",
      "Iteration 8, inertia 2143.766937387027\n",
      "Iteration 9, inertia 2138.962162970309\n",
      "Iteration 10, inertia 2135.017222053545\n",
      "Iteration 11, inertia 2132.4689801721665\n",
      "Iteration 12, inertia 2131.5856717648744\n",
      "Iteration 13, inertia 2131.1529318896783\n",
      "Iteration 14, inertia 2130.990795090235\n",
      "Iteration 15, inertia 2130.837298217846\n",
      "Iteration 16, inertia 2130.77505208224\n",
      "Iteration 17, inertia 2130.753340537959\n",
      "Iteration 18, inertia 2130.7434802892726\n",
      "Iteration 19, inertia 2130.7370911183616\n",
      "Iteration 20, inertia 2130.7302019387066\n",
      "Iteration 21, inertia 2130.727971623332\n",
      "Converged at iteration 21: center shift 1.2519362213763349e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3286.503383962126\n",
      "Iteration 1, inertia 2423.4389219984473\n",
      "Iteration 2, inertia 2335.966939586731\n",
      "Iteration 3, inertia 2263.6291312092726\n",
      "Iteration 4, inertia 2207.855782435995\n",
      "Iteration 5, inertia 2185.995538146384\n",
      "Iteration 6, inertia 2169.561590661405\n",
      "Iteration 7, inertia 2152.852487598363\n",
      "Iteration 8, inertia 2143.940580513437\n",
      "Iteration 9, inertia 2138.0509873155324\n",
      "Iteration 10, inertia 2134.326063591527\n",
      "Iteration 11, inertia 2132.193192287629\n",
      "Iteration 12, inertia 2131.4429621696004\n",
      "Iteration 13, inertia 2131.0567412736355\n",
      "Iteration 14, inertia 2130.8765714112324\n",
      "Iteration 15, inertia 2130.7723098627475\n",
      "Iteration 16, inertia 2130.7524771285857\n",
      "Iteration 17, inertia 2130.743583483448\n",
      "Iteration 18, inertia 2130.735754522157\n",
      "Iteration 19, inertia 2130.728777747687\n",
      "Iteration 20, inertia 2130.7250842516974\n",
      "Iteration 21, inertia 2130.703404284231\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3301.024101979842\n",
      "Iteration 1, inertia 2213.1134641851827\n",
      "Iteration 2, inertia 2134.779880442103\n",
      "Iteration 3, inertia 2131.443741175346\n",
      "Iteration 4, inertia 2130.9837027930375\n",
      "Iteration 5, inertia 2130.8839643875795\n",
      "Iteration 6, inertia 2130.8312970155935\n",
      "Iteration 7, inertia 2130.7566319093407\n",
      "Iteration 8, inertia 2130.7077670599647\n",
      "Iteration 9, inertia 2130.6905553074794\n",
      "Iteration 10, inertia 2130.6880285784814\n",
      "Iteration 11, inertia 2130.683218862738\n",
      "Iteration 12, inertia 2130.679567641836\n",
      "Iteration 13, inertia 2130.6773626022286\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3449.4595501564354\n",
      "Iteration 1, inertia 2415.572534715643\n",
      "Iteration 2, inertia 2283.2888098530493\n",
      "Iteration 3, inertia 2227.92310441022\n",
      "Iteration 4, inertia 2210.4035746884597\n",
      "Iteration 5, inertia 2203.7015606531504\n",
      "Iteration 6, inertia 2197.100600605639\n",
      "Iteration 7, inertia 2189.154369821603\n",
      "Iteration 8, inertia 2180.7251045941325\n",
      "Iteration 9, inertia 2170.304211892915\n",
      "Iteration 10, inertia 2160.0726077780987\n",
      "Iteration 11, inertia 2151.0448609102623\n",
      "Iteration 12, inertia 2139.507842930052\n",
      "Iteration 13, inertia 2132.4478593094614\n",
      "Iteration 14, inertia 2131.0200851636614\n",
      "Iteration 15, inertia 2130.6812431030094\n",
      "Iteration 16, inertia 2130.6427153318814\n",
      "Iteration 17, inertia 2130.6293457389393\n",
      "Iteration 18, inertia 2130.623015656437\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3583.5612008259495\n",
      "Iteration 1, inertia 2659.5198886686876\n",
      "Iteration 2, inertia 2545.2878172026403\n",
      "Iteration 3, inertia 2479.8597142406306\n",
      "Iteration 4, inertia 2411.3258125962416\n",
      "Iteration 5, inertia 2367.6964101262047\n",
      "Iteration 6, inertia 2354.774588830113\n",
      "Iteration 7, inertia 2342.8625229298327\n",
      "Iteration 8, inertia 2331.766768166465\n",
      "Iteration 9, inertia 2327.032696832399\n",
      "Iteration 10, inertia 2325.936421293888\n",
      "Iteration 11, inertia 2325.7063614594304\n",
      "Iteration 12, inertia 2325.6305935702717\n",
      "Iteration 13, inertia 2325.602752627574\n",
      "Converged at iteration 13: center shift 1.2152554848224678e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2991.2186174153744\n",
      "Iteration 1, inertia 2252.5899804413807\n",
      "Iteration 2, inertia 2164.75353056089\n",
      "Iteration 3, inertia 2136.9436506369248\n",
      "Iteration 4, inertia 2131.7939916834316\n",
      "Iteration 5, inertia 2130.8316498950703\n",
      "Iteration 6, inertia 2130.7357169817406\n",
      "Iteration 7, inertia 2130.717224131503\n",
      "Iteration 8, inertia 2130.711490557965\n",
      "Iteration 9, inertia 2130.708699573788\n",
      "Iteration 10, inertia 2130.705015471731\n",
      "Iteration 11, inertia 2130.702436401295\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3507.161419832977\n",
      "Iteration 1, inertia 2464.7935690477707\n",
      "Iteration 2, inertia 2325.6517255755602\n",
      "Iteration 3, inertia 2269.423822638968\n",
      "Iteration 4, inertia 2218.480036751355\n",
      "Iteration 5, inertia 2197.392783971984\n",
      "Iteration 6, inertia 2187.419417858874\n",
      "Iteration 7, inertia 2178.1421246571836\n",
      "Iteration 8, inertia 2168.6013556001535\n",
      "Iteration 9, inertia 2158.86797087115\n",
      "Iteration 10, inertia 2150.7152793106634\n",
      "Iteration 11, inertia 2144.242482319379\n",
      "Iteration 12, inertia 2139.2948247322893\n",
      "Iteration 13, inertia 2135.028574094236\n",
      "Iteration 14, inertia 2132.4684532729643\n",
      "Iteration 15, inertia 2131.6010831398817\n",
      "Iteration 16, inertia 2131.17335729608\n",
      "Iteration 17, inertia 2131.0090438246584\n",
      "Iteration 18, inertia 2130.86096895801\n",
      "Iteration 19, inertia 2130.777545636046\n",
      "Iteration 20, inertia 2130.7533405379586\n",
      "Iteration 21, inertia 2130.7434802892726\n",
      "Iteration 22, inertia 2130.7370911183616\n",
      "Iteration 23, inertia 2130.7302019387066\n",
      "Iteration 24, inertia 2130.727971623332\n",
      "Converged at iteration 24: center shift 1.251936221376316e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3453.9948438157758\n",
      "Iteration 1, inertia 2480.4659399737843\n",
      "Iteration 2, inertia 2420.205277450566\n",
      "Iteration 3, inertia 2392.934746001256\n",
      "Iteration 4, inertia 2375.517608794784\n",
      "Iteration 5, inertia 2368.906546993171\n",
      "Iteration 6, inertia 2363.4154801779073\n",
      "Iteration 7, inertia 2354.4623828190215\n",
      "Iteration 8, inertia 2341.199827373387\n",
      "Iteration 9, inertia 2335.148730158929\n",
      "Iteration 10, inertia 2333.7611993018013\n",
      "Iteration 11, inertia 2333.4096619820016\n",
      "Iteration 12, inertia 2333.142024758291\n",
      "Iteration 13, inertia 2333.085673562482\n",
      "Iteration 14, inertia 2333.072740728573\n",
      "Iteration 15, inertia 2333.0672625845546\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3040.7082891405453\n",
      "Iteration 1, inertia 2334.5624335980037\n",
      "Iteration 2, inertia 2260.542484556494\n",
      "Iteration 3, inertia 2249.1537280028774\n",
      "Iteration 4, inertia 2244.150332118019\n",
      "Iteration 5, inertia 2241.334939633555\n",
      "Iteration 6, inertia 2239.9616510163887\n",
      "Iteration 7, inertia 2238.9823796509536\n",
      "Iteration 8, inertia 2237.738630986858\n",
      "Iteration 9, inertia 2232.2811374366256\n",
      "Iteration 10, inertia 2215.381809539079\n",
      "Iteration 11, inertia 2191.0491131667036\n",
      "Iteration 12, inertia 2165.9613005921\n",
      "Iteration 13, inertia 2145.8734451979544\n",
      "Iteration 14, inertia 2136.496585877872\n",
      "Iteration 15, inertia 2132.868104689127\n",
      "Iteration 16, inertia 2131.6834783032446\n",
      "Iteration 17, inertia 2131.24581556697\n",
      "Iteration 18, inertia 2131.0199235880705\n",
      "Iteration 19, inertia 2130.825638168138\n",
      "Iteration 20, inertia 2130.7780625838095\n",
      "Iteration 21, inertia 2130.7637183841575\n",
      "Iteration 22, inertia 2130.7509583726664\n",
      "Iteration 23, inertia 2130.7425594960796\n",
      "Iteration 24, inertia 2130.7339412013616\n",
      "Iteration 25, inertia 2130.728777747687\n",
      "Iteration 26, inertia 2130.7250842516974\n",
      "Iteration 27, inertia 2130.7034042842315\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3671.2789826914045\n",
      "Iteration 1, inertia 2460.0194030323955\n",
      "Iteration 2, inertia 2387.153657861599\n",
      "Iteration 3, inertia 2353.802609870993\n",
      "Iteration 4, inertia 2330.011444086854\n",
      "Iteration 5, inertia 2308.3208657551677\n",
      "Iteration 6, inertia 2292.527651806453\n",
      "Iteration 7, inertia 2281.1603146803627\n",
      "Iteration 8, inertia 2275.60932425766\n",
      "Iteration 9, inertia 2273.1505377382496\n",
      "Iteration 10, inertia 2272.410355535464\n",
      "Iteration 11, inertia 2272.0009972403304\n",
      "Iteration 12, inertia 2271.7486023386264\n",
      "Iteration 13, inertia 2271.6196289022823\n",
      "Iteration 14, inertia 2271.5878413686805\n",
      "Iteration 15, inertia 2271.5675672553084\n",
      "Iteration 16, inertia 2271.5447049833656\n",
      "Iteration 17, inertia 2271.538235849567\n",
      "Iteration 18, inertia 2271.535827925008\n",
      "Converged at iteration 18: center shift 9.708863675635025e-07 within tolerance 1.436774592781648e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 7 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2709.537491416275\n",
      "Iteration 1, inertia 2077.1319573643386\n",
      "Iteration 2, inertia 2032.639269702104\n",
      "Iteration 3, inertia 2028.1240464893865\n",
      "Iteration 4, inertia 2026.6177429224156\n",
      "Iteration 5, inertia 2025.9324087303246\n",
      "Iteration 6, inertia 2025.48448973204\n",
      "Iteration 7, inertia 2025.3615188334975\n",
      "Iteration 8, inertia 2025.230570070431\n",
      "Iteration 9, inertia 2025.1596875209914\n",
      "Iteration 10, inertia 2025.105451929846\n",
      "Iteration 11, inertia 2025.0557107179623\n",
      "Iteration 12, inertia 2025.0113669159175\n",
      "Iteration 13, inertia 2024.967686952041\n",
      "Iteration 14, inertia 2024.9447773843463\n",
      "Iteration 15, inertia 2024.9377231463213\n",
      "Iteration 16, inertia 2024.9291054446626\n",
      "Iteration 17, inertia 2024.9109165761452\n",
      "Iteration 18, inertia 2024.894906846689\n",
      "Iteration 19, inertia 2024.890507634917\n",
      "Iteration 20, inertia 2024.8844985804212\n",
      "Converged at iteration 20: center shift 5.439222463860306e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2812.7947591249163\n",
      "Iteration 1, inertia 2108.2296187543484\n",
      "Iteration 2, inertia 2043.1397116648336\n",
      "Iteration 3, inertia 2029.1741353640086\n",
      "Iteration 4, inertia 2026.1256506846596\n",
      "Iteration 5, inertia 2025.2231395053632\n",
      "Iteration 6, inertia 2024.8072876361534\n",
      "Iteration 7, inertia 2024.569765154977\n",
      "Iteration 8, inertia 2024.3836517932673\n",
      "Iteration 9, inertia 2024.3234507155726\n",
      "Iteration 10, inertia 2024.285597760057\n",
      "Iteration 11, inertia 2024.239768032181\n",
      "Iteration 12, inertia 2024.1861949486145\n",
      "Iteration 13, inertia 2024.1463732166649\n",
      "Iteration 14, inertia 2024.1319016366967\n",
      "Iteration 15, inertia 2024.1152903793889\n",
      "Iteration 16, inertia 2024.1106172056157\n",
      "Iteration 17, inertia 2024.1035231951523\n",
      "Iteration 18, inertia 2024.0920136512755\n",
      "Converged at iteration 18: center shift 1.2064943151472266e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3142.0731289858377\n",
      "Iteration 1, inertia 2354.9270366207384\n",
      "Iteration 2, inertia 2235.2843028610887\n",
      "Iteration 3, inertia 2151.1647039513764\n",
      "Iteration 4, inertia 2104.974978243049\n",
      "Iteration 5, inertia 2092.626623224472\n",
      "Iteration 6, inertia 2087.2836504303273\n",
      "Iteration 7, inertia 2084.586049150273\n",
      "Iteration 8, inertia 2082.5687396594126\n",
      "Iteration 9, inertia 2081.967562145806\n",
      "Iteration 10, inertia 2081.7132785083986\n",
      "Iteration 11, inertia 2081.545731919497\n",
      "Iteration 12, inertia 2081.4973408824826\n",
      "Iteration 13, inertia 2081.478450043044\n",
      "Iteration 14, inertia 2081.467500268507\n",
      "Iteration 15, inertia 2081.4590614288472\n",
      "Iteration 16, inertia 2081.4452272704734\n",
      "Iteration 17, inertia 2081.424116061456\n",
      "Iteration 18, inertia 2081.4212109306536\n",
      "Iteration 19, inertia 2081.416212448298\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3033.9925146368496\n",
      "Iteration 1, inertia 2347.773979288299\n",
      "Iteration 2, inertia 2260.495401121656\n",
      "Iteration 3, inertia 2241.153604616439\n",
      "Iteration 4, inertia 2236.434958931045\n",
      "Iteration 5, inertia 2234.115603838406\n",
      "Iteration 6, inertia 2233.3419655173752\n",
      "Iteration 7, inertia 2232.94205061743\n",
      "Iteration 8, inertia 2232.113554725746\n",
      "Iteration 9, inertia 2230.160807150555\n",
      "Iteration 10, inertia 2225.9016467026136\n",
      "Iteration 11, inertia 2219.9814635596763\n",
      "Iteration 12, inertia 2213.6060322300295\n",
      "Iteration 13, inertia 2211.292814785159\n",
      "Iteration 14, inertia 2210.1608626887155\n",
      "Iteration 15, inertia 2209.7914318670582\n",
      "Iteration 16, inertia 2209.3947154950006\n",
      "Iteration 17, inertia 2209.040329497629\n",
      "Iteration 18, inertia 2208.863022370832\n",
      "Iteration 19, inertia 2208.781991654799\n",
      "Iteration 20, inertia 2208.732751420499\n",
      "Iteration 21, inertia 2208.718364095008\n",
      "Iteration 22, inertia 2208.7069783442503\n",
      "Iteration 23, inertia 2208.69028465221\n",
      "Iteration 24, inertia 2208.6767414145843\n",
      "Iteration 25, inertia 2208.665534610344\n",
      "Iteration 26, inertia 2208.6575565778794\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3687.6875114637064\n",
      "Iteration 1, inertia 2496.729312411102\n",
      "Iteration 2, inertia 2386.5293280613155\n",
      "Iteration 3, inertia 2317.9761263761343\n",
      "Iteration 4, inertia 2268.391119710983\n",
      "Iteration 5, inertia 2237.638512182981\n",
      "Iteration 6, inertia 2230.8218431208115\n",
      "Iteration 7, inertia 2227.2740149976457\n",
      "Iteration 8, inertia 2224.418162255767\n",
      "Iteration 9, inertia 2222.7121805658608\n",
      "Iteration 10, inertia 2221.146000181236\n",
      "Iteration 11, inertia 2219.8938128466557\n",
      "Iteration 12, inertia 2218.84447911151\n",
      "Iteration 13, inertia 2217.275269880157\n",
      "Iteration 14, inertia 2214.667389889354\n",
      "Iteration 15, inertia 2211.145817948065\n",
      "Iteration 16, inertia 2205.4472249007677\n",
      "Iteration 17, inertia 2199.532214886724\n",
      "Iteration 18, inertia 2191.6390272185913\n",
      "Iteration 19, inertia 2183.8656189536223\n",
      "Iteration 20, inertia 2178.1729786698343\n",
      "Iteration 21, inertia 2174.759401910927\n",
      "Iteration 22, inertia 2173.241912346475\n",
      "Iteration 23, inertia 2172.720110403967\n",
      "Iteration 24, inertia 2172.551146554285\n",
      "Iteration 25, inertia 2172.434896551335\n",
      "Iteration 26, inertia 2172.398106697321\n",
      "Iteration 27, inertia 2172.35665154382\n",
      "Iteration 28, inertia 2172.3115340580725\n",
      "Iteration 29, inertia 2172.2981262425074\n",
      "Iteration 30, inertia 2172.2805980940893\n",
      "Iteration 31, inertia 2172.2761858150957\n",
      "Converged at iteration 31: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3156.2085316732623\n",
      "Iteration 1, inertia 2312.114334494314\n",
      "Iteration 2, inertia 2183.200333638707\n",
      "Iteration 3, inertia 2141.6314166360235\n",
      "Iteration 4, inertia 2116.3058081526115\n",
      "Iteration 5, inertia 2091.481755700432\n",
      "Iteration 6, inertia 2055.9345861867187\n",
      "Iteration 7, inertia 2029.979048200463\n",
      "Iteration 8, inertia 2024.9393331846723\n",
      "Iteration 9, inertia 2024.4121711668492\n",
      "Iteration 10, inertia 2024.234866913884\n",
      "Iteration 11, inertia 2024.1758152788932\n",
      "Iteration 12, inertia 2024.1474535397442\n",
      "Iteration 13, inertia 2024.1180869995992\n",
      "Iteration 14, inertia 2024.1062783528048\n",
      "Iteration 15, inertia 2024.0968043079006\n",
      "Iteration 16, inertia 2024.0938880292456\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3063.3939118712533\n",
      "Iteration 1, inertia 2234.753558088517\n",
      "Iteration 2, inertia 2160.5280970989766\n",
      "Iteration 3, inertia 2125.2562785399773\n",
      "Iteration 4, inertia 2102.653104720543\n",
      "Iteration 5, inertia 2090.3233610750394\n",
      "Iteration 6, inertia 2085.1942826190552\n",
      "Iteration 7, inertia 2082.028187877505\n",
      "Iteration 8, inertia 2080.60292972722\n",
      "Iteration 9, inertia 2079.5920475538956\n",
      "Iteration 10, inertia 2079.161043349447\n",
      "Iteration 11, inertia 2078.826105287023\n",
      "Iteration 12, inertia 2078.6547825893945\n",
      "Iteration 13, inertia 2078.58245070917\n",
      "Iteration 14, inertia 2078.5555960857637\n",
      "Iteration 15, inertia 2078.504324315497\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2909.120938855484\n",
      "Iteration 1, inertia 2276.9796713271808\n",
      "Iteration 2, inertia 2196.5649495566277\n",
      "Iteration 3, inertia 2164.010960808383\n",
      "Iteration 4, inertia 2145.9978115005733\n",
      "Iteration 5, inertia 2132.8418479001975\n",
      "Iteration 6, inertia 2123.711627027452\n",
      "Iteration 7, inertia 2114.048478438734\n",
      "Iteration 8, inertia 2105.894727122949\n",
      "Iteration 9, inertia 2098.6480063053255\n",
      "Iteration 10, inertia 2093.595556975837\n",
      "Iteration 11, inertia 2089.9867013980797\n",
      "Iteration 12, inertia 2086.8217233001346\n",
      "Iteration 13, inertia 2084.545794790031\n",
      "Iteration 14, inertia 2083.3313740349067\n",
      "Iteration 15, inertia 2082.7166643645346\n",
      "Iteration 16, inertia 2082.341802865181\n",
      "Iteration 17, inertia 2082.048207914286\n",
      "Iteration 18, inertia 2081.9109541184976\n",
      "Iteration 19, inertia 2081.8077619132364\n",
      "Iteration 20, inertia 2081.7762297146182\n",
      "Iteration 21, inertia 2081.7389080550947\n",
      "Iteration 22, inertia 2081.709469338364\n",
      "Iteration 23, inertia 2081.6992123507803\n",
      "Iteration 24, inertia 2081.6842279029956\n",
      "Iteration 25, inertia 2081.668146668021\n",
      "Iteration 26, inertia 2081.610391654832\n",
      "Iteration 27, inertia 2081.5235367701753\n",
      "Iteration 28, inertia 2081.508758096324\n",
      "Iteration 29, inertia 2081.5073456445907\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3326.406045678021\n",
      "Iteration 1, inertia 2306.1010755002017\n",
      "Iteration 2, inertia 2240.961851888064\n",
      "Iteration 3, inertia 2225.542256470629\n",
      "Iteration 4, inertia 2213.093360219787\n",
      "Iteration 5, inertia 2189.9450142212345\n",
      "Iteration 6, inertia 2155.5271108275074\n",
      "Iteration 7, inertia 2140.778850067154\n",
      "Iteration 8, inertia 2138.523695103985\n",
      "Iteration 9, inertia 2138.093079895916\n",
      "Iteration 10, inertia 2137.9362284039007\n",
      "Iteration 11, inertia 2137.905137252721\n",
      "Iteration 12, inertia 2137.893128433089\n",
      "Iteration 13, inertia 2137.890924119768\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3016.8013365315983\n",
      "Iteration 1, inertia 2180.886875823132\n",
      "Iteration 2, inertia 2060.7956074280646\n",
      "Iteration 3, inertia 2031.858874223255\n",
      "Iteration 4, inertia 2027.006231503685\n",
      "Iteration 5, inertia 2025.6515245951293\n",
      "Iteration 6, inertia 2025.009115530237\n",
      "Iteration 7, inertia 2024.6049321268115\n",
      "Iteration 8, inertia 2024.4312608960633\n",
      "Iteration 9, inertia 2024.374386984876\n",
      "Iteration 10, inertia 2024.3446340122484\n",
      "Iteration 11, inertia 2024.3311229254077\n",
      "Iteration 12, inertia 2024.296100510106\n",
      "Iteration 13, inertia 2024.272611096449\n",
      "Iteration 14, inertia 2024.2539286508975\n",
      "Iteration 15, inertia 2024.2409068802738\n",
      "Iteration 16, inertia 2024.2301107870715\n",
      "Iteration 17, inertia 2024.1913525556668\n",
      "Iteration 18, inertia 2024.1406020897484\n",
      "Iteration 19, inertia 2024.1104939509685\n",
      "Converged at iteration 19: center shift 1.128094835118878e-06 within tolerance 1.436774592781648e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 8 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2911.4633522793165\n",
      "Iteration 1, inertia 2110.2087990049895\n",
      "Iteration 2, inertia 2010.7268463515022\n",
      "Iteration 3, inertia 1985.7635669867354\n",
      "Iteration 4, inertia 1980.7873822352126\n",
      "Iteration 5, inertia 1979.3010806875884\n",
      "Iteration 6, inertia 1978.58247026145\n",
      "Iteration 7, inertia 1978.0781094365136\n",
      "Iteration 8, inertia 1977.6226878211542\n",
      "Iteration 9, inertia 1977.501387089322\n",
      "Iteration 10, inertia 1977.4480464218368\n",
      "Iteration 11, inertia 1977.408476676965\n",
      "Iteration 12, inertia 1977.403603447867\n",
      "Iteration 13, inertia 1977.4001658957484\n",
      "Converged at iteration 13: center shift 4.859265405426487e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3007.231212845899\n",
      "Iteration 1, inertia 2300.368056823791\n",
      "Iteration 2, inertia 2233.4611557801973\n",
      "Iteration 3, inertia 2199.8335207332075\n",
      "Iteration 4, inertia 2182.354789326209\n",
      "Iteration 5, inertia 2176.408969347961\n",
      "Iteration 6, inertia 2172.842682267727\n",
      "Iteration 7, inertia 2170.166222441054\n",
      "Iteration 8, inertia 2168.3042611931132\n",
      "Iteration 9, inertia 2165.8846117327203\n",
      "Iteration 10, inertia 2161.640169776346\n",
      "Iteration 11, inertia 2151.7467726233435\n",
      "Iteration 12, inertia 2129.7008385020126\n",
      "Iteration 13, inertia 2091.5781521389713\n",
      "Iteration 14, inertia 2060.998119908188\n",
      "Iteration 15, inertia 2044.1517654417644\n",
      "Iteration 16, inertia 2032.628735676292\n",
      "Iteration 17, inertia 2020.3193439632478\n",
      "Iteration 18, inertia 2012.317016145419\n",
      "Iteration 19, inertia 2010.3632676410452\n",
      "Iteration 20, inertia 2009.9182664861007\n",
      "Iteration 21, inertia 2009.78023167349\n",
      "Iteration 22, inertia 2009.6943128658397\n",
      "Iteration 23, inertia 2009.6652095955496\n",
      "Iteration 24, inertia 2009.6545595487808\n",
      "Iteration 25, inertia 2009.6448929750575\n",
      "Iteration 26, inertia 2009.6425628930551\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2797.5440115974457\n",
      "Iteration 1, inertia 2098.29356563549\n",
      "Iteration 2, inertia 2037.8772501409856\n",
      "Iteration 3, inertia 2006.087401230716\n",
      "Iteration 4, inertia 1989.262772095559\n",
      "Iteration 5, inertia 1985.8360878148944\n",
      "Iteration 6, inertia 1984.4196371398032\n",
      "Iteration 7, inertia 1983.675824341323\n",
      "Iteration 8, inertia 1982.2204848355746\n",
      "Iteration 9, inertia 1979.229226450115\n",
      "Iteration 10, inertia 1971.7326510705252\n",
      "Iteration 11, inertia 1960.2837123833383\n",
      "Iteration 12, inertia 1954.2644636198215\n",
      "Iteration 13, inertia 1952.0811107162547\n",
      "Iteration 14, inertia 1951.6213985182906\n",
      "Iteration 15, inertia 1951.449636576276\n",
      "Iteration 16, inertia 1951.2811062768947\n",
      "Iteration 17, inertia 1951.1854467126768\n",
      "Iteration 18, inertia 1951.1548667474249\n",
      "Iteration 19, inertia 1951.1084342400686\n",
      "Iteration 20, inertia 1951.0723564145164\n",
      "Iteration 21, inertia 1951.0565549355738\n",
      "Iteration 22, inertia 1951.0433265101474\n",
      "Iteration 23, inertia 1951.0338266042654\n",
      "Iteration 24, inertia 1951.031485604883\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2831.0550347901817\n",
      "Iteration 1, inertia 2028.8055363189478\n",
      "Iteration 2, inertia 1990.7585937167871\n",
      "Iteration 3, inertia 1983.1899978867768\n",
      "Iteration 4, inertia 1980.0341116218033\n",
      "Iteration 5, inertia 1977.7112919566198\n",
      "Iteration 6, inertia 1975.8045847208493\n",
      "Iteration 7, inertia 1974.7968503947382\n",
      "Iteration 8, inertia 1974.1563898503316\n",
      "Iteration 9, inertia 1973.9438672345832\n",
      "Iteration 10, inertia 1973.7638659568167\n",
      "Iteration 11, inertia 1973.5872129121583\n",
      "Iteration 12, inertia 1973.4187704489816\n",
      "Iteration 13, inertia 1973.3086368562144\n",
      "Iteration 14, inertia 1973.2408776677294\n",
      "Iteration 15, inertia 1973.1936399691206\n",
      "Iteration 16, inertia 1973.1128956973057\n",
      "Iteration 17, inertia 1972.9520371931305\n",
      "Iteration 18, inertia 1972.7410260699692\n",
      "Iteration 19, inertia 1972.5724744264362\n",
      "Iteration 20, inertia 1972.4641198214815\n",
      "Iteration 21, inertia 1972.3822387261057\n",
      "Iteration 22, inertia 1972.2960130263846\n",
      "Iteration 23, inertia 1972.2360892107\n",
      "Iteration 24, inertia 1972.2243333744786\n",
      "Iteration 25, inertia 1972.2218035080784\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2765.5836565168274\n",
      "Iteration 1, inertia 2304.5822801897502\n",
      "Iteration 2, inertia 2242.875064464462\n",
      "Iteration 3, inertia 2219.963072459374\n",
      "Iteration 4, inertia 2212.8175994206345\n",
      "Iteration 5, inertia 2208.175453706204\n",
      "Iteration 6, inertia 2202.5304838506972\n",
      "Iteration 7, inertia 2193.266222234448\n",
      "Iteration 8, inertia 2177.1018412068233\n",
      "Iteration 9, inertia 2146.3316642084615\n",
      "Iteration 10, inertia 2080.682421844563\n",
      "Iteration 11, inertia 2044.1868409546091\n",
      "Iteration 12, inertia 2033.7149521236595\n",
      "Iteration 13, inertia 2028.7855946144841\n",
      "Iteration 14, inertia 2026.2108050439674\n",
      "Iteration 15, inertia 2025.9244757934357\n",
      "Iteration 16, inertia 2025.838368838667\n",
      "Iteration 17, inertia 2025.7932715681052\n",
      "Iteration 18, inertia 2025.78022411257\n",
      "Iteration 19, inertia 2025.7607227273957\n",
      "Iteration 20, inertia 2025.7454062101265\n",
      "Iteration 21, inertia 2025.7340363159926\n",
      "Converged at iteration 21: center shift 6.521647424692195e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3171.4839119328444\n",
      "Iteration 1, inertia 2287.8395125299826\n",
      "Iteration 2, inertia 2158.9749244285144\n",
      "Iteration 3, inertia 2046.4798319283716\n",
      "Iteration 4, inertia 2011.5506173409822\n",
      "Iteration 5, inertia 2002.9982169612485\n",
      "Iteration 6, inertia 1997.8430270185122\n",
      "Iteration 7, inertia 1995.510263593541\n",
      "Iteration 8, inertia 1992.3931300989327\n",
      "Iteration 9, inertia 1986.0505809602973\n",
      "Iteration 10, inertia 1972.9353690481257\n",
      "Iteration 11, inertia 1958.1486261707066\n",
      "Iteration 12, inertia 1947.7915130617969\n",
      "Iteration 13, inertia 1941.0040660814684\n",
      "Iteration 14, inertia 1938.4190233457634\n",
      "Iteration 15, inertia 1937.708032888058\n",
      "Iteration 16, inertia 1937.2892269815263\n",
      "Iteration 17, inertia 1936.8847730254802\n",
      "Iteration 18, inertia 1936.6572168226517\n",
      "Iteration 19, inertia 1936.5293031818749\n",
      "Iteration 20, inertia 1936.4475386391093\n",
      "Iteration 21, inertia 1936.406495582089\n",
      "Iteration 22, inertia 1936.374070809538\n",
      "Iteration 23, inertia 1936.3331525649935\n",
      "Iteration 24, inertia 1936.2802697019183\n",
      "Iteration 25, inertia 1936.1419645617484\n",
      "Iteration 26, inertia 1936.0486501140454\n",
      "Iteration 27, inertia 1936.0119029727994\n",
      "Iteration 28, inertia 1936.0017356113817\n",
      "Iteration 29, inertia 1935.9966553786226\n",
      "Iteration 30, inertia 1935.9910995513858\n",
      "Iteration 31, inertia 1935.9602537342355\n",
      "Iteration 32, inertia 1935.9124815132823\n",
      "Iteration 33, inertia 1935.893320758942\n",
      "Iteration 34, inertia 1935.8895702796435\n",
      "Converged at iteration 34: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2796.8083471434843\n",
      "Iteration 1, inertia 2123.5263445425044\n",
      "Iteration 2, inertia 2041.791174911248\n",
      "Iteration 3, inertia 1974.5361188742972\n",
      "Iteration 4, inertia 1955.7375143732952\n",
      "Iteration 5, inertia 1953.0320849985837\n",
      "Iteration 6, inertia 1952.1427691648921\n",
      "Iteration 7, inertia 1951.7554405627307\n",
      "Iteration 8, inertia 1951.5507364144437\n",
      "Iteration 9, inertia 1951.4035565688703\n",
      "Iteration 10, inertia 1951.300475010294\n",
      "Iteration 11, inertia 1951.2554518110142\n",
      "Iteration 12, inertia 1951.1581077103892\n",
      "Iteration 13, inertia 1951.0681275057832\n",
      "Iteration 14, inertia 1951.0487955876836\n",
      "Iteration 15, inertia 1951.0428107479033\n",
      "Iteration 16, inertia 1951.0405930914926\n",
      "Iteration 17, inertia 1951.0351052213227\n",
      "Converged at iteration 17: center shift 5.804961031717526e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2871.200272025979\n",
      "Iteration 1, inertia 2112.364803956566\n",
      "Iteration 2, inertia 2049.2128482731\n",
      "Iteration 3, inertia 2043.3619259893144\n",
      "Iteration 4, inertia 2040.5606234203865\n",
      "Iteration 5, inertia 2038.2601039426193\n",
      "Iteration 6, inertia 2035.9616867626642\n",
      "Iteration 7, inertia 2034.5514854845414\n",
      "Iteration 8, inertia 2033.9234003333538\n",
      "Iteration 9, inertia 2033.7589733305867\n",
      "Iteration 10, inertia 2033.6942812289622\n",
      "Iteration 11, inertia 2033.612583010438\n",
      "Iteration 12, inertia 2033.5725866620219\n",
      "Iteration 13, inertia 2033.5400031788913\n",
      "Iteration 14, inertia 2033.522670045863\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2845.1705807473777\n",
      "Iteration 1, inertia 2037.6805930209257\n",
      "Iteration 2, inertia 1991.1579957413073\n",
      "Iteration 3, inertia 1984.6789417787995\n",
      "Iteration 4, inertia 1984.003315341967\n",
      "Iteration 5, inertia 1983.7071774584215\n",
      "Iteration 6, inertia 1983.6039203089251\n",
      "Iteration 7, inertia 1983.5447863074073\n",
      "Iteration 8, inertia 1983.516230831404\n",
      "Iteration 9, inertia 1983.510681217485\n",
      "Iteration 10, inertia 1983.5078330770089\n",
      "Iteration 11, inertia 1983.5024390308854\n",
      "Converged at iteration 11: center shift 6.045079240183508e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3038.1010345320556\n",
      "Iteration 1, inertia 2105.7751229468554\n",
      "Iteration 2, inertia 2030.1629190668407\n",
      "Iteration 3, inertia 2013.5678432157454\n",
      "Iteration 4, inertia 2003.8610721808734\n",
      "Iteration 5, inertia 1995.3682425600146\n",
      "Iteration 6, inertia 1988.2041461023125\n",
      "Iteration 7, inertia 1981.90035639162\n",
      "Iteration 8, inertia 1974.0065586911371\n",
      "Iteration 9, inertia 1966.3055872030611\n",
      "Iteration 10, inertia 1961.9090482112133\n",
      "Iteration 11, inertia 1959.3160537078884\n",
      "Iteration 12, inertia 1957.4821106165623\n",
      "Iteration 13, inertia 1956.5481825037048\n",
      "Iteration 14, inertia 1955.944910818372\n",
      "Iteration 15, inertia 1955.6431240934608\n",
      "Iteration 16, inertia 1955.4821377254816\n",
      "Iteration 17, inertia 1955.422369027156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, inertia 1955.3790709386703\n",
      "Iteration 19, inertia 1955.3733411327682\n",
      "Converged at iteration 19: strict convergence.\n",
      "Training a K-Means model with 9 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2826.7113020113748\n",
      "Iteration 1, inertia 2038.8515985607958\n",
      "Iteration 2, inertia 1960.3646375396777\n",
      "Iteration 3, inertia 1925.0325040706623\n",
      "Iteration 4, inertia 1908.5780641023837\n",
      "Iteration 5, inertia 1899.5004866215368\n",
      "Iteration 6, inertia 1893.4920903279778\n",
      "Iteration 7, inertia 1891.5623914093183\n",
      "Iteration 8, inertia 1891.1055612204993\n",
      "Iteration 9, inertia 1890.952044597331\n",
      "Iteration 10, inertia 1890.8267738840646\n",
      "Iteration 11, inertia 1890.6552870909222\n",
      "Iteration 12, inertia 1890.3764485163235\n",
      "Iteration 13, inertia 1890.17473386493\n",
      "Iteration 14, inertia 1890.090536884535\n",
      "Iteration 15, inertia 1890.0456713699473\n",
      "Iteration 16, inertia 1889.9850038512363\n",
      "Iteration 17, inertia 1889.9682752354681\n",
      "Iteration 18, inertia 1889.9612462605965\n",
      "Iteration 19, inertia 1889.9547781787\n",
      "Iteration 20, inertia 1889.9493062396148\n",
      "Iteration 21, inertia 1889.937206503543\n",
      "Iteration 22, inertia 1889.9322890238213\n",
      "Iteration 23, inertia 1889.9303074767106\n",
      "Iteration 24, inertia 1889.919006166137\n",
      "Iteration 25, inertia 1889.9095831974416\n",
      "Iteration 26, inertia 1889.9040566864264\n",
      "Iteration 27, inertia 1889.8979327671009\n",
      "Iteration 28, inertia 1889.8964136366847\n",
      "Iteration 29, inertia 1889.8943729213254\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2759.0078799993444\n",
      "Iteration 1, inertia 2060.3524811980224\n",
      "Iteration 2, inertia 2000.4168966655363\n",
      "Iteration 3, inertia 1961.6100790682026\n",
      "Iteration 4, inertia 1930.0749589515258\n",
      "Iteration 5, inertia 1912.580151973215\n",
      "Iteration 6, inertia 1905.2491657972244\n",
      "Iteration 7, inertia 1901.8974558470165\n",
      "Iteration 8, inertia 1899.9037721826555\n",
      "Iteration 9, inertia 1898.648857755095\n",
      "Iteration 10, inertia 1897.8802800395704\n",
      "Iteration 11, inertia 1897.4014000350944\n",
      "Iteration 12, inertia 1897.1241508555718\n",
      "Iteration 13, inertia 1896.8391123637448\n",
      "Iteration 14, inertia 1896.4156446499187\n",
      "Iteration 15, inertia 1895.6979839699368\n",
      "Iteration 16, inertia 1893.9572011695457\n",
      "Iteration 17, inertia 1890.6137617906018\n",
      "Iteration 18, inertia 1887.901285453946\n",
      "Iteration 19, inertia 1885.4735158639444\n",
      "Iteration 20, inertia 1883.860736662906\n",
      "Iteration 21, inertia 1883.0935906840848\n",
      "Iteration 22, inertia 1882.76484869662\n",
      "Iteration 23, inertia 1882.5381805239442\n",
      "Iteration 24, inertia 1882.4096417390856\n",
      "Iteration 25, inertia 1882.2890498094928\n",
      "Iteration 26, inertia 1882.2082543579252\n",
      "Iteration 27, inertia 1882.1677701246088\n",
      "Iteration 28, inertia 1882.138929019097\n",
      "Iteration 29, inertia 1882.1353005618732\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2658.470688697317\n",
      "Iteration 1, inertia 2012.1266989459502\n",
      "Iteration 2, inertia 1973.316802417614\n",
      "Iteration 3, inertia 1960.2247666423639\n",
      "Iteration 4, inertia 1953.102879785451\n",
      "Iteration 5, inertia 1947.5986634131461\n",
      "Iteration 6, inertia 1941.704169678819\n",
      "Iteration 7, inertia 1936.4098404644208\n",
      "Iteration 8, inertia 1931.0624204699175\n",
      "Iteration 9, inertia 1927.7617813213626\n",
      "Iteration 10, inertia 1924.7151780472316\n",
      "Iteration 11, inertia 1921.198159399295\n",
      "Iteration 12, inertia 1915.850018672818\n",
      "Iteration 13, inertia 1911.3685109313126\n",
      "Iteration 14, inertia 1906.4090468987274\n",
      "Iteration 15, inertia 1900.9215828544386\n",
      "Iteration 16, inertia 1895.1925161574895\n",
      "Iteration 17, inertia 1891.6184364928215\n",
      "Iteration 18, inertia 1888.0946529258892\n",
      "Iteration 19, inertia 1885.5157378864465\n",
      "Iteration 20, inertia 1884.6346015454412\n",
      "Iteration 21, inertia 1883.7145252879718\n",
      "Iteration 22, inertia 1882.8742435617714\n",
      "Iteration 23, inertia 1882.4963094181044\n",
      "Iteration 24, inertia 1882.2747996235362\n",
      "Iteration 25, inertia 1882.19434525408\n",
      "Iteration 26, inertia 1882.1591987249706\n",
      "Iteration 27, inertia 1882.1482002180041\n",
      "Iteration 28, inertia 1882.1361520881883\n",
      "Iteration 29, inertia 1882.133728302399\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2819.10982263814\n",
      "Iteration 1, inertia 2034.8196140624993\n",
      "Iteration 2, inertia 1978.4335754931733\n",
      "Iteration 3, inertia 1968.5840199213185\n",
      "Iteration 4, inertia 1962.4195494427258\n",
      "Iteration 5, inertia 1954.139326293719\n",
      "Iteration 6, inertia 1940.4675599867026\n",
      "Iteration 7, inertia 1928.781082991833\n",
      "Iteration 8, inertia 1918.6757371024942\n",
      "Iteration 9, inertia 1909.167065439908\n",
      "Iteration 10, inertia 1903.2880034138493\n",
      "Iteration 11, inertia 1900.1599038824581\n",
      "Iteration 12, inertia 1898.6358838454375\n",
      "Iteration 13, inertia 1897.8659860533483\n",
      "Iteration 14, inertia 1897.168274008126\n",
      "Iteration 15, inertia 1895.7884537709913\n",
      "Iteration 16, inertia 1894.4891564375396\n",
      "Iteration 17, inertia 1892.0849541350838\n",
      "Iteration 18, inertia 1887.403395052897\n",
      "Iteration 19, inertia 1882.4931132119652\n",
      "Iteration 20, inertia 1877.9024932727646\n",
      "Iteration 21, inertia 1873.6280879975238\n",
      "Iteration 22, inertia 1870.4046967600611\n",
      "Iteration 23, inertia 1867.6725052667794\n",
      "Iteration 24, inertia 1866.0691211940386\n",
      "Iteration 25, inertia 1865.4929081485589\n",
      "Iteration 26, inertia 1865.307429750943\n",
      "Iteration 27, inertia 1865.1810423967554\n",
      "Iteration 28, inertia 1865.002858763356\n",
      "Iteration 29, inertia 1864.6936024980137\n",
      "Iteration 30, inertia 1864.4557956278377\n",
      "Iteration 31, inertia 1864.250457123975\n",
      "Iteration 32, inertia 1864.0814025899406\n",
      "Iteration 33, inertia 1864.006006344442\n",
      "Iteration 34, inertia 1863.9858951806757\n",
      "Iteration 35, inertia 1863.96406451022\n",
      "Iteration 36, inertia 1863.953181242649\n",
      "Iteration 37, inertia 1863.9472516296953\n",
      "Converged at iteration 37: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2689.6040277380603\n",
      "Iteration 1, inertia 2068.2697964342756\n",
      "Iteration 2, inertia 1968.3451423716338\n",
      "Iteration 3, inertia 1928.821170440996\n",
      "Iteration 4, inertia 1917.2631904611694\n",
      "Iteration 5, inertia 1912.6941057993026\n",
      "Iteration 6, inertia 1910.6514381332202\n",
      "Iteration 7, inertia 1909.8203310495312\n",
      "Iteration 8, inertia 1909.3504529473162\n",
      "Iteration 9, inertia 1908.9274134451648\n",
      "Iteration 10, inertia 1908.749907165347\n",
      "Iteration 11, inertia 1908.696297741733\n",
      "Iteration 12, inertia 1908.6594961708774\n",
      "Iteration 13, inertia 1908.6418335745047\n",
      "Iteration 14, inertia 1908.6362375074725\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2945.1805697392324\n",
      "Iteration 1, inertia 2035.2003575177378\n",
      "Iteration 2, inertia 1982.9151799670947\n",
      "Iteration 3, inertia 1967.953546468348\n",
      "Iteration 4, inertia 1963.0372216258288\n",
      "Iteration 5, inertia 1960.1412167934745\n",
      "Iteration 6, inertia 1958.4740650610856\n",
      "Iteration 7, inertia 1957.6229983095727\n",
      "Iteration 8, inertia 1957.066552905377\n",
      "Iteration 9, inertia 1956.7997934569776\n",
      "Iteration 10, inertia 1956.5092572573312\n",
      "Iteration 11, inertia 1956.1311241568224\n",
      "Iteration 12, inertia 1955.7181007704437\n",
      "Iteration 13, inertia 1955.3144484745244\n",
      "Iteration 14, inertia 1954.9562725549335\n",
      "Iteration 15, inertia 1954.6969020666554\n",
      "Iteration 16, inertia 1954.4690470376931\n",
      "Iteration 17, inertia 1954.3172947133319\n",
      "Iteration 18, inertia 1954.203704029851\n",
      "Iteration 19, inertia 1954.0859087232195\n",
      "Iteration 20, inertia 1953.8930857200107\n",
      "Iteration 21, inertia 1953.7004817692336\n",
      "Iteration 22, inertia 1953.5272357169658\n",
      "Iteration 23, inertia 1953.3811138868784\n",
      "Iteration 24, inertia 1953.257341495168\n",
      "Iteration 25, inertia 1953.1353449940914\n",
      "Iteration 26, inertia 1953.0003727868952\n",
      "Iteration 27, inertia 1952.895450229952\n",
      "Iteration 28, inertia 1952.7858760183203\n",
      "Iteration 29, inertia 1952.696848668791\n",
      "Iteration 30, inertia 1952.6194330001176\n",
      "Iteration 31, inertia 1952.5279115982523\n",
      "Iteration 32, inertia 1952.467468469637\n",
      "Iteration 33, inertia 1952.3645091431476\n",
      "Iteration 34, inertia 1952.2604672174461\n",
      "Iteration 35, inertia 1952.093906859471\n",
      "Iteration 36, inertia 1951.894340525736\n",
      "Iteration 37, inertia 1951.731970000572\n",
      "Iteration 38, inertia 1951.3531876378124\n",
      "Iteration 39, inertia 1950.5234805090906\n",
      "Iteration 40, inertia 1949.365637979825\n",
      "Iteration 41, inertia 1948.0330976177736\n",
      "Iteration 42, inertia 1946.1017265010814\n",
      "Iteration 43, inertia 1941.6722157677907\n",
      "Iteration 44, inertia 1933.3561408298392\n",
      "Iteration 45, inertia 1921.7274733289157\n",
      "Iteration 46, inertia 1915.2273745512944\n",
      "Iteration 47, inertia 1912.7303623580356\n",
      "Iteration 48, inertia 1911.5623912295127\n",
      "Iteration 49, inertia 1910.6343367521708\n",
      "Iteration 50, inertia 1910.1731471711848\n",
      "Iteration 51, inertia 1910.0574955606576\n",
      "Iteration 52, inertia 1909.96546801838\n",
      "Iteration 53, inertia 1909.8092736306505\n",
      "Iteration 54, inertia 1909.6224990117398\n",
      "Iteration 55, inertia 1909.4055702925134\n",
      "Iteration 56, inertia 1909.1296488094704\n",
      "Iteration 57, inertia 1908.8312697463437\n",
      "Iteration 58, inertia 1908.7181861641639\n",
      "Iteration 59, inertia 1908.6886378463694\n",
      "Iteration 60, inertia 1908.6615250364587\n",
      "Iteration 61, inertia 1908.6487854558973\n",
      "Iteration 62, inertia 1908.6362195456834\n",
      "Iteration 63, inertia 1908.6344942161345\n",
      "Converged at iteration 63: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2649.2691568895975\n",
      "Iteration 1, inertia 1998.392788135792\n",
      "Iteration 2, inertia 1939.525803350475\n",
      "Iteration 3, inertia 1911.7397591858944\n",
      "Iteration 4, inertia 1896.0116098419521\n",
      "Iteration 5, inertia 1884.1127474951766\n",
      "Iteration 6, inertia 1874.1638051933262\n",
      "Iteration 7, inertia 1868.8165214873964\n",
      "Iteration 8, inertia 1866.3097978529247\n",
      "Iteration 9, inertia 1865.4494929796151\n",
      "Iteration 10, inertia 1864.8966791430516\n",
      "Iteration 11, inertia 1864.4528337665968\n",
      "Iteration 12, inertia 1864.1634881087666\n",
      "Iteration 13, inertia 1864.0523186085059\n",
      "Iteration 14, inertia 1863.974130691683\n",
      "Iteration 15, inertia 1863.9594529811034\n",
      "Iteration 16, inertia 1863.9572815540012\n",
      "Iteration 17, inertia 1863.9548318679335\n",
      "Converged at iteration 17: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2828.3698787548346\n",
      "Iteration 1, inertia 2094.4139845096024\n",
      "Iteration 2, inertia 1982.4543976052682\n",
      "Iteration 3, inertia 1928.466454889877\n",
      "Iteration 4, inertia 1918.1510110446045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, inertia 1916.4461903424349\n",
      "Iteration 6, inertia 1916.0962505287705\n",
      "Iteration 7, inertia 1916.0131662728722\n",
      "Iteration 8, inertia 1915.955733076161\n",
      "Iteration 9, inertia 1915.8459276768683\n",
      "Iteration 10, inertia 1915.7723129153283\n",
      "Iteration 11, inertia 1915.7488224052715\n",
      "Iteration 12, inertia 1915.7368911921449\n",
      "Iteration 13, inertia 1915.7341056048824\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2822.815810444522\n",
      "Iteration 1, inertia 2013.6867942062074\n",
      "Iteration 2, inertia 1942.4494187141363\n",
      "Iteration 3, inertia 1913.4658985652563\n",
      "Iteration 4, inertia 1901.364903006695\n",
      "Iteration 5, inertia 1898.1347114475404\n",
      "Iteration 6, inertia 1897.316262326832\n",
      "Iteration 7, inertia 1896.8421155062715\n",
      "Iteration 8, inertia 1896.6056864413108\n",
      "Iteration 9, inertia 1896.4629683317726\n",
      "Iteration 10, inertia 1896.297629677811\n",
      "Iteration 11, inertia 1896.1773453693547\n",
      "Iteration 12, inertia 1896.062943733218\n",
      "Iteration 13, inertia 1895.9823403766964\n",
      "Iteration 14, inertia 1895.8713201725825\n",
      "Iteration 15, inertia 1895.7374840886364\n",
      "Iteration 16, inertia 1895.6562143680967\n",
      "Iteration 17, inertia 1895.589756828189\n",
      "Iteration 18, inertia 1895.5552564724208\n",
      "Iteration 19, inertia 1895.5417270235953\n",
      "Iteration 20, inertia 1895.5231408175125\n",
      "Iteration 21, inertia 1895.5115381965832\n",
      "Iteration 22, inertia 1895.4749058799982\n",
      "Iteration 23, inertia 1895.3994141700596\n",
      "Iteration 24, inertia 1895.3885567038888\n",
      "Iteration 25, inertia 1895.3868623590538\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2756.0541938801352\n",
      "Iteration 1, inertia 2025.0759365010047\n",
      "Iteration 2, inertia 1944.9426510190194\n",
      "Iteration 3, inertia 1924.7874262584078\n",
      "Iteration 4, inertia 1915.449610578663\n",
      "Iteration 5, inertia 1909.5091898703981\n",
      "Iteration 6, inertia 1906.5943321910502\n",
      "Iteration 7, inertia 1905.4480988842133\n",
      "Iteration 8, inertia 1904.9855076893475\n",
      "Iteration 9, inertia 1904.7879019180225\n",
      "Iteration 10, inertia 1904.706151892594\n",
      "Iteration 11, inertia 1904.6669000004224\n",
      "Iteration 12, inertia 1904.6156563135012\n",
      "Iteration 13, inertia 1904.5953962922606\n",
      "Iteration 14, inertia 1904.5818413216778\n",
      "Iteration 15, inertia 1904.5673994069623\n",
      "Iteration 16, inertia 1904.5534544888562\n",
      "Converged at iteration 16: strict convergence.\n",
      "Training a K-Means model with 10 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2777.974097126369\n",
      "Iteration 1, inertia 2009.3141945921868\n",
      "Iteration 2, inertia 1929.654804145534\n",
      "Iteration 3, inertia 1888.9284291401732\n",
      "Iteration 4, inertia 1869.0238810097017\n",
      "Iteration 5, inertia 1858.0711427790075\n",
      "Iteration 6, inertia 1852.5670583416318\n",
      "Iteration 7, inertia 1850.5351940664098\n",
      "Iteration 8, inertia 1849.4623860475153\n",
      "Iteration 9, inertia 1848.744354163875\n",
      "Iteration 10, inertia 1848.1113833023403\n",
      "Iteration 11, inertia 1847.6667843946734\n",
      "Iteration 12, inertia 1847.3277460405002\n",
      "Iteration 13, inertia 1847.1712672463004\n",
      "Iteration 14, inertia 1847.109492404867\n",
      "Iteration 15, inertia 1847.0896491055175\n",
      "Iteration 16, inertia 1847.0743244334005\n",
      "Iteration 17, inertia 1847.0407475268198\n",
      "Iteration 18, inertia 1846.9722007398605\n",
      "Iteration 19, inertia 1846.923482124122\n",
      "Iteration 20, inertia 1846.866793411792\n",
      "Iteration 21, inertia 1846.8003228063671\n",
      "Iteration 22, inertia 1846.680205320483\n",
      "Iteration 23, inertia 1846.5067564227554\n",
      "Iteration 24, inertia 1846.2735856899576\n",
      "Iteration 25, inertia 1845.9695760809502\n",
      "Iteration 26, inertia 1845.8793588486499\n",
      "Iteration 27, inertia 1845.8491079541932\n",
      "Iteration 28, inertia 1845.8317751027353\n",
      "Iteration 29, inertia 1845.8222155713006\n",
      "Iteration 30, inertia 1845.8098735538194\n",
      "Iteration 31, inertia 1845.796039570062\n",
      "Iteration 32, inertia 1845.7825962150941\n",
      "Iteration 33, inertia 1845.7712172330846\n",
      "Iteration 34, inertia 1845.7628918716766\n",
      "Iteration 35, inertia 1845.7516452745353\n",
      "Iteration 36, inertia 1845.7405207912186\n",
      "Iteration 37, inertia 1845.7363640907379\n",
      "Converged at iteration 37: center shift 6.851562573557367e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2523.2385298356053\n",
      "Iteration 1, inertia 1976.873867975811\n",
      "Iteration 2, inertia 1907.2788014521313\n",
      "Iteration 3, inertia 1893.0809597938876\n",
      "Iteration 4, inertia 1885.3701493948968\n",
      "Iteration 5, inertia 1879.0752826464857\n",
      "Iteration 6, inertia 1876.138010626265\n",
      "Iteration 7, inertia 1874.776247239817\n",
      "Iteration 8, inertia 1873.972576282736\n",
      "Iteration 9, inertia 1873.493665835373\n",
      "Iteration 10, inertia 1873.2370094127843\n",
      "Iteration 11, inertia 1873.0539565242373\n",
      "Iteration 12, inertia 1872.9371917626263\n",
      "Iteration 13, inertia 1872.8024019756897\n",
      "Iteration 14, inertia 1872.7043874695771\n",
      "Iteration 15, inertia 1872.64373436158\n",
      "Iteration 16, inertia 1872.637526317782\n",
      "Converged at iteration 16: center shift 5.289440772066211e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2821.652631024767\n",
      "Iteration 1, inertia 1926.998157119621\n",
      "Iteration 2, inertia 1858.7855386730978\n",
      "Iteration 3, inertia 1845.4123701744822\n",
      "Iteration 4, inertia 1841.8908067130058\n",
      "Iteration 5, inertia 1840.1308735460082\n",
      "Iteration 6, inertia 1839.413727936899\n",
      "Iteration 7, inertia 1839.0663440150806\n",
      "Iteration 8, inertia 1838.7499308529177\n",
      "Iteration 9, inertia 1838.369541961645\n",
      "Iteration 10, inertia 1838.0690785131078\n",
      "Iteration 11, inertia 1837.764296632482\n",
      "Iteration 12, inertia 1837.4202964247297\n",
      "Iteration 13, inertia 1837.095808681332\n",
      "Iteration 14, inertia 1837.010641083907\n",
      "Iteration 15, inertia 1836.9719616484008\n",
      "Iteration 16, inertia 1836.9488331809096\n",
      "Iteration 17, inertia 1836.9444945867149\n",
      "Iteration 18, inertia 1836.9432728127467\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2610.2249366511555\n",
      "Iteration 1, inertia 2023.544091259465\n",
      "Iteration 2, inertia 1950.2414934119731\n",
      "Iteration 3, inertia 1927.3224923298367\n",
      "Iteration 4, inertia 1917.0376786163056\n",
      "Iteration 5, inertia 1909.083026726476\n",
      "Iteration 6, inertia 1902.2470875221939\n",
      "Iteration 7, inertia 1897.1470937313056\n",
      "Iteration 8, inertia 1892.262917835666\n",
      "Iteration 9, inertia 1888.464469852089\n",
      "Iteration 10, inertia 1886.1542985170743\n",
      "Iteration 11, inertia 1884.0032138484362\n",
      "Iteration 12, inertia 1882.7591105110469\n",
      "Iteration 13, inertia 1881.961153139864\n",
      "Iteration 14, inertia 1881.4154945809341\n",
      "Iteration 15, inertia 1881.1229312385246\n",
      "Iteration 16, inertia 1881.0229563236394\n",
      "Iteration 17, inertia 1880.9570203260055\n",
      "Iteration 18, inertia 1880.9083959837003\n",
      "Iteration 19, inertia 1880.8501998712322\n",
      "Iteration 20, inertia 1880.717372037224\n",
      "Iteration 21, inertia 1880.6353283870699\n",
      "Iteration 22, inertia 1880.5725664889355\n",
      "Iteration 23, inertia 1880.538278769896\n",
      "Iteration 24, inertia 1880.5025455220991\n",
      "Iteration 25, inertia 1880.456650065744\n",
      "Iteration 26, inertia 1880.42470930121\n",
      "Iteration 27, inertia 1880.4010088020384\n",
      "Iteration 28, inertia 1880.3967851599728\n",
      "Converged at iteration 28: center shift 1.1263262410166996e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2697.024246388079\n",
      "Iteration 1, inertia 2037.4823501262163\n",
      "Iteration 2, inertia 1973.3381582859593\n",
      "Iteration 3, inertia 1926.1709609321736\n",
      "Iteration 4, inertia 1877.0768331455486\n",
      "Iteration 5, inertia 1852.3966752390522\n",
      "Iteration 6, inertia 1845.0096379638553\n",
      "Iteration 7, inertia 1843.091778651534\n",
      "Iteration 8, inertia 1842.2454607131474\n",
      "Iteration 9, inertia 1841.6104262354952\n",
      "Iteration 10, inertia 1841.3137712452701\n",
      "Iteration 11, inertia 1841.1243936049154\n",
      "Iteration 12, inertia 1841.0084279816658\n",
      "Iteration 13, inertia 1840.9455289244177\n",
      "Iteration 14, inertia 1840.9191663811648\n",
      "Iteration 15, inertia 1840.8654731077697\n",
      "Iteration 16, inertia 1840.8275951754076\n",
      "Iteration 17, inertia 1840.8059465076765\n",
      "Iteration 18, inertia 1840.8040421774017\n",
      "Iteration 19, inertia 1840.8003120811788\n",
      "Iteration 20, inertia 1840.7954877986365\n",
      "Iteration 21, inertia 1840.7934697775918\n",
      "Iteration 22, inertia 1840.792570408981\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2657.0308969664466\n",
      "Iteration 1, inertia 1944.881683187061\n",
      "Iteration 2, inertia 1869.0359525792442\n",
      "Iteration 3, inertia 1854.385248240836\n",
      "Iteration 4, inertia 1847.9387667499361\n",
      "Iteration 5, inertia 1842.558276572163\n",
      "Iteration 6, inertia 1837.9975714234315\n",
      "Iteration 7, inertia 1835.6703807467986\n",
      "Iteration 8, inertia 1833.9821813684598\n",
      "Iteration 9, inertia 1831.7664319368253\n",
      "Iteration 10, inertia 1827.7783719161505\n",
      "Iteration 11, inertia 1823.4811390996638\n",
      "Iteration 12, inertia 1819.9449399329371\n",
      "Iteration 13, inertia 1817.8745818773502\n",
      "Iteration 14, inertia 1816.5615592762347\n",
      "Iteration 15, inertia 1815.8798942418027\n",
      "Iteration 16, inertia 1815.1306634282075\n",
      "Iteration 17, inertia 1814.6147921710042\n",
      "Iteration 18, inertia 1814.2225879566186\n",
      "Iteration 19, inertia 1813.7709986199977\n",
      "Iteration 20, inertia 1813.4680029443912\n",
      "Iteration 21, inertia 1813.1774885727311\n",
      "Iteration 22, inertia 1813.0567273581382\n",
      "Iteration 23, inertia 1813.0214815634304\n",
      "Iteration 24, inertia 1812.9833731423919\n",
      "Iteration 25, inertia 1812.9641723877792\n",
      "Iteration 26, inertia 1812.9139826403648\n",
      "Iteration 27, inertia 1812.9048722265243\n",
      "Iteration 28, inertia 1812.889862141562\n",
      "Iteration 29, inertia 1812.8845594950544\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, inertia 2733.3039756382645\n",
      "Iteration 1, inertia 1934.5626225423698\n",
      "Iteration 2, inertia 1863.4182100169303\n",
      "Iteration 3, inertia 1839.3334454171998\n",
      "Iteration 4, inertia 1825.719992927663\n",
      "Iteration 5, inertia 1820.4320937180169\n",
      "Iteration 6, inertia 1817.9388953161472\n",
      "Iteration 7, inertia 1816.254816103134\n",
      "Iteration 8, inertia 1815.1964859613022\n",
      "Iteration 9, inertia 1814.597481237172\n",
      "Iteration 10, inertia 1814.1860896000198\n",
      "Iteration 11, inertia 1813.8665380518237\n",
      "Iteration 12, inertia 1813.6126091227404\n",
      "Iteration 13, inertia 1813.4234293823\n",
      "Iteration 14, inertia 1813.2349963650565\n",
      "Iteration 15, inertia 1813.1433438462527\n",
      "Iteration 16, inertia 1813.050445639556\n",
      "Iteration 17, inertia 1812.9983207248852\n",
      "Iteration 18, inertia 1812.9788535725995\n",
      "Iteration 19, inertia 1812.9694675003532\n",
      "Iteration 20, inertia 1812.9677250871448\n",
      "Converged at iteration 20: center shift 6.866833058265452e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2657.5704802503424\n",
      "Iteration 1, inertia 1942.1630011411632\n",
      "Iteration 2, inertia 1902.3550694754563\n",
      "Iteration 3, inertia 1884.4465742673665\n",
      "Iteration 4, inertia 1870.7382019249649\n",
      "Iteration 5, inertia 1859.8933078683413\n",
      "Iteration 6, inertia 1849.2729665767272\n",
      "Iteration 7, inertia 1841.61993861213\n",
      "Iteration 8, inertia 1837.599331265952\n",
      "Iteration 9, inertia 1835.5178539411975\n",
      "Iteration 10, inertia 1834.199262883662\n",
      "Iteration 11, inertia 1833.574084577018\n",
      "Iteration 12, inertia 1833.3581883815605\n",
      "Iteration 13, inertia 1833.282729263529\n",
      "Iteration 14, inertia 1833.2321156748887\n",
      "Iteration 15, inertia 1833.2058225249198\n",
      "Iteration 16, inertia 1833.1863087398772\n",
      "Iteration 17, inertia 1833.1754330169142\n",
      "Iteration 18, inertia 1833.1690080258038\n",
      "Iteration 19, inertia 1833.1608571659926\n",
      "Iteration 20, inertia 1833.155446230281\n",
      "Iteration 21, inertia 1833.1542663414327\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2614.876759312521\n",
      "Iteration 1, inertia 2050.3891705447127\n",
      "Iteration 2, inertia 1962.7083765890093\n",
      "Iteration 3, inertia 1931.5153325273936\n",
      "Iteration 4, inertia 1907.0075907621845\n",
      "Iteration 5, inertia 1885.6035097905615\n",
      "Iteration 6, inertia 1866.5576960296323\n",
      "Iteration 7, inertia 1859.899325899968\n",
      "Iteration 8, inertia 1857.1405506038102\n",
      "Iteration 9, inertia 1854.8776606711667\n",
      "Iteration 10, inertia 1853.3630848216626\n",
      "Iteration 11, inertia 1852.6907244464246\n",
      "Iteration 12, inertia 1852.0340438623414\n",
      "Iteration 13, inertia 1851.6161792312414\n",
      "Iteration 14, inertia 1851.293804629012\n",
      "Iteration 15, inertia 1851.0442198688509\n",
      "Iteration 16, inertia 1850.962428960957\n",
      "Iteration 17, inertia 1850.8857962768116\n",
      "Iteration 18, inertia 1850.8429437815275\n",
      "Iteration 19, inertia 1850.7949155970407\n",
      "Iteration 20, inertia 1850.6690181126621\n",
      "Iteration 21, inertia 1850.5997144374664\n",
      "Iteration 22, inertia 1850.5531046292956\n",
      "Iteration 23, inertia 1850.5033032132387\n",
      "Iteration 24, inertia 1850.4728539777154\n",
      "Iteration 25, inertia 1850.445031681963\n",
      "Iteration 26, inertia 1850.4285056626682\n",
      "Iteration 27, inertia 1850.4235565658123\n",
      "Iteration 28, inertia 1850.4211268010388\n",
      "Iteration 29, inertia 1850.4189932719155\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2596.1169972112093\n",
      "Iteration 1, inertia 1958.7329125434783\n",
      "Iteration 2, inertia 1931.6322876869044\n",
      "Iteration 3, inertia 1926.7107317266846\n",
      "Iteration 4, inertia 1923.565303446635\n",
      "Iteration 5, inertia 1919.4906622370559\n",
      "Iteration 6, inertia 1914.4207292268256\n",
      "Iteration 7, inertia 1908.3613053961358\n",
      "Iteration 8, inertia 1903.0418007580736\n",
      "Iteration 9, inertia 1899.269077186411\n",
      "Iteration 10, inertia 1896.5333589000188\n",
      "Iteration 11, inertia 1894.7498276295066\n",
      "Iteration 12, inertia 1893.0656173770994\n",
      "Iteration 13, inertia 1891.353893490053\n",
      "Iteration 14, inertia 1889.9832328263028\n",
      "Iteration 15, inertia 1889.5863326170615\n",
      "Iteration 16, inertia 1889.263657009244\n",
      "Iteration 17, inertia 1889.0652604236107\n",
      "Iteration 18, inertia 1888.966618917671\n",
      "Iteration 19, inertia 1888.9320057531745\n",
      "Iteration 20, inertia 1888.9129687498887\n",
      "Iteration 21, inertia 1888.8994658710503\n",
      "Iteration 22, inertia 1888.8850997238014\n",
      "Iteration 23, inertia 1888.878273567829\n",
      "Iteration 24, inertia 1888.876107672699\n",
      "Converged at iteration 24: center shift 4.446157682616965e-07 within tolerance 1.436774592781648e-06.\n",
      "Training a K-Means model with 11 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2660.4903405477053\n",
      "Iteration 1, inertia 1901.2425565326034\n",
      "Iteration 2, inertia 1861.6745520900652\n",
      "Iteration 3, inertia 1843.620969830717\n",
      "Iteration 4, inertia 1825.5776924940121\n",
      "Iteration 5, inertia 1813.0450598204804\n",
      "Iteration 6, inertia 1805.1436627880746\n",
      "Iteration 7, inertia 1801.5733302139467\n",
      "Iteration 8, inertia 1800.4229412411391\n",
      "Iteration 9, inertia 1799.728961310268\n",
      "Iteration 10, inertia 1799.141227117031\n",
      "Iteration 11, inertia 1798.7386979314665\n",
      "Iteration 12, inertia 1798.555199312613\n",
      "Iteration 13, inertia 1798.3750422357875\n",
      "Iteration 14, inertia 1798.2845327512543\n",
      "Iteration 15, inertia 1798.2206127487034\n",
      "Iteration 16, inertia 1798.1360636381467\n",
      "Iteration 17, inertia 1798.0286797341118\n",
      "Iteration 18, inertia 1797.9119004024371\n",
      "Iteration 19, inertia 1797.8582056333803\n",
      "Iteration 20, inertia 1797.8154115002972\n",
      "Iteration 21, inertia 1797.7995355841297\n",
      "Iteration 22, inertia 1797.7802049887148\n",
      "Iteration 23, inertia 1797.7651984856907\n",
      "Iteration 24, inertia 1797.7517192293997\n",
      "Iteration 25, inertia 1797.7351161518447\n",
      "Iteration 26, inertia 1797.7217932213612\n",
      "Iteration 27, inertia 1797.7123452001745\n",
      "Iteration 28, inertia 1797.703092507003\n",
      "Iteration 29, inertia 1797.6894823298187\n",
      "Iteration 30, inertia 1797.6427493163737\n",
      "Iteration 31, inertia 1797.5653234432818\n",
      "Iteration 32, inertia 1797.4791285393849\n",
      "Iteration 33, inertia 1797.4339850767528\n",
      "Iteration 34, inertia 1797.4126434551174\n",
      "Iteration 35, inertia 1797.4014931859576\n",
      "Iteration 36, inertia 1797.3972811936026\n",
      "Iteration 37, inertia 1797.3939164025032\n",
      "Iteration 38, inertia 1797.3886969812534\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2467.3862723136035\n",
      "Iteration 1, inertia 1912.8633723291412\n",
      "Iteration 2, inertia 1874.292627881005\n",
      "Iteration 3, inertia 1856.1067947795289\n",
      "Iteration 4, inertia 1837.9551975420354\n",
      "Iteration 5, inertia 1825.8640035756664\n",
      "Iteration 6, inertia 1818.5590677508271\n",
      "Iteration 7, inertia 1813.3685957231326\n",
      "Iteration 8, inertia 1810.38700176037\n",
      "Iteration 9, inertia 1808.577397669717\n",
      "Iteration 10, inertia 1807.6702989650048\n",
      "Iteration 11, inertia 1807.2124623547365\n",
      "Iteration 12, inertia 1807.0025920853661\n",
      "Iteration 13, inertia 1806.8915388391165\n",
      "Iteration 14, inertia 1806.8378163568846\n",
      "Iteration 15, inertia 1806.7756841959022\n",
      "Iteration 16, inertia 1806.7196037238584\n",
      "Iteration 17, inertia 1806.6607537841737\n",
      "Iteration 18, inertia 1806.6002963288124\n",
      "Iteration 19, inertia 1806.554721233825\n",
      "Iteration 20, inertia 1806.492227168296\n",
      "Iteration 21, inertia 1806.4170535283547\n",
      "Iteration 22, inertia 1806.3565958343688\n",
      "Iteration 23, inertia 1806.3002180758444\n",
      "Iteration 24, inertia 1806.2566782727733\n",
      "Iteration 25, inertia 1806.218475774707\n",
      "Iteration 26, inertia 1806.1798841899054\n",
      "Iteration 27, inertia 1806.1300633506546\n",
      "Iteration 28, inertia 1806.0917254154153\n",
      "Iteration 29, inertia 1806.0678404699454\n",
      "Iteration 30, inertia 1806.0445253606413\n",
      "Iteration 31, inertia 1806.0421726874147\n",
      "Converged at iteration 31: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2603.732734019\n",
      "Iteration 1, inertia 1934.7603449948792\n",
      "Iteration 2, inertia 1849.6012147532076\n",
      "Iteration 3, inertia 1818.0286949530137\n",
      "Iteration 4, inertia 1810.000056128008\n",
      "Iteration 5, inertia 1808.2502376311513\n",
      "Iteration 6, inertia 1807.5179613918165\n",
      "Iteration 7, inertia 1807.0103299989344\n",
      "Iteration 8, inertia 1806.597540721286\n",
      "Iteration 9, inertia 1806.2893729419848\n",
      "Iteration 10, inertia 1806.0250487759154\n",
      "Iteration 11, inertia 1805.8478192000846\n",
      "Iteration 12, inertia 1805.7196572739865\n",
      "Iteration 13, inertia 1805.5978084141225\n",
      "Iteration 14, inertia 1805.5170487899159\n",
      "Iteration 15, inertia 1805.3957843692556\n",
      "Iteration 16, inertia 1805.2748841838809\n",
      "Iteration 17, inertia 1805.208358445757\n",
      "Iteration 18, inertia 1805.184954387125\n",
      "Iteration 19, inertia 1805.1828082320185\n",
      "Iteration 20, inertia 1805.176504787194\n",
      "Iteration 21, inertia 1805.1737822042833\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2520.309088106481\n",
      "Iteration 1, inertia 1935.3682639634158\n",
      "Iteration 2, inertia 1879.273455325022\n",
      "Iteration 3, inertia 1851.1443003465806\n",
      "Iteration 4, inertia 1824.7547403386918\n",
      "Iteration 5, inertia 1809.9284957408295\n",
      "Iteration 6, inertia 1801.71529200071\n",
      "Iteration 7, inertia 1797.092400365696\n",
      "Iteration 8, inertia 1795.8158484738274\n",
      "Iteration 9, inertia 1795.2028734106495\n",
      "Iteration 10, inertia 1794.8555384468868\n",
      "Iteration 11, inertia 1794.652236284579\n",
      "Iteration 12, inertia 1794.5196956609093\n",
      "Iteration 13, inertia 1794.4429140559519\n",
      "Iteration 14, inertia 1794.3645578974642\n",
      "Iteration 15, inertia 1794.2421844500893\n",
      "Iteration 16, inertia 1794.1608032148988\n",
      "Iteration 17, inertia 1794.0753706375986\n",
      "Iteration 18, inertia 1794.0251417061243\n",
      "Iteration 19, inertia 1793.9748517995497\n",
      "Iteration 20, inertia 1793.9324644283197\n",
      "Iteration 21, inertia 1793.8947951261764\n",
      "Iteration 22, inertia 1793.848907952956\n",
      "Iteration 23, inertia 1793.7893324051074\n",
      "Iteration 24, inertia 1793.718699533256\n",
      "Iteration 25, inertia 1793.7057736201714\n",
      "Iteration 26, inertia 1793.6883413108496\n",
      "Iteration 27, inertia 1793.667512690179\n",
      "Iteration 28, inertia 1793.623020294536\n",
      "Iteration 29, inertia 1793.493963305274\n",
      "Iteration 30, inertia 1793.3982615080818\n",
      "Iteration 31, inertia 1793.354482827457\n",
      "Iteration 32, inertia 1793.3014817701783\n",
      "Iteration 33, inertia 1793.264064329623\n",
      "Iteration 34, inertia 1793.2431860209288\n",
      "Iteration 35, inertia 1793.2280687415903\n",
      "Iteration 36, inertia 1793.221195161078\n",
      "Iteration 37, inertia 1793.215319841619\n",
      "Iteration 38, inertia 1793.2094073248365\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2486.40489223892\n",
      "Iteration 1, inertia 1971.5068174338198\n",
      "Iteration 2, inertia 1892.1184392268085\n",
      "Iteration 3, inertia 1848.496848669542\n",
      "Iteration 4, inertia 1825.5972216016512\n",
      "Iteration 5, inertia 1816.4970419553933\n",
      "Iteration 6, inertia 1812.579981315337\n",
      "Iteration 7, inertia 1810.1853873914165\n",
      "Iteration 8, inertia 1808.7243609000964\n",
      "Iteration 9, inertia 1807.8956324075855\n",
      "Iteration 10, inertia 1807.3823473682673\n",
      "Iteration 11, inertia 1806.9923123160775\n",
      "Iteration 12, inertia 1806.595254198138\n",
      "Iteration 13, inertia 1806.203136912071\n",
      "Iteration 14, inertia 1806.0250403703478\n",
      "Iteration 15, inertia 1805.8720099196332\n",
      "Iteration 16, inertia 1805.758329470677\n",
      "Iteration 17, inertia 1805.6653190818902\n",
      "Iteration 18, inertia 1805.565491187833\n",
      "Iteration 19, inertia 1805.5015796290081\n",
      "Iteration 20, inertia 1805.4531428540472\n",
      "Iteration 21, inertia 1805.4281182713726\n",
      "Iteration 22, inertia 1805.3894204439239\n",
      "Iteration 23, inertia 1805.3824404047748\n",
      "Iteration 24, inertia 1805.3807253519637\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2672.214544588954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, inertia 1967.7496575645837\n",
      "Iteration 2, inertia 1872.671367136907\n",
      "Iteration 3, inertia 1837.3617216743532\n",
      "Iteration 4, inertia 1821.2855903928078\n",
      "Iteration 5, inertia 1810.7366419535613\n",
      "Iteration 6, inertia 1803.5175074473984\n",
      "Iteration 7, inertia 1799.342740873064\n",
      "Iteration 8, inertia 1796.2262934766095\n",
      "Iteration 9, inertia 1793.6913433902942\n",
      "Iteration 10, inertia 1791.7769831718497\n",
      "Iteration 11, inertia 1790.9289785827925\n",
      "Iteration 12, inertia 1789.9267190034902\n",
      "Iteration 13, inertia 1788.7930326691617\n",
      "Iteration 14, inertia 1788.1406258911823\n",
      "Iteration 15, inertia 1787.9207877983993\n",
      "Iteration 16, inertia 1787.866208916146\n",
      "Iteration 17, inertia 1787.8264733645353\n",
      "Iteration 18, inertia 1787.8125361555672\n",
      "Iteration 19, inertia 1787.7945929086893\n",
      "Iteration 20, inertia 1787.7746107396356\n",
      "Iteration 21, inertia 1787.7626394583676\n",
      "Iteration 22, inertia 1787.7596225967318\n",
      "Iteration 23, inertia 1787.7568536495944\n",
      "Iteration 24, inertia 1787.7530954507383\n",
      "Iteration 25, inertia 1787.7451262881887\n",
      "Iteration 26, inertia 1787.7337592502254\n",
      "Iteration 27, inertia 1787.722862739126\n",
      "Iteration 28, inertia 1787.7083821339368\n",
      "Iteration 29, inertia 1787.7032074809622\n",
      "Iteration 30, inertia 1787.69668038545\n",
      "Iteration 31, inertia 1787.6946624378388\n",
      "Converged at iteration 31: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2474.231342436063\n",
      "Iteration 1, inertia 1910.5915648704017\n",
      "Iteration 2, inertia 1871.9512121387029\n",
      "Iteration 3, inertia 1860.527867960007\n",
      "Iteration 4, inertia 1855.2463319218036\n",
      "Iteration 5, inertia 1851.7744469688068\n",
      "Iteration 6, inertia 1844.8313136589247\n",
      "Iteration 7, inertia 1830.0610743329498\n",
      "Iteration 8, inertia 1809.6432340833153\n",
      "Iteration 9, inertia 1790.8658927282358\n",
      "Iteration 10, inertia 1776.269631909376\n",
      "Iteration 11, inertia 1770.8635710914095\n",
      "Iteration 12, inertia 1768.1022366643929\n",
      "Iteration 13, inertia 1766.8337937862675\n",
      "Iteration 14, inertia 1766.2515798883169\n",
      "Iteration 15, inertia 1765.9948117562012\n",
      "Iteration 16, inertia 1765.8509835855566\n",
      "Iteration 17, inertia 1765.7480551577535\n",
      "Iteration 18, inertia 1765.6653112036684\n",
      "Iteration 19, inertia 1765.5743440980214\n",
      "Iteration 20, inertia 1765.518081651241\n",
      "Iteration 21, inertia 1765.4689534118606\n",
      "Iteration 22, inertia 1765.452098470748\n",
      "Iteration 23, inertia 1765.4336795005515\n",
      "Iteration 24, inertia 1765.4107826642908\n",
      "Iteration 25, inertia 1765.3987918026519\n",
      "Iteration 26, inertia 1765.3880106261026\n",
      "Iteration 27, inertia 1765.3810422732204\n",
      "Iteration 28, inertia 1765.3799725502893\n",
      "Converged at iteration 28: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2705.476570650206\n",
      "Iteration 1, inertia 1885.2608257781187\n",
      "Iteration 2, inertia 1823.4502023223006\n",
      "Iteration 3, inertia 1796.5247802773595\n",
      "Iteration 4, inertia 1778.56047300596\n",
      "Iteration 5, inertia 1770.931607908849\n",
      "Iteration 6, inertia 1767.9001165447833\n",
      "Iteration 7, inertia 1766.2945128377378\n",
      "Iteration 8, inertia 1765.1465449694365\n",
      "Iteration 9, inertia 1764.1874512965976\n",
      "Iteration 10, inertia 1763.7780342176893\n",
      "Iteration 11, inertia 1763.586347841328\n",
      "Iteration 12, inertia 1763.4598434128432\n",
      "Iteration 13, inertia 1763.3604854740315\n",
      "Iteration 14, inertia 1763.297267793479\n",
      "Iteration 15, inertia 1763.2645443293418\n",
      "Iteration 16, inertia 1763.2490117005896\n",
      "Iteration 17, inertia 1763.2336763254323\n",
      "Iteration 18, inertia 1763.2242618832279\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2780.5311539433073\n",
      "Iteration 1, inertia 1885.0370906478809\n",
      "Iteration 2, inertia 1807.623204997946\n",
      "Iteration 3, inertia 1785.9907960851806\n",
      "Iteration 4, inertia 1775.5792869292015\n",
      "Iteration 5, inertia 1770.8288593883485\n",
      "Iteration 6, inertia 1768.947503070798\n",
      "Iteration 7, inertia 1767.7357905310657\n",
      "Iteration 8, inertia 1766.8901759103837\n",
      "Iteration 9, inertia 1766.2994709810916\n",
      "Iteration 10, inertia 1766.1391453464605\n",
      "Iteration 11, inertia 1766.0257771848312\n",
      "Iteration 12, inertia 1765.879182557803\n",
      "Iteration 13, inertia 1765.616498416899\n",
      "Iteration 14, inertia 1765.4459856178505\n",
      "Iteration 15, inertia 1765.3272456118775\n",
      "Iteration 16, inertia 1765.2887566761667\n",
      "Iteration 17, inertia 1765.2679893289546\n",
      "Iteration 18, inertia 1765.254433408824\n",
      "Iteration 19, inertia 1765.2375153329249\n",
      "Iteration 20, inertia 1765.2251469371618\n",
      "Iteration 21, inertia 1765.2106136741256\n",
      "Iteration 22, inertia 1765.2069021308719\n",
      "Iteration 23, inertia 1765.2019585510027\n",
      "Iteration 24, inertia 1765.1938293878125\n",
      "Iteration 25, inertia 1765.1904453679192\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2410.1113431501635\n",
      "Iteration 1, inertia 1873.4396988064873\n",
      "Iteration 2, inertia 1841.4428961655958\n",
      "Iteration 3, inertia 1837.2504110963505\n",
      "Iteration 4, inertia 1836.129904253146\n",
      "Iteration 5, inertia 1835.684072253417\n",
      "Iteration 6, inertia 1835.3894076758984\n",
      "Iteration 7, inertia 1835.1495183576062\n",
      "Iteration 8, inertia 1835.0495225751877\n",
      "Iteration 9, inertia 1834.942930943387\n",
      "Iteration 10, inertia 1834.855827784682\n",
      "Iteration 11, inertia 1834.7515518079563\n",
      "Iteration 12, inertia 1834.6260687717702\n",
      "Iteration 13, inertia 1834.5635601092351\n",
      "Iteration 14, inertia 1834.5095559154731\n",
      "Iteration 15, inertia 1834.485899362302\n",
      "Iteration 16, inertia 1834.4813177408369\n",
      "Converged at iteration 16: strict convergence.\n",
      "Training a K-Means model with 12 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2528.7922215925228\n",
      "Iteration 1, inertia 1887.6615242533392\n",
      "Iteration 2, inertia 1845.443338089981\n",
      "Iteration 3, inertia 1827.009304717109\n",
      "Iteration 4, inertia 1812.7261698917164\n",
      "Iteration 5, inertia 1799.2122106450752\n",
      "Iteration 6, inertia 1784.5382084124872\n",
      "Iteration 7, inertia 1775.6261148798735\n",
      "Iteration 8, inertia 1770.5841366632044\n",
      "Iteration 9, inertia 1769.0615475456461\n",
      "Iteration 10, inertia 1768.5576464488684\n",
      "Iteration 11, inertia 1768.3514052762587\n",
      "Iteration 12, inertia 1768.3160914269854\n",
      "Iteration 13, inertia 1768.294046762692\n",
      "Iteration 14, inertia 1768.2711157606288\n",
      "Iteration 15, inertia 1768.251200973427\n",
      "Iteration 16, inertia 1768.2385369431101\n",
      "Iteration 17, inertia 1768.2260159032273\n",
      "Iteration 18, inertia 1768.1992587645543\n",
      "Iteration 19, inertia 1768.1518117880291\n",
      "Iteration 20, inertia 1768.125541114105\n",
      "Iteration 21, inertia 1768.112376353149\n",
      "Iteration 22, inertia 1768.093748060738\n",
      "Iteration 23, inertia 1768.0752635218237\n",
      "Iteration 24, inertia 1768.0523219826296\n",
      "Iteration 25, inertia 1768.0328072893567\n",
      "Iteration 26, inertia 1768.0197997623056\n",
      "Iteration 27, inertia 1768.0054542426676\n",
      "Iteration 28, inertia 1768.0009362450467\n",
      "Iteration 29, inertia 1768.0001436766925\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2535.6416029998927\n",
      "Iteration 1, inertia 1923.394720481242\n",
      "Iteration 2, inertia 1830.1272241703505\n",
      "Iteration 3, inertia 1792.6539176647286\n",
      "Iteration 4, inertia 1782.329677356475\n",
      "Iteration 5, inertia 1777.4782658633853\n",
      "Iteration 6, inertia 1774.2969422701174\n",
      "Iteration 7, inertia 1770.9885923827046\n",
      "Iteration 8, inertia 1767.6907273219874\n",
      "Iteration 9, inertia 1763.0948827220177\n",
      "Iteration 10, inertia 1760.3761477341266\n",
      "Iteration 11, inertia 1758.5340178181614\n",
      "Iteration 12, inertia 1757.5595748047656\n",
      "Iteration 13, inertia 1757.0393678648331\n",
      "Iteration 14, inertia 1756.870460189462\n",
      "Iteration 15, inertia 1756.7656506259407\n",
      "Iteration 16, inertia 1756.7024617230563\n",
      "Iteration 17, inertia 1756.6568795241083\n",
      "Iteration 18, inertia 1756.6356374167863\n",
      "Iteration 19, inertia 1756.6133019865413\n",
      "Iteration 20, inertia 1756.5628897054562\n",
      "Iteration 21, inertia 1756.5369320835425\n",
      "Iteration 22, inertia 1756.4767181673221\n",
      "Iteration 23, inertia 1756.4526827482978\n",
      "Iteration 24, inertia 1756.4250590896218\n",
      "Iteration 25, inertia 1756.4142947791481\n",
      "Iteration 26, inertia 1756.4011348823524\n",
      "Iteration 27, inertia 1756.3692705804565\n",
      "Iteration 28, inertia 1756.3462311146611\n",
      "Iteration 29, inertia 1756.3427258177753\n",
      "Iteration 30, inertia 1756.3409948254066\n",
      "Iteration 31, inertia 1756.3374907396028\n",
      "Converged at iteration 31: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2432.7979196810747\n",
      "Iteration 1, inertia 1864.1104591492053\n",
      "Iteration 2, inertia 1813.5259904988836\n",
      "Iteration 3, inertia 1800.8694430461787\n",
      "Iteration 4, inertia 1794.429819947482\n",
      "Iteration 5, inertia 1789.65985394464\n",
      "Iteration 6, inertia 1781.5183910410822\n",
      "Iteration 7, inertia 1769.0292649492699\n",
      "Iteration 8, inertia 1754.6171860983409\n",
      "Iteration 9, inertia 1745.545217859534\n",
      "Iteration 10, inertia 1740.1762753985884\n",
      "Iteration 11, inertia 1737.0482583178498\n",
      "Iteration 12, inertia 1735.1320396341844\n",
      "Iteration 13, inertia 1734.1995940175154\n",
      "Iteration 14, inertia 1733.8580152729733\n",
      "Iteration 15, inertia 1733.6521025090885\n",
      "Iteration 16, inertia 1733.5541347398346\n",
      "Iteration 17, inertia 1733.4458870691165\n",
      "Iteration 18, inertia 1733.3922276308851\n",
      "Iteration 19, inertia 1733.379713136636\n",
      "Iteration 20, inertia 1733.3756608760732\n",
      "Iteration 21, inertia 1733.3721851746495\n",
      "Iteration 22, inertia 1733.368927663587\n",
      "Iteration 23, inertia 1733.3641426928966\n",
      "Iteration 24, inertia 1733.361747975947\n",
      "Iteration 25, inertia 1733.3582699642104\n",
      "Iteration 26, inertia 1733.3515516202344\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2564.176329954778\n",
      "Iteration 1, inertia 1893.898194444362\n",
      "Iteration 2, inertia 1818.8634245530257\n",
      "Iteration 3, inertia 1786.4273945938896\n",
      "Iteration 4, inertia 1771.584089273349\n",
      "Iteration 5, inertia 1767.3379261253335\n",
      "Iteration 6, inertia 1765.1706773816452\n",
      "Iteration 7, inertia 1764.239960255878\n",
      "Iteration 8, inertia 1763.5695967009015\n",
      "Iteration 9, inertia 1763.0743406180645\n",
      "Iteration 10, inertia 1762.7053939542786\n",
      "Iteration 11, inertia 1762.375902512927\n",
      "Iteration 12, inertia 1762.1297390337384\n",
      "Iteration 13, inertia 1762.0723641387917\n",
      "Iteration 14, inertia 1762.0581756840352\n",
      "Iteration 15, inertia 1762.0485261276594\n",
      "Iteration 16, inertia 1762.034852497603\n",
      "Iteration 17, inertia 1762.0164011743545\n",
      "Iteration 18, inertia 1761.9986468268798\n",
      "Iteration 19, inertia 1761.9942675263026\n",
      "Iteration 20, inertia 1761.9930508157206\n",
      "Iteration 21, inertia 1761.985732402787\n",
      "Iteration 22, inertia 1761.981960266717\n",
      "Iteration 23, inertia 1761.9772823858173\n",
      "Converged at iteration 23: center shift 8.357943332578619e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2529.487434720773\n",
      "Iteration 1, inertia 1915.5980475571698\n",
      "Iteration 2, inertia 1844.9119610894606\n",
      "Iteration 3, inertia 1822.3459366720804\n",
      "Iteration 4, inertia 1811.3425419652285\n",
      "Iteration 5, inertia 1805.3173484346285\n",
      "Iteration 6, inertia 1801.390693677765\n",
      "Iteration 7, inertia 1799.1005836813836\n",
      "Iteration 8, inertia 1797.4627023606972\n",
      "Iteration 9, inertia 1796.25470444393\n",
      "Iteration 10, inertia 1795.500287237161\n",
      "Iteration 11, inertia 1794.7137667414015\n",
      "Iteration 12, inertia 1794.4623485802665\n",
      "Iteration 13, inertia 1794.3636734965112\n",
      "Iteration 14, inertia 1794.2440651860163\n",
      "Iteration 15, inertia 1794.1009747187413\n",
      "Iteration 16, inertia 1793.9187305902053\n",
      "Iteration 17, inertia 1793.7835566484055\n",
      "Iteration 18, inertia 1793.6317387659003\n",
      "Iteration 19, inertia 1793.519582533953\n",
      "Iteration 20, inertia 1793.450662794878\n",
      "Iteration 21, inertia 1793.3529349992632\n",
      "Iteration 22, inertia 1793.2305983506565\n",
      "Iteration 23, inertia 1793.1279721442831\n",
      "Iteration 24, inertia 1793.0596297931945\n",
      "Iteration 25, inertia 1792.9886882125259\n",
      "Iteration 26, inertia 1792.9133410670122\n",
      "Iteration 27, inertia 1792.8734906600516\n",
      "Iteration 28, inertia 1792.8461154184251\n",
      "Iteration 29, inertia 1792.831970815775\n",
      "Iteration 30, inertia 1792.8083075812835\n",
      "Iteration 31, inertia 1792.7845430167079\n",
      "Iteration 32, inertia 1792.7680835154158\n",
      "Iteration 33, inertia 1792.751010501868\n",
      "Iteration 34, inertia 1792.7422986300076\n",
      "Iteration 35, inertia 1792.7405942997582\n",
      "Converged at iteration 35: center shift 1.2433771893987653e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2420.056093960545\n",
      "Iteration 1, inertia 1808.9344133379548\n",
      "Iteration 2, inertia 1773.984150417779\n",
      "Iteration 3, inertia 1762.8769624489125\n",
      "Iteration 4, inertia 1755.8813366799523\n",
      "Iteration 5, inertia 1752.8895471637315\n",
      "Iteration 6, inertia 1750.6428605300375\n",
      "Iteration 7, inertia 1748.5686934612768\n",
      "Iteration 8, inertia 1745.1294750235563\n",
      "Iteration 9, inertia 1740.1279894810193\n",
      "Iteration 10, inertia 1738.5023580759093\n",
      "Iteration 11, inertia 1738.1848844272258\n",
      "Iteration 12, inertia 1738.0617165634258\n",
      "Iteration 13, inertia 1737.962632641233\n",
      "Iteration 14, inertia 1737.867999437864\n",
      "Iteration 15, inertia 1737.778548963165\n",
      "Iteration 16, inertia 1737.724284840172\n",
      "Iteration 17, inertia 1737.6674205501301\n",
      "Iteration 18, inertia 1737.6270814168315\n",
      "Iteration 19, inertia 1737.6121610645218\n",
      "Iteration 20, inertia 1737.584986721144\n",
      "Iteration 21, inertia 1737.4898929528433\n",
      "Iteration 22, inertia 1737.1281874519193\n",
      "Iteration 23, inertia 1736.0079045020507\n",
      "Iteration 24, inertia 1734.0394396395038\n",
      "Iteration 25, inertia 1732.716217795399\n",
      "Iteration 26, inertia 1732.1247311221587\n",
      "Iteration 27, inertia 1731.7327901693593\n",
      "Iteration 28, inertia 1731.5418442128666\n",
      "Iteration 29, inertia 1731.4484399946411\n",
      "Iteration 30, inertia 1731.3795089993012\n",
      "Iteration 31, inertia 1731.3267520244408\n",
      "Iteration 32, inertia 1731.3229239230207\n",
      "Iteration 33, inertia 1731.3203120528847\n",
      "Converged at iteration 33: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2360.1902580198866\n",
      "Iteration 1, inertia 1833.9926599082178\n",
      "Iteration 2, inertia 1796.7176282716837\n",
      "Iteration 3, inertia 1786.3719527798971\n",
      "Iteration 4, inertia 1781.7420456120833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, inertia 1779.5537712420096\n",
      "Iteration 6, inertia 1778.3264653484346\n",
      "Iteration 7, inertia 1777.6599650293692\n",
      "Iteration 8, inertia 1777.3185163782264\n",
      "Iteration 9, inertia 1777.1745000703909\n",
      "Iteration 10, inertia 1777.1197306815782\n",
      "Iteration 11, inertia 1777.064384856108\n",
      "Iteration 12, inertia 1777.0535494355324\n",
      "Converged at iteration 12: center shift 6.338389350477308e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2449.294129734136\n",
      "Iteration 1, inertia 1867.5614651798323\n",
      "Iteration 2, inertia 1797.8748775689264\n",
      "Iteration 3, inertia 1774.505361819314\n",
      "Iteration 4, inertia 1763.2098709073352\n",
      "Iteration 5, inertia 1753.188216193269\n",
      "Iteration 6, inertia 1745.980116876706\n",
      "Iteration 7, inertia 1742.7249600525663\n",
      "Iteration 8, inertia 1740.8127893951685\n",
      "Iteration 9, inertia 1739.5648287232596\n",
      "Iteration 10, inertia 1738.223011469225\n",
      "Iteration 11, inertia 1737.9185970674423\n",
      "Iteration 12, inertia 1737.7445498122197\n",
      "Iteration 13, inertia 1737.5799128129759\n",
      "Iteration 14, inertia 1737.4737347229125\n",
      "Iteration 15, inertia 1737.4113925147547\n",
      "Iteration 16, inertia 1737.3864595975194\n",
      "Iteration 17, inertia 1737.3833244625448\n",
      "Iteration 18, inertia 1737.3800776977428\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2418.6473589101543\n",
      "Iteration 1, inertia 1902.269271821077\n",
      "Iteration 2, inertia 1852.3891192094702\n",
      "Iteration 3, inertia 1829.4664074303191\n",
      "Iteration 4, inertia 1815.8121798580707\n",
      "Iteration 5, inertia 1802.6657300541522\n",
      "Iteration 6, inertia 1790.210590319868\n",
      "Iteration 7, inertia 1777.0890075338086\n",
      "Iteration 8, inertia 1763.0674871855267\n",
      "Iteration 9, inertia 1755.3629345419604\n",
      "Iteration 10, inertia 1752.135450608283\n",
      "Iteration 11, inertia 1749.7996751817336\n",
      "Iteration 12, inertia 1748.0926469427661\n",
      "Iteration 13, inertia 1747.2122736790373\n",
      "Iteration 14, inertia 1746.750507630383\n",
      "Iteration 15, inertia 1746.419460005633\n",
      "Iteration 16, inertia 1746.2157411973017\n",
      "Iteration 17, inertia 1746.0275603793693\n",
      "Iteration 18, inertia 1745.9558107775151\n",
      "Iteration 19, inertia 1745.8814093639905\n",
      "Iteration 20, inertia 1745.8448072529204\n",
      "Iteration 21, inertia 1745.8204751903177\n",
      "Iteration 22, inertia 1745.8158603446143\n",
      "Iteration 23, inertia 1745.8146606338237\n",
      "Converged at iteration 23: center shift 1.0576420681781755e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2394.34419810358\n",
      "Iteration 1, inertia 1855.7857669150662\n",
      "Iteration 2, inertia 1802.2692975121954\n",
      "Iteration 3, inertia 1776.9107071341339\n",
      "Iteration 4, inertia 1763.89340219158\n",
      "Iteration 5, inertia 1755.4111291772545\n",
      "Iteration 6, inertia 1750.886948027051\n",
      "Iteration 7, inertia 1748.2205769252128\n",
      "Iteration 8, inertia 1746.4781545031856\n",
      "Iteration 9, inertia 1744.9571235452106\n",
      "Iteration 10, inertia 1744.1398459691886\n",
      "Iteration 11, inertia 1743.4277585906484\n",
      "Iteration 12, inertia 1742.8457210409513\n",
      "Iteration 13, inertia 1742.4887993782997\n",
      "Iteration 14, inertia 1742.3293886517347\n",
      "Iteration 15, inertia 1742.0698501591976\n",
      "Iteration 16, inertia 1741.569830402792\n",
      "Iteration 17, inertia 1741.2117605588228\n",
      "Iteration 18, inertia 1740.5852980777613\n",
      "Iteration 19, inertia 1740.115375353339\n",
      "Iteration 20, inertia 1739.7012113777967\n",
      "Iteration 21, inertia 1739.3128437688822\n",
      "Iteration 22, inertia 1739.0448658387295\n",
      "Iteration 23, inertia 1738.704768268187\n",
      "Iteration 24, inertia 1738.3977567591241\n",
      "Iteration 25, inertia 1738.0287582094481\n",
      "Iteration 26, inertia 1737.9410875431329\n",
      "Iteration 27, inertia 1737.92182292163\n",
      "Iteration 28, inertia 1737.9073573626774\n",
      "Converged at iteration 28: strict convergence.\n",
      "Training a K-Means model with 13 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2418.08137288656\n",
      "Iteration 1, inertia 1809.2761335090347\n",
      "Iteration 2, inertia 1772.1058803006304\n",
      "Iteration 3, inertia 1758.5699885338192\n",
      "Iteration 4, inertia 1751.9512346380873\n",
      "Iteration 5, inertia 1746.8381301494812\n",
      "Iteration 6, inertia 1743.1289025450276\n",
      "Iteration 7, inertia 1740.7619119258893\n",
      "Iteration 8, inertia 1739.0288081717354\n",
      "Iteration 9, inertia 1737.8779076384224\n",
      "Iteration 10, inertia 1737.227109407349\n",
      "Iteration 11, inertia 1736.6638292124703\n",
      "Iteration 12, inertia 1736.0365364580173\n",
      "Iteration 13, inertia 1735.4043162714286\n",
      "Iteration 14, inertia 1734.8088224289213\n",
      "Iteration 15, inertia 1733.1893487804523\n",
      "Iteration 16, inertia 1730.0508741707984\n",
      "Iteration 17, inertia 1727.4463059790778\n",
      "Iteration 18, inertia 1725.9312808035756\n",
      "Iteration 19, inertia 1725.3482429061303\n",
      "Iteration 20, inertia 1725.0801942009402\n",
      "Iteration 21, inertia 1724.84890898138\n",
      "Iteration 22, inertia 1724.6522057153934\n",
      "Iteration 23, inertia 1724.4407842530356\n",
      "Iteration 24, inertia 1724.28764674637\n",
      "Iteration 25, inertia 1724.1807384933093\n",
      "Iteration 26, inertia 1724.0864684937096\n",
      "Iteration 27, inertia 1724.0508561083457\n",
      "Iteration 28, inertia 1724.0181116273418\n",
      "Iteration 29, inertia 1724.0049294284745\n",
      "Iteration 30, inertia 1723.9962426882973\n",
      "Iteration 31, inertia 1723.9889863758547\n",
      "Iteration 32, inertia 1723.9875935181508\n",
      "Converged at iteration 32: center shift 1.2288703019998243e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2345.489153675761\n",
      "Iteration 1, inertia 1829.567853841134\n",
      "Iteration 2, inertia 1768.280647393683\n",
      "Iteration 3, inertia 1752.736323497818\n",
      "Iteration 4, inertia 1743.8740736718264\n",
      "Iteration 5, inertia 1737.018117405087\n",
      "Iteration 6, inertia 1729.7593279946445\n",
      "Iteration 7, inertia 1721.6849150410064\n",
      "Iteration 8, inertia 1716.118769170875\n",
      "Iteration 9, inertia 1713.5741078674553\n",
      "Iteration 10, inertia 1712.3601438947042\n",
      "Iteration 11, inertia 1711.789373097757\n",
      "Iteration 12, inertia 1711.4202386403545\n",
      "Iteration 13, inertia 1711.2011116213378\n",
      "Iteration 14, inertia 1710.8526491794355\n",
      "Iteration 15, inertia 1710.5403878835157\n",
      "Iteration 16, inertia 1710.2834687789234\n",
      "Iteration 17, inertia 1710.128763221337\n",
      "Iteration 18, inertia 1710.0440993682519\n",
      "Iteration 19, inertia 1710.0175476942777\n",
      "Iteration 20, inertia 1710.002892849551\n",
      "Iteration 21, inertia 1709.9856553688599\n",
      "Iteration 22, inertia 1709.9553117337368\n",
      "Iteration 23, inertia 1709.9136120218693\n",
      "Iteration 24, inertia 1709.846297753574\n",
      "Iteration 25, inertia 1709.6949739626102\n",
      "Iteration 26, inertia 1709.4402101647215\n",
      "Iteration 27, inertia 1709.093083237126\n",
      "Iteration 28, inertia 1708.9527861960041\n",
      "Iteration 29, inertia 1708.868951611058\n",
      "Iteration 30, inertia 1708.8422162761879\n",
      "Iteration 31, inertia 1708.8134042705585\n",
      "Iteration 32, inertia 1708.7803419299312\n",
      "Iteration 33, inertia 1708.7556840027926\n",
      "Iteration 34, inertia 1708.7337655548629\n",
      "Iteration 35, inertia 1708.7186452250514\n",
      "Iteration 36, inertia 1708.7109892430221\n",
      "Iteration 37, inertia 1708.702948995912\n",
      "Iteration 38, inertia 1708.6983857834377\n",
      "Iteration 39, inertia 1708.695319756744\n",
      "Iteration 40, inertia 1708.6941893429089\n",
      "Iteration 41, inertia 1708.6918144475105\n",
      "Iteration 42, inertia 1708.6880862024525\n",
      "Iteration 43, inertia 1708.6734288669018\n",
      "Iteration 44, inertia 1708.6435493785893\n",
      "Iteration 45, inertia 1708.5829391446387\n",
      "Iteration 46, inertia 1708.5166886881605\n",
      "Iteration 47, inertia 1708.482923161549\n",
      "Iteration 48, inertia 1708.4751285887173\n",
      "Iteration 49, inertia 1708.4720964213307\n",
      "Iteration 50, inertia 1708.4700862708441\n",
      "Converged at iteration 50: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2472.741295629397\n",
      "Iteration 1, inertia 1814.3005227807255\n",
      "Iteration 2, inertia 1767.379162470679\n",
      "Iteration 3, inertia 1732.6323015761698\n",
      "Iteration 4, inertia 1716.9353426114833\n",
      "Iteration 5, inertia 1709.7022863783654\n",
      "Iteration 6, inertia 1707.0460816532825\n",
      "Iteration 7, inertia 1705.8279331948472\n",
      "Iteration 8, inertia 1705.4046489297964\n",
      "Iteration 9, inertia 1705.0540039710563\n",
      "Iteration 10, inertia 1704.752368110754\n",
      "Iteration 11, inertia 1704.642956424805\n",
      "Iteration 12, inertia 1704.5765908095696\n",
      "Iteration 13, inertia 1704.533925773388\n",
      "Iteration 14, inertia 1704.4747717825558\n",
      "Iteration 15, inertia 1704.43651406012\n",
      "Iteration 16, inertia 1704.4227353433882\n",
      "Iteration 17, inertia 1704.4198332195465\n",
      "Converged at iteration 17: center shift 5.847512689280156e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2400.264219106252\n",
      "Iteration 1, inertia 1868.3268362427677\n",
      "Iteration 2, inertia 1826.952340325246\n",
      "Iteration 3, inertia 1808.8014665057358\n",
      "Iteration 4, inertia 1792.4100547090388\n",
      "Iteration 5, inertia 1773.383456876292\n",
      "Iteration 6, inertia 1756.3535448824382\n",
      "Iteration 7, inertia 1748.0011500579062\n",
      "Iteration 8, inertia 1743.3761278036175\n",
      "Iteration 9, inertia 1742.1094638268917\n",
      "Iteration 10, inertia 1741.5639263498772\n",
      "Iteration 11, inertia 1741.295188027597\n",
      "Iteration 12, inertia 1741.1822535424194\n",
      "Iteration 13, inertia 1741.129058323631\n",
      "Iteration 14, inertia 1741.10265917215\n",
      "Iteration 15, inertia 1741.076104010763\n",
      "Iteration 16, inertia 1741.0517638182005\n",
      "Iteration 17, inertia 1741.0257556556094\n",
      "Iteration 18, inertia 1741.0118991465097\n",
      "Iteration 19, inertia 1741.0034649494582\n",
      "Iteration 20, inertia 1741.000876477831\n",
      "Iteration 21, inertia 1740.9898900012151\n",
      "Iteration 22, inertia 1740.973145075329\n",
      "Iteration 23, inertia 1740.971605249508\n",
      "Iteration 24, inertia 1740.9692534189674\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2501.626684252139\n",
      "Iteration 1, inertia 1872.533515746361\n",
      "Iteration 2, inertia 1775.2672898627036\n",
      "Iteration 3, inertia 1737.6749468952762\n",
      "Iteration 4, inertia 1731.0745775385803\n",
      "Iteration 5, inertia 1727.552340763459\n",
      "Iteration 6, inertia 1725.9023789607666\n",
      "Iteration 7, inertia 1724.6970269646608\n",
      "Iteration 8, inertia 1723.8027599473835\n",
      "Iteration 9, inertia 1723.2424402676525\n",
      "Iteration 10, inertia 1722.8328070624352\n",
      "Iteration 11, inertia 1722.615462758081\n",
      "Iteration 12, inertia 1722.4800524190734\n",
      "Iteration 13, inertia 1722.3420321405197\n",
      "Iteration 14, inertia 1722.2092040306156\n",
      "Iteration 15, inertia 1722.1085321554112\n",
      "Iteration 16, inertia 1722.0421922096332\n",
      "Iteration 17, inertia 1721.8961858133773\n",
      "Iteration 18, inertia 1721.710359220121\n",
      "Iteration 19, inertia 1721.5786369118216\n",
      "Iteration 20, inertia 1721.5026511906954\n",
      "Iteration 21, inertia 1721.4264375900157\n",
      "Iteration 22, inertia 1721.3861339812465\n",
      "Iteration 23, inertia 1721.312085026485\n",
      "Iteration 24, inertia 1721.2569497842833\n",
      "Iteration 25, inertia 1721.229086015492\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2360.6559265144506\n",
      "Iteration 1, inertia 1839.353737468065\n",
      "Iteration 2, inertia 1798.091682612655\n",
      "Iteration 3, inertia 1778.1575291562767\n",
      "Iteration 4, inertia 1758.1711686896779\n",
      "Iteration 5, inertia 1744.575000382075\n",
      "Iteration 6, inertia 1738.424559448166\n",
      "Iteration 7, inertia 1735.9686854658414\n",
      "Iteration 8, inertia 1734.9109158223664\n",
      "Iteration 9, inertia 1734.2747661544065\n",
      "Iteration 10, inertia 1733.046349742377\n",
      "Iteration 11, inertia 1729.170495241101\n",
      "Iteration 12, inertia 1718.4489379247761\n",
      "Iteration 13, inertia 1702.8682478716194\n",
      "Iteration 14, inertia 1691.0714221368203\n",
      "Iteration 15, inertia 1688.1684179094618\n",
      "Iteration 16, inertia 1687.3288666058206\n",
      "Iteration 17, inertia 1687.0335954898253\n",
      "Iteration 18, inertia 1686.8665113777747\n",
      "Iteration 19, inertia 1686.733370050098\n",
      "Iteration 20, inertia 1686.6740201214407\n",
      "Iteration 21, inertia 1686.6560387150507\n",
      "Iteration 22, inertia 1686.6541493387576\n",
      "Iteration 23, inertia 1686.650839590864\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, inertia 2414.893207733058\n",
      "Iteration 1, inertia 1796.385890442086\n",
      "Iteration 2, inertia 1738.181506717714\n",
      "Iteration 3, inertia 1715.4225028589199\n",
      "Iteration 4, inertia 1707.3602535841212\n",
      "Iteration 5, inertia 1703.4565065575418\n",
      "Iteration 6, inertia 1700.7164227808626\n",
      "Iteration 7, inertia 1698.3373332392407\n",
      "Iteration 8, inertia 1696.2716606394547\n",
      "Iteration 9, inertia 1694.9456963861355\n",
      "Iteration 10, inertia 1694.1549897427644\n",
      "Iteration 11, inertia 1693.6308510822041\n",
      "Iteration 12, inertia 1693.1797608035927\n",
      "Iteration 13, inertia 1692.8390472575365\n",
      "Iteration 14, inertia 1692.5909891055446\n",
      "Iteration 15, inertia 1692.4013281156729\n",
      "Iteration 16, inertia 1692.2578427421472\n",
      "Iteration 17, inertia 1692.1557228301822\n",
      "Iteration 18, inertia 1692.0572960103832\n",
      "Iteration 19, inertia 1691.9934342253166\n",
      "Iteration 20, inertia 1691.8640509356946\n",
      "Iteration 21, inertia 1691.7218244329638\n",
      "Iteration 22, inertia 1691.6437517701195\n",
      "Iteration 23, inertia 1691.5721308680318\n",
      "Iteration 24, inertia 1691.520337924818\n",
      "Iteration 25, inertia 1691.509210454528\n",
      "Iteration 26, inertia 1691.5063261003447\n",
      "Iteration 27, inertia 1691.5046394989195\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2395.7377006861384\n",
      "Iteration 1, inertia 1876.3030105535445\n",
      "Iteration 2, inertia 1816.891678480455\n",
      "Iteration 3, inertia 1780.0970421837976\n",
      "Iteration 4, inertia 1759.4208034059836\n",
      "Iteration 5, inertia 1744.8053488822209\n",
      "Iteration 6, inertia 1735.1043744008343\n",
      "Iteration 7, inertia 1730.8028193875366\n",
      "Iteration 8, inertia 1728.7883908341428\n",
      "Iteration 9, inertia 1726.8849497447645\n",
      "Iteration 10, inertia 1724.3382205384296\n",
      "Iteration 11, inertia 1720.4169307813074\n",
      "Iteration 12, inertia 1716.1461344248166\n",
      "Iteration 13, inertia 1713.427626443604\n",
      "Iteration 14, inertia 1710.8381034627373\n",
      "Iteration 15, inertia 1708.4589884163477\n",
      "Iteration 16, inertia 1706.6861363764242\n",
      "Iteration 17, inertia 1704.979132485777\n",
      "Iteration 18, inertia 1704.1917515425534\n",
      "Iteration 19, inertia 1703.769685365334\n",
      "Iteration 20, inertia 1703.1891285384131\n",
      "Iteration 21, inertia 1702.6333338682682\n",
      "Iteration 22, inertia 1702.4104576895013\n",
      "Iteration 23, inertia 1702.2778927290603\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2308.8768445531637\n",
      "Iteration 1, inertia 1775.3272418936224\n",
      "Iteration 2, inertia 1726.6242186867082\n",
      "Iteration 3, inertia 1709.1158910477611\n",
      "Iteration 4, inertia 1700.888149402867\n",
      "Iteration 5, inertia 1698.0481691623568\n",
      "Iteration 6, inertia 1696.865525458095\n",
      "Iteration 7, inertia 1696.4934912771541\n",
      "Iteration 8, inertia 1696.316082849828\n",
      "Iteration 9, inertia 1696.2109505919311\n",
      "Iteration 10, inertia 1696.0838637361624\n",
      "Iteration 11, inertia 1695.9587575011303\n",
      "Iteration 12, inertia 1695.8964570152093\n",
      "Iteration 13, inertia 1695.8402727900423\n",
      "Iteration 14, inertia 1695.8028711774425\n",
      "Iteration 15, inertia 1695.7744560814383\n",
      "Iteration 16, inertia 1695.7554313102346\n",
      "Iteration 17, inertia 1695.7297911064459\n",
      "Iteration 18, inertia 1695.7056672541844\n",
      "Iteration 19, inertia 1695.6917433447018\n",
      "Iteration 20, inertia 1695.6829712796762\n",
      "Iteration 21, inertia 1695.6816616145784\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2332.9856894814143\n",
      "Iteration 1, inertia 1788.6402629459328\n",
      "Iteration 2, inertia 1754.0489976884446\n",
      "Iteration 3, inertia 1746.4366588424637\n",
      "Iteration 4, inertia 1738.8973714318124\n",
      "Iteration 5, inertia 1729.9595646998712\n",
      "Iteration 6, inertia 1721.8106865510126\n",
      "Iteration 7, inertia 1712.6412028705543\n",
      "Iteration 8, inertia 1704.409427163222\n",
      "Iteration 9, inertia 1700.4209800766107\n",
      "Iteration 10, inertia 1698.5070731551386\n",
      "Iteration 11, inertia 1697.5474281166041\n",
      "Iteration 12, inertia 1697.0269823602116\n",
      "Iteration 13, inertia 1696.5146513988816\n",
      "Iteration 14, inertia 1695.575943882666\n",
      "Iteration 15, inertia 1695.0818416847014\n",
      "Iteration 16, inertia 1694.7939939922342\n",
      "Iteration 17, inertia 1694.6684322350134\n",
      "Iteration 18, inertia 1694.5690949782365\n",
      "Iteration 19, inertia 1694.4603484231634\n",
      "Iteration 20, inertia 1694.337710875989\n",
      "Iteration 21, inertia 1694.183036904973\n",
      "Iteration 22, inertia 1694.0969771864543\n",
      "Iteration 23, inertia 1694.0159519668264\n",
      "Iteration 24, inertia 1693.9647941181963\n",
      "Iteration 25, inertia 1693.945753426499\n",
      "Iteration 26, inertia 1693.9334297175913\n",
      "Iteration 27, inertia 1693.9191980263965\n",
      "Iteration 28, inertia 1693.9157133541753\n",
      "Converged at iteration 28: strict convergence.\n",
      "Training a K-Means model with 14 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2389.5129591282343\n",
      "Iteration 1, inertia 1777.5250731891356\n",
      "Iteration 2, inertia 1740.4985726580728\n",
      "Iteration 3, inertia 1727.3352486888375\n",
      "Iteration 4, inertia 1720.711723223202\n",
      "Iteration 5, inertia 1715.359998613337\n",
      "Iteration 6, inertia 1711.6783985679242\n",
      "Iteration 7, inertia 1709.4804189465917\n",
      "Iteration 8, inertia 1707.8519233326233\n",
      "Iteration 9, inertia 1706.6335235534145\n",
      "Iteration 10, inertia 1705.8455645220677\n",
      "Iteration 11, inertia 1705.2337177368108\n",
      "Iteration 12, inertia 1704.5855899911153\n",
      "Iteration 13, inertia 1703.9733235211759\n",
      "Iteration 14, inertia 1703.3385223143791\n",
      "Iteration 15, inertia 1702.0422552758869\n",
      "Iteration 16, inertia 1699.1968409818442\n",
      "Iteration 17, inertia 1696.477471316949\n",
      "Iteration 18, inertia 1695.0864632877024\n",
      "Iteration 19, inertia 1694.3765453564336\n",
      "Iteration 20, inertia 1694.1570696885462\n",
      "Iteration 21, inertia 1693.967472285508\n",
      "Iteration 22, inertia 1693.8241114114037\n",
      "Iteration 23, inertia 1693.6570483197638\n",
      "Iteration 24, inertia 1693.5220630372\n",
      "Iteration 25, inertia 1693.3953234530372\n",
      "Iteration 26, inertia 1693.3013660152262\n",
      "Iteration 27, inertia 1693.2566109539432\n",
      "Iteration 28, inertia 1693.1972949055614\n",
      "Iteration 29, inertia 1693.1344340784922\n",
      "Iteration 30, inertia 1693.1213415346488\n",
      "Iteration 31, inertia 1693.1157244803362\n",
      "Iteration 32, inertia 1693.1145464538922\n",
      "Converged at iteration 32: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2458.9199623330405\n",
      "Iteration 1, inertia 1856.4505230191562\n",
      "Iteration 2, inertia 1769.8426380641517\n",
      "Iteration 3, inertia 1732.4859708349647\n",
      "Iteration 4, inertia 1717.34461499496\n",
      "Iteration 5, inertia 1708.4636075181234\n",
      "Iteration 6, inertia 1702.2742459842352\n",
      "Iteration 7, inertia 1697.379861276873\n",
      "Iteration 8, inertia 1693.1621891067923\n",
      "Iteration 9, inertia 1691.07030562851\n",
      "Iteration 10, inertia 1690.414002419121\n",
      "Iteration 11, inertia 1690.223483291447\n",
      "Iteration 12, inertia 1690.0239106697218\n",
      "Iteration 13, inertia 1689.7530998257178\n",
      "Iteration 14, inertia 1689.6333317134493\n",
      "Iteration 15, inertia 1689.5361101339552\n",
      "Iteration 16, inertia 1689.5051644116695\n",
      "Iteration 17, inertia 1689.4683975807834\n",
      "Iteration 18, inertia 1689.4523402993511\n",
      "Iteration 19, inertia 1689.4125504772871\n",
      "Iteration 20, inertia 1689.3783118867993\n",
      "Iteration 21, inertia 1689.2798161369853\n",
      "Iteration 22, inertia 1689.2057710590768\n",
      "Iteration 23, inertia 1689.1653775449465\n",
      "Iteration 24, inertia 1689.1468875978194\n",
      "Iteration 25, inertia 1689.139230550046\n",
      "Iteration 26, inertia 1689.1325361556926\n",
      "Iteration 27, inertia 1689.1290911992942\n",
      "Iteration 28, inertia 1689.1267163866255\n",
      "Iteration 29, inertia 1689.1203412391028\n",
      "Iteration 30, inertia 1689.1174678695522\n",
      "Converged at iteration 30: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2370.305712346393\n",
      "Iteration 1, inertia 1810.8440146792136\n",
      "Iteration 2, inertia 1751.434684685858\n",
      "Iteration 3, inertia 1732.0733345534495\n",
      "Iteration 4, inertia 1718.0305012949052\n",
      "Iteration 5, inertia 1707.668270879139\n",
      "Iteration 6, inertia 1697.7085197246477\n",
      "Iteration 7, inertia 1687.0420896574576\n",
      "Iteration 8, inertia 1677.108298988736\n",
      "Iteration 9, inertia 1669.1545518084308\n",
      "Iteration 10, inertia 1663.5105841939487\n",
      "Iteration 11, inertia 1660.3251551000897\n",
      "Iteration 12, inertia 1658.2364679830653\n",
      "Iteration 13, inertia 1657.2391714919654\n",
      "Iteration 14, inertia 1656.887493429131\n",
      "Iteration 15, inertia 1656.61363178699\n",
      "Iteration 16, inertia 1656.4700971484274\n",
      "Iteration 17, inertia 1656.372115866278\n",
      "Iteration 18, inertia 1656.3350499113715\n",
      "Iteration 19, inertia 1656.3062967665767\n",
      "Iteration 20, inertia 1656.29537577951\n",
      "Iteration 21, inertia 1656.2592095525195\n",
      "Iteration 22, inertia 1656.2024622548433\n",
      "Iteration 23, inertia 1656.1737363715372\n",
      "Iteration 24, inertia 1656.16730686767\n",
      "Iteration 25, inertia 1656.166307497151\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2199.2150798556436\n",
      "Iteration 1, inertia 1768.4510177387642\n",
      "Iteration 2, inertia 1713.9338572456072\n",
      "Iteration 3, inertia 1692.3751998148045\n",
      "Iteration 4, inertia 1678.4109376572128\n",
      "Iteration 5, inertia 1668.1162418077533\n",
      "Iteration 6, inertia 1662.6935550239857\n",
      "Iteration 7, inertia 1659.7696461755281\n",
      "Iteration 8, inertia 1657.7444335597604\n",
      "Iteration 9, inertia 1656.4726477337022\n",
      "Iteration 10, inertia 1655.7958149155465\n",
      "Iteration 11, inertia 1655.4935606630766\n",
      "Iteration 12, inertia 1655.2808677539724\n",
      "Iteration 13, inertia 1655.146642162705\n",
      "Iteration 14, inertia 1654.992918487882\n",
      "Iteration 15, inertia 1654.887504926769\n",
      "Iteration 16, inertia 1654.81938002001\n",
      "Iteration 17, inertia 1654.7431311068322\n",
      "Iteration 18, inertia 1654.6843581398534\n",
      "Iteration 19, inertia 1654.6724400737123\n",
      "Iteration 20, inertia 1654.6402951804964\n",
      "Iteration 21, inertia 1654.6171961958673\n",
      "Iteration 22, inertia 1654.5942124744959\n",
      "Iteration 23, inertia 1654.5633714941298\n",
      "Iteration 24, inertia 1654.5420683397758\n",
      "Iteration 25, inertia 1654.5308636659743\n",
      "Iteration 26, inertia 1654.5081000128978\n",
      "Iteration 27, inertia 1654.4867318805584\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2286.5077025219066\n",
      "Iteration 1, inertia 1729.5878064601502\n",
      "Iteration 2, inertia 1693.3964885375008\n",
      "Iteration 3, inertia 1685.199642454087\n",
      "Iteration 4, inertia 1680.7556092180514\n",
      "Iteration 5, inertia 1678.1816034388771\n",
      "Iteration 6, inertia 1676.7441917266674\n",
      "Iteration 7, inertia 1675.640805928771\n",
      "Iteration 8, inertia 1674.9048876547668\n",
      "Iteration 9, inertia 1674.3660989621653\n",
      "Iteration 10, inertia 1673.9265868423986\n",
      "Iteration 11, inertia 1673.4900223495886\n",
      "Iteration 12, inertia 1673.0799002765716\n",
      "Iteration 13, inertia 1672.8040997546905\n",
      "Iteration 14, inertia 1672.6295897339974\n",
      "Iteration 15, inertia 1672.2862121448152\n",
      "Iteration 16, inertia 1671.929325408681\n",
      "Iteration 17, inertia 1671.7062519185565\n",
      "Iteration 18, inertia 1671.3891483206528\n",
      "Iteration 19, inertia 1671.058006169583\n",
      "Iteration 20, inertia 1670.5974691391339\n",
      "Iteration 21, inertia 1670.0174100281863\n",
      "Iteration 22, inertia 1669.5590285825697\n",
      "Iteration 23, inertia 1668.9040166132638\n",
      "Iteration 24, inertia 1667.6389560836044\n",
      "Iteration 25, inertia 1666.737092412561\n",
      "Iteration 26, inertia 1666.3087674699157\n",
      "Iteration 27, inertia 1666.20805588046\n",
      "Iteration 28, inertia 1666.1412408779536\n",
      "Iteration 29, inertia 1666.073392756049\n",
      "Iteration 30, inertia 1666.0075967505873\n",
      "Iteration 31, inertia 1665.8840564110185\n",
      "Iteration 32, inertia 1665.7403368955825\n",
      "Iteration 33, inertia 1665.631832164803\n",
      "Iteration 34, inertia 1665.588288962228\n",
      "Iteration 35, inertia 1665.5623582325459\n",
      "Iteration 36, inertia 1665.5583836179567\n",
      "Iteration 37, inertia 1665.5575734499628\n",
      "Converged at iteration 37: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2331.540487207645\n",
      "Iteration 1, inertia 1775.170711217218\n",
      "Iteration 2, inertia 1698.12731322679\n",
      "Iteration 3, inertia 1677.1625211568821\n",
      "Iteration 4, inertia 1670.6584979484921\n",
      "Iteration 5, inertia 1667.732535355322\n",
      "Iteration 6, inertia 1666.0662128894207\n",
      "Iteration 7, inertia 1665.1546167332967\n",
      "Iteration 8, inertia 1664.7133945190276\n",
      "Iteration 9, inertia 1664.3520713790185\n",
      "Iteration 10, inertia 1664.1188367186105\n",
      "Iteration 11, inertia 1663.9498046342076\n",
      "Iteration 12, inertia 1663.8236322170228\n",
      "Iteration 13, inertia 1663.6885143256259\n",
      "Iteration 14, inertia 1663.5971332456218\n",
      "Iteration 15, inertia 1663.5289657437015\n",
      "Iteration 16, inertia 1663.508341709411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, inertia 1663.5025704801023\n",
      "Iteration 18, inertia 1663.4965065974047\n",
      "Iteration 19, inertia 1663.4848230797825\n",
      "Iteration 20, inertia 1663.4734689041486\n",
      "Iteration 21, inertia 1663.4601678470522\n",
      "Iteration 22, inertia 1663.4527933669074\n",
      "Iteration 23, inertia 1663.445951131483\n",
      "Iteration 24, inertia 1663.4394043941745\n",
      "Iteration 25, inertia 1663.431003114049\n",
      "Iteration 26, inertia 1663.4213438313852\n",
      "Iteration 27, inertia 1663.411697755482\n",
      "Iteration 28, inertia 1663.4055922946\n",
      "Iteration 29, inertia 1663.3899320879918\n",
      "Iteration 30, inertia 1663.3677678145016\n",
      "Iteration 31, inertia 1663.354716314449\n",
      "Iteration 32, inertia 1663.3434621964445\n",
      "Iteration 33, inertia 1663.3280181636549\n",
      "Iteration 34, inertia 1663.312471403425\n",
      "Iteration 35, inertia 1663.2978610692762\n",
      "Iteration 36, inertia 1663.2930059647908\n",
      "Iteration 37, inertia 1663.283642041267\n",
      "Iteration 38, inertia 1663.2818065197364\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2284.7884491162126\n",
      "Iteration 1, inertia 1807.947661435304\n",
      "Iteration 2, inertia 1763.1884379193664\n",
      "Iteration 3, inertia 1748.1669903936777\n",
      "Iteration 4, inertia 1732.516307464017\n",
      "Iteration 5, inertia 1717.6683195191727\n",
      "Iteration 6, inertia 1704.9283066517878\n",
      "Iteration 7, inertia 1692.3150308565487\n",
      "Iteration 8, inertia 1683.7614900402364\n",
      "Iteration 9, inertia 1679.7335485063809\n",
      "Iteration 10, inertia 1676.9963001931278\n",
      "Iteration 11, inertia 1675.2450507823019\n",
      "Iteration 12, inertia 1674.365841108549\n",
      "Iteration 13, inertia 1674.0177542843664\n",
      "Iteration 14, inertia 1673.758897039269\n",
      "Iteration 15, inertia 1673.558166618414\n",
      "Iteration 16, inertia 1673.434438729871\n",
      "Iteration 17, inertia 1673.3603518092798\n",
      "Iteration 18, inertia 1673.33084147491\n",
      "Iteration 19, inertia 1673.2923521896023\n",
      "Iteration 20, inertia 1673.2342020978774\n",
      "Iteration 21, inertia 1673.1775516943007\n",
      "Iteration 22, inertia 1673.0985724995166\n",
      "Iteration 23, inertia 1673.0554699565214\n",
      "Iteration 24, inertia 1673.0066353922546\n",
      "Iteration 25, inertia 1672.9488785050535\n",
      "Iteration 26, inertia 1672.893953027362\n",
      "Iteration 27, inertia 1672.831210766013\n",
      "Iteration 28, inertia 1672.7582228976935\n",
      "Iteration 29, inertia 1672.674718422659\n",
      "Iteration 30, inertia 1672.598151899115\n",
      "Iteration 31, inertia 1672.5444408040476\n",
      "Iteration 32, inertia 1672.5133824205243\n",
      "Iteration 33, inertia 1672.4542473967872\n",
      "Iteration 34, inertia 1672.3991024202048\n",
      "Iteration 35, inertia 1672.3566789144315\n",
      "Iteration 36, inertia 1672.303859644999\n",
      "Iteration 37, inertia 1672.271585885157\n",
      "Iteration 38, inertia 1672.262535188892\n",
      "Iteration 39, inertia 1672.2529537817964\n",
      "Iteration 40, inertia 1672.2354730202808\n",
      "Iteration 41, inertia 1672.228097501037\n",
      "Iteration 42, inertia 1672.2227062448187\n",
      "Iteration 43, inertia 1672.2140772557696\n",
      "Iteration 44, inertia 1672.201299986853\n",
      "Iteration 45, inertia 1672.1815532102692\n",
      "Iteration 46, inertia 1672.1642228291748\n",
      "Iteration 47, inertia 1672.1291120173562\n",
      "Iteration 48, inertia 1672.0737805208241\n",
      "Iteration 49, inertia 1672.0149486844066\n",
      "Iteration 50, inertia 1671.9426251893074\n",
      "Iteration 51, inertia 1671.871432583827\n",
      "Iteration 52, inertia 1671.760189134554\n",
      "Iteration 53, inertia 1671.5316678217937\n",
      "Iteration 54, inertia 1671.2540172189438\n",
      "Iteration 55, inertia 1670.9096825398854\n",
      "Iteration 56, inertia 1670.5094384876143\n",
      "Iteration 57, inertia 1670.2479290247788\n",
      "Iteration 58, inertia 1669.924828211128\n",
      "Iteration 59, inertia 1669.5946459804034\n",
      "Iteration 60, inertia 1669.3531167181347\n",
      "Iteration 61, inertia 1669.2406131173973\n",
      "Iteration 62, inertia 1669.2129051323705\n",
      "Iteration 63, inertia 1669.189519747882\n",
      "Iteration 64, inertia 1669.186211472101\n",
      "Iteration 65, inertia 1669.170752701748\n",
      "Iteration 66, inertia 1669.1621006111059\n",
      "Iteration 67, inertia 1669.1578682361614\n",
      "Iteration 68, inertia 1669.156323118782\n",
      "Converged at iteration 68: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2357.621680913795\n",
      "Iteration 1, inertia 1774.4975109699733\n",
      "Iteration 2, inertia 1742.4805518627818\n",
      "Iteration 3, inertia 1730.855365863225\n",
      "Iteration 4, inertia 1724.6369364848063\n",
      "Iteration 5, inertia 1720.6733663463272\n",
      "Iteration 6, inertia 1715.8004377652774\n",
      "Iteration 7, inertia 1709.4351445307782\n",
      "Iteration 8, inertia 1704.5726594024695\n",
      "Iteration 9, inertia 1701.7075798327114\n",
      "Iteration 10, inertia 1700.3727697604443\n",
      "Iteration 11, inertia 1699.5410367862212\n",
      "Iteration 12, inertia 1699.0356472592036\n",
      "Iteration 13, inertia 1698.6673123962787\n",
      "Iteration 14, inertia 1698.4279127895986\n",
      "Iteration 15, inertia 1698.272241762886\n",
      "Iteration 16, inertia 1698.1988211274277\n",
      "Iteration 17, inertia 1698.1634774624831\n",
      "Iteration 18, inertia 1698.1138030953393\n",
      "Iteration 19, inertia 1698.0657929008958\n",
      "Iteration 20, inertia 1698.0179544350563\n",
      "Iteration 21, inertia 1697.9074487860246\n",
      "Iteration 22, inertia 1697.7998850752751\n",
      "Iteration 23, inertia 1697.7325160423018\n",
      "Iteration 24, inertia 1697.6841207936413\n",
      "Iteration 25, inertia 1697.648551956874\n",
      "Iteration 26, inertia 1697.6033259403894\n",
      "Iteration 27, inertia 1697.5813857602986\n",
      "Iteration 28, inertia 1697.5715174895893\n",
      "Iteration 29, inertia 1697.5636172242537\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2311.985447989142\n",
      "Iteration 1, inertia 1768.2722336889597\n",
      "Iteration 2, inertia 1696.7621068742985\n",
      "Iteration 3, inertia 1672.5914326634768\n",
      "Iteration 4, inertia 1662.7757939544178\n",
      "Iteration 5, inertia 1658.160025135205\n",
      "Iteration 6, inertia 1655.4058922506522\n",
      "Iteration 7, inertia 1654.088449986324\n",
      "Iteration 8, inertia 1653.4545997409477\n",
      "Iteration 9, inertia 1652.9346242123825\n",
      "Iteration 10, inertia 1652.7130701315775\n",
      "Iteration 11, inertia 1652.5617933990463\n",
      "Iteration 12, inertia 1652.465797161682\n",
      "Iteration 13, inertia 1652.3585150316228\n",
      "Iteration 14, inertia 1652.248389114545\n",
      "Iteration 15, inertia 1652.1850582901106\n",
      "Iteration 16, inertia 1652.1351607062288\n",
      "Iteration 17, inertia 1652.108695678787\n",
      "Iteration 18, inertia 1652.0931554323479\n",
      "Iteration 19, inertia 1652.0803518098091\n",
      "Iteration 20, inertia 1652.066206413183\n",
      "Converged at iteration 20: center shift 1.348837700583117e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2217.679613698157\n",
      "Iteration 1, inertia 1778.1185429170603\n",
      "Iteration 2, inertia 1727.0215543932475\n",
      "Iteration 3, inertia 1707.6489462421257\n",
      "Iteration 4, inertia 1695.206773210283\n",
      "Iteration 5, inertia 1683.4701847608026\n",
      "Iteration 6, inertia 1673.684846461059\n",
      "Iteration 7, inertia 1666.6842051970639\n",
      "Iteration 8, inertia 1663.1342518554684\n",
      "Iteration 9, inertia 1661.0605563639058\n",
      "Iteration 10, inertia 1659.9388155777513\n",
      "Iteration 11, inertia 1659.290114903629\n",
      "Iteration 12, inertia 1658.858222122472\n",
      "Iteration 13, inertia 1658.6390259901013\n",
      "Iteration 14, inertia 1658.434116545067\n",
      "Iteration 15, inertia 1658.1650723746657\n",
      "Iteration 16, inertia 1657.8652553190632\n",
      "Iteration 17, inertia 1657.6111244159029\n",
      "Iteration 18, inertia 1657.4255761730494\n",
      "Iteration 19, inertia 1657.2638828375175\n",
      "Iteration 20, inertia 1657.1095890551676\n",
      "Iteration 21, inertia 1656.95261529145\n",
      "Iteration 22, inertia 1656.8735867471016\n",
      "Iteration 23, inertia 1656.824349203172\n",
      "Iteration 24, inertia 1656.7744114147185\n",
      "Iteration 25, inertia 1656.7443925522114\n",
      "Iteration 26, inertia 1656.6859723789366\n",
      "Iteration 27, inertia 1656.6277371616384\n",
      "Iteration 28, inertia 1656.5633716804416\n",
      "Iteration 29, inertia 1656.4355742600317\n",
      "Iteration 30, inertia 1656.2718681668275\n",
      "Iteration 31, inertia 1656.1184110138008\n",
      "Iteration 32, inertia 1656.0485916503485\n",
      "Iteration 33, inertia 1655.9917860789162\n",
      "Iteration 34, inertia 1655.9144149023052\n",
      "Iteration 35, inertia 1655.845745111862\n",
      "Iteration 36, inertia 1655.801768174028\n",
      "Iteration 37, inertia 1655.7886776018563\n",
      "Iteration 38, inertia 1655.7856530621088\n",
      "Converged at iteration 38: strict convergence.\n",
      "Training a K-Means model with 15 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2329.9016363705623\n",
      "Iteration 1, inertia 1732.7652058955007\n",
      "Iteration 2, inertia 1694.3260984875149\n",
      "Iteration 3, inertia 1683.856899847571\n",
      "Iteration 4, inertia 1679.9898139316244\n",
      "Iteration 5, inertia 1677.4247195160344\n",
      "Iteration 6, inertia 1675.6235596835027\n",
      "Iteration 7, inertia 1674.197558843535\n",
      "Iteration 8, inertia 1672.989102259188\n",
      "Iteration 9, inertia 1672.1967179693863\n",
      "Iteration 10, inertia 1671.6483289931448\n",
      "Iteration 11, inertia 1671.1569957374022\n",
      "Iteration 12, inertia 1670.5918723808977\n",
      "Iteration 13, inertia 1670.0702273893962\n",
      "Iteration 14, inertia 1669.4789951220187\n",
      "Iteration 15, inertia 1668.8976838679405\n",
      "Iteration 16, inertia 1668.396998372021\n",
      "Iteration 17, inertia 1667.9236994967841\n",
      "Iteration 18, inertia 1667.4014622444417\n",
      "Iteration 19, inertia 1667.0563466349015\n",
      "Iteration 20, inertia 1666.8223827704232\n",
      "Iteration 21, inertia 1666.6267087840486\n",
      "Iteration 22, inertia 1666.4701132602747\n",
      "Iteration 23, inertia 1666.2926641221827\n",
      "Iteration 24, inertia 1666.0268987201619\n",
      "Iteration 25, inertia 1665.7984465409395\n",
      "Iteration 26, inertia 1665.446325639668\n",
      "Iteration 27, inertia 1665.288234831403\n",
      "Iteration 28, inertia 1665.1691211644554\n",
      "Iteration 29, inertia 1665.0749637546398\n",
      "Iteration 30, inertia 1664.9726741797053\n",
      "Iteration 31, inertia 1664.878588414596\n",
      "Iteration 32, inertia 1664.8141801148424\n",
      "Iteration 33, inertia 1664.7566482012314\n",
      "Iteration 34, inertia 1664.7253708012786\n",
      "Iteration 35, inertia 1664.697238961811\n",
      "Iteration 36, inertia 1664.6483644571586\n",
      "Iteration 37, inertia 1664.599021372554\n",
      "Iteration 38, inertia 1664.5355394831547\n",
      "Iteration 39, inertia 1664.48048147062\n",
      "Iteration 40, inertia 1664.3777931184509\n",
      "Iteration 41, inertia 1664.3308234190927\n",
      "Iteration 42, inertia 1664.2997763699318\n",
      "Iteration 43, inertia 1664.2657961761563\n",
      "Iteration 44, inertia 1664.220497194012\n",
      "Iteration 45, inertia 1664.1712329834754\n",
      "Iteration 46, inertia 1664.1508827976697\n",
      "Iteration 47, inertia 1664.1217454467826\n",
      "Iteration 48, inertia 1664.1047614214242\n",
      "Iteration 49, inertia 1664.1008635020187\n",
      "Iteration 50, inertia 1664.0988254907659\n",
      "Converged at iteration 50: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2358.014889242104\n",
      "Iteration 1, inertia 1739.506343373476\n",
      "Iteration 2, inertia 1695.388581444192\n",
      "Iteration 3, inertia 1678.223885705613\n",
      "Iteration 4, inertia 1669.8301801205098\n",
      "Iteration 5, inertia 1664.2506946039152\n",
      "Iteration 6, inertia 1659.2570744477937\n",
      "Iteration 7, inertia 1650.9217887703812\n",
      "Iteration 8, inertia 1644.1327964401785\n",
      "Iteration 9, inertia 1640.20651123575\n",
      "Iteration 10, inertia 1637.882423838598\n",
      "Iteration 11, inertia 1636.6538008326945\n",
      "Iteration 12, inertia 1636.0065611661892\n",
      "Iteration 13, inertia 1635.3039478790813\n",
      "Iteration 14, inertia 1634.7436766672176\n",
      "Iteration 15, inertia 1634.3788149909974\n",
      "Iteration 16, inertia 1634.1471494215089\n",
      "Iteration 17, inertia 1633.9993300894614\n",
      "Iteration 18, inertia 1633.9313286387821\n",
      "Iteration 19, inertia 1633.909261395729\n",
      "Iteration 20, inertia 1633.8916173288671\n",
      "Iteration 21, inertia 1633.882932931614\n",
      "Iteration 22, inertia 1633.8756277896537\n",
      "Iteration 23, inertia 1633.874482460169\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2195.5861955545906\n",
      "Iteration 1, inertia 1702.7298868670007\n",
      "Iteration 2, inertia 1674.012717636133\n",
      "Iteration 3, inertia 1665.8350795169106\n",
      "Iteration 4, inertia 1659.6582055527283\n",
      "Iteration 5, inertia 1651.9308199373806\n",
      "Iteration 6, inertia 1646.7761493212818\n",
      "Iteration 7, inertia 1644.2865442732245\n",
      "Iteration 8, inertia 1642.718913320137\n",
      "Iteration 9, inertia 1640.6648597813441\n",
      "Iteration 10, inertia 1637.1428168962145\n",
      "Iteration 11, inertia 1632.249389423241\n",
      "Iteration 12, inertia 1626.805950838607\n",
      "Iteration 13, inertia 1622.9068849165956\n",
      "Iteration 14, inertia 1621.6675469088639\n",
      "Iteration 15, inertia 1620.897695651556\n",
      "Iteration 16, inertia 1620.16889262274\n",
      "Iteration 17, inertia 1619.8418077360661\n",
      "Iteration 18, inertia 1619.7184289656714\n",
      "Iteration 19, inertia 1619.6716591482323\n",
      "Iteration 20, inertia 1619.6589158571358\n",
      "Iteration 21, inertia 1619.6460055824032\n",
      "Iteration 22, inertia 1619.6076044424892\n",
      "Iteration 23, inertia 1619.5771079424176\n",
      "Iteration 24, inertia 1619.4864087640246\n",
      "Iteration 25, inertia 1619.4455830994334\n",
      "Iteration 26, inertia 1619.4158014044638\n",
      "Iteration 27, inertia 1619.3824522199666\n",
      "Iteration 28, inertia 1619.3682483731861\n",
      "Iteration 29, inertia 1619.3612063344833\n",
      "Converged at iteration 29: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2268.8777932769067\n",
      "Iteration 1, inertia 1795.3292253607399\n",
      "Iteration 2, inertia 1715.8043849700755\n",
      "Iteration 3, inertia 1679.2583002138588\n",
      "Iteration 4, inertia 1666.9662291677914\n",
      "Iteration 5, inertia 1660.5696933214992\n",
      "Iteration 6, inertia 1657.248911923707\n",
      "Iteration 7, inertia 1655.191545432189\n",
      "Iteration 8, inertia 1652.6151883624061\n",
      "Iteration 9, inertia 1650.6381604421647\n",
      "Iteration 10, inertia 1650.0981384635359\n",
      "Iteration 11, inertia 1649.768793526878\n",
      "Iteration 12, inertia 1649.4852851870933\n",
      "Iteration 13, inertia 1649.2752395292293\n",
      "Iteration 14, inertia 1649.0781287895393\n",
      "Iteration 15, inertia 1648.7768000045446\n",
      "Iteration 16, inertia 1648.5487874332648\n",
      "Iteration 17, inertia 1648.386061407571\n",
      "Iteration 18, inertia 1648.2634449597438\n",
      "Iteration 19, inertia 1648.0966155094939\n",
      "Iteration 20, inertia 1647.9929774651844\n",
      "Iteration 21, inertia 1647.9623789522448\n",
      "Iteration 22, inertia 1647.9522614816676\n",
      "Iteration 23, inertia 1647.9469135711897\n",
      "Iteration 24, inertia 1647.943484221309\n",
      "Iteration 25, inertia 1647.9356028977977\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2306.3371210233586\n",
      "Iteration 1, inertia 1788.9179946034976\n",
      "Iteration 2, inertia 1706.838597566442\n",
      "Iteration 3, inertia 1670.4276717434325\n",
      "Iteration 4, inertia 1653.8507442371672\n",
      "Iteration 5, inertia 1646.0981356122193\n",
      "Iteration 6, inertia 1642.011369547582\n",
      "Iteration 7, inertia 1640.2081101982812\n",
      "Iteration 8, inertia 1639.0436940938225\n",
      "Iteration 9, inertia 1638.2489963208152\n",
      "Iteration 10, inertia 1637.3651604068084\n",
      "Iteration 11, inertia 1636.4858365260352\n",
      "Iteration 12, inertia 1635.203208263872\n",
      "Iteration 13, inertia 1632.8265429829246\n",
      "Iteration 14, inertia 1631.0596821237043\n",
      "Iteration 15, inertia 1630.2381841393892\n",
      "Iteration 16, inertia 1629.6490730421663\n",
      "Iteration 17, inertia 1629.2667945420526\n",
      "Iteration 18, inertia 1628.9114064017317\n",
      "Iteration 19, inertia 1628.7816895686965\n",
      "Iteration 20, inertia 1628.6498354320736\n",
      "Iteration 21, inertia 1628.5171588500634\n",
      "Iteration 22, inertia 1628.3066635193911\n",
      "Iteration 23, inertia 1628.1763067256\n",
      "Iteration 24, inertia 1628.1442834388295\n",
      "Iteration 25, inertia 1628.0982750245514\n",
      "Iteration 26, inertia 1628.0763632008448\n",
      "Iteration 27, inertia 1628.0706558735133\n",
      "Iteration 28, inertia 1628.0635481709007\n",
      "Iteration 29, inertia 1628.054532571255\n",
      "Iteration 30, inertia 1628.0471892684764\n",
      "Iteration 31, inertia 1628.0368592645018\n",
      "Iteration 32, inertia 1627.9568942326691\n",
      "Iteration 33, inertia 1627.722786390273\n",
      "Iteration 34, inertia 1627.0760752064048\n",
      "Iteration 35, inertia 1626.2870172970868\n",
      "Iteration 36, inertia 1625.789565081641\n",
      "Iteration 37, inertia 1625.3666604202492\n",
      "Iteration 38, inertia 1625.0703617418778\n",
      "Iteration 39, inertia 1624.9267348125613\n",
      "Iteration 40, inertia 1624.8789603950247\n",
      "Iteration 41, inertia 1624.862269332785\n",
      "Iteration 42, inertia 1624.8446743877207\n",
      "Iteration 43, inertia 1624.8066486232267\n",
      "Iteration 44, inertia 1624.75249062736\n",
      "Iteration 45, inertia 1624.682959505943\n",
      "Iteration 46, inertia 1624.615617111402\n",
      "Iteration 47, inertia 1624.5983878514746\n",
      "Iteration 48, inertia 1624.5648627269886\n",
      "Iteration 49, inertia 1624.5359670489618\n",
      "Iteration 50, inertia 1624.525554349269\n",
      "Iteration 51, inertia 1624.5192333776533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, inertia 1624.5118745635718\n",
      "Iteration 53, inertia 1624.5055023504658\n",
      "Iteration 54, inertia 1624.5022171708692\n",
      "Iteration 55, inertia 1624.491334390286\n",
      "Iteration 56, inertia 1624.4888147885426\n",
      "Iteration 57, inertia 1624.4873476269108\n",
      "Converged at iteration 57: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2305.2080324450626\n",
      "Iteration 1, inertia 1717.7052312066492\n",
      "Iteration 2, inertia 1666.7404458896065\n",
      "Iteration 3, inertia 1641.0611941665718\n",
      "Iteration 4, inertia 1629.8651397325043\n",
      "Iteration 5, inertia 1624.0235795965775\n",
      "Iteration 6, inertia 1620.496567853722\n",
      "Iteration 7, inertia 1618.7110288424867\n",
      "Iteration 8, inertia 1617.2939293917862\n",
      "Iteration 9, inertia 1616.3891512740656\n",
      "Iteration 10, inertia 1615.8509859347848\n",
      "Iteration 11, inertia 1615.188050513205\n",
      "Iteration 12, inertia 1614.7652386515658\n",
      "Iteration 13, inertia 1614.3394700643767\n",
      "Iteration 14, inertia 1614.1776575514693\n",
      "Iteration 15, inertia 1614.048600437158\n",
      "Iteration 16, inertia 1613.9943465698036\n",
      "Iteration 17, inertia 1613.9694729362948\n",
      "Iteration 18, inertia 1613.9485056940862\n",
      "Iteration 19, inertia 1613.9439248465208\n",
      "Iteration 20, inertia 1613.94271070969\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2222.9050896324884\n",
      "Iteration 1, inertia 1728.0298689950948\n",
      "Iteration 2, inertia 1681.532030876962\n",
      "Iteration 3, inertia 1656.2813315479868\n",
      "Iteration 4, inertia 1638.6744785253181\n",
      "Iteration 5, inertia 1631.8779025270169\n",
      "Iteration 6, inertia 1629.4239938409803\n",
      "Iteration 7, inertia 1626.6029905255232\n",
      "Iteration 8, inertia 1623.314752065486\n",
      "Iteration 9, inertia 1621.1491966405565\n",
      "Iteration 10, inertia 1620.1759875028297\n",
      "Iteration 11, inertia 1619.686124502619\n",
      "Iteration 12, inertia 1619.4661512907564\n",
      "Iteration 13, inertia 1619.065731483979\n",
      "Iteration 14, inertia 1618.4997098605554\n",
      "Iteration 15, inertia 1618.1482230461256\n",
      "Iteration 16, inertia 1617.960729411103\n",
      "Iteration 17, inertia 1617.6490422252214\n",
      "Iteration 18, inertia 1617.322996258388\n",
      "Iteration 19, inertia 1617.1721787944819\n",
      "Iteration 20, inertia 1617.0564297463038\n",
      "Iteration 21, inertia 1616.8828050593927\n",
      "Iteration 22, inertia 1616.8359554520891\n",
      "Iteration 23, inertia 1616.8095226864543\n",
      "Iteration 24, inertia 1616.7804866390068\n",
      "Iteration 25, inertia 1616.7569278976223\n",
      "Iteration 26, inertia 1616.728395243614\n",
      "Iteration 27, inertia 1616.718086874514\n",
      "Iteration 28, inertia 1616.7127722274909\n",
      "Iteration 29, inertia 1616.7079605798451\n",
      "Converged at iteration 29: center shift 6.640047370411486e-07 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2260.497648490893\n",
      "Iteration 1, inertia 1770.2862394918297\n",
      "Iteration 2, inertia 1704.0188895305857\n",
      "Iteration 3, inertia 1677.3076577511094\n",
      "Iteration 4, inertia 1668.3490314786927\n",
      "Iteration 5, inertia 1665.2834394878948\n",
      "Iteration 6, inertia 1664.0783493833167\n",
      "Iteration 7, inertia 1663.6136213455961\n",
      "Iteration 8, inertia 1663.20782045279\n",
      "Iteration 9, inertia 1662.7776871446251\n",
      "Iteration 10, inertia 1662.4778266923436\n",
      "Iteration 11, inertia 1662.3727589293767\n",
      "Iteration 12, inertia 1662.2501829220814\n",
      "Iteration 13, inertia 1662.1124249674288\n",
      "Iteration 14, inertia 1661.9843298789565\n",
      "Iteration 15, inertia 1661.9288952965544\n",
      "Iteration 16, inertia 1661.862630936437\n",
      "Iteration 17, inertia 1661.8236623929702\n",
      "Iteration 18, inertia 1661.776143585523\n",
      "Iteration 19, inertia 1661.7343266995376\n",
      "Iteration 20, inertia 1661.6928225544054\n",
      "Iteration 21, inertia 1661.6696854869215\n",
      "Iteration 22, inertia 1661.639700310086\n",
      "Iteration 23, inertia 1661.6189801319001\n",
      "Iteration 24, inertia 1661.6158727371003\n",
      "Iteration 25, inertia 1661.604893975488\n",
      "Iteration 26, inertia 1661.5964569007078\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2207.2266543438022\n",
      "Iteration 1, inertia 1745.3528662111653\n",
      "Iteration 2, inertia 1700.6756042970917\n",
      "Iteration 3, inertia 1686.4749291013286\n",
      "Iteration 4, inertia 1679.490344360892\n",
      "Iteration 5, inertia 1675.5468340092123\n",
      "Iteration 6, inertia 1671.564535424129\n",
      "Iteration 7, inertia 1667.308438547648\n",
      "Iteration 8, inertia 1663.8177842582122\n",
      "Iteration 9, inertia 1661.0844486442452\n",
      "Iteration 10, inertia 1658.3624783327691\n",
      "Iteration 11, inertia 1654.547111260047\n",
      "Iteration 12, inertia 1651.511489049378\n",
      "Iteration 13, inertia 1649.267094937354\n",
      "Iteration 14, inertia 1647.5217515026407\n",
      "Iteration 15, inertia 1646.5878615358622\n",
      "Iteration 16, inertia 1646.1488976507105\n",
      "Iteration 17, inertia 1645.8236257481096\n",
      "Iteration 18, inertia 1645.589215657428\n",
      "Iteration 19, inertia 1645.442779309932\n",
      "Iteration 20, inertia 1645.297966869285\n",
      "Iteration 21, inertia 1645.1793682832104\n",
      "Iteration 22, inertia 1645.0644534984701\n",
      "Iteration 23, inertia 1644.9498872974832\n",
      "Iteration 24, inertia 1644.873580169973\n",
      "Iteration 25, inertia 1644.8436420084201\n",
      "Iteration 26, inertia 1644.807852419745\n",
      "Iteration 27, inertia 1644.7807502536757\n",
      "Iteration 28, inertia 1644.7674983776346\n",
      "Iteration 29, inertia 1644.745489077518\n",
      "Iteration 30, inertia 1644.7275763122113\n",
      "Iteration 31, inertia 1644.7023679014587\n",
      "Iteration 32, inertia 1644.680197518316\n",
      "Iteration 33, inertia 1644.6647648291232\n",
      "Iteration 34, inertia 1644.6553584389958\n",
      "Iteration 35, inertia 1644.6484673120315\n",
      "Iteration 36, inertia 1644.6335647417447\n",
      "Iteration 37, inertia 1644.6290668748625\n",
      "Iteration 38, inertia 1644.619832311292\n",
      "Iteration 39, inertia 1644.6045381953406\n",
      "Iteration 40, inertia 1644.5840818132779\n",
      "Iteration 41, inertia 1644.5830271932218\n",
      "Converged at iteration 41: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2287.311254779601\n",
      "Iteration 1, inertia 1713.2427308698743\n",
      "Iteration 2, inertia 1658.691644447864\n",
      "Iteration 3, inertia 1639.8452943106668\n",
      "Iteration 4, inertia 1630.120322692314\n",
      "Iteration 5, inertia 1625.2521739852148\n",
      "Iteration 6, inertia 1623.3747003349044\n",
      "Iteration 7, inertia 1622.4551581349556\n",
      "Iteration 8, inertia 1622.1485966589162\n",
      "Iteration 9, inertia 1621.9731775259531\n",
      "Iteration 10, inertia 1621.789118392464\n",
      "Iteration 11, inertia 1621.6436439755441\n",
      "Iteration 12, inertia 1621.5356728410686\n",
      "Iteration 13, inertia 1621.4793436609073\n",
      "Iteration 14, inertia 1621.4399962348814\n",
      "Iteration 15, inertia 1621.4221932051087\n",
      "Iteration 16, inertia 1621.4161811582703\n",
      "Iteration 17, inertia 1621.4152940620033\n",
      "Converged at iteration 17: strict convergence.\n",
      "Training a K-Means model with 16 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2289.779784704753\n",
      "Iteration 1, inertia 1677.9441521972426\n",
      "Iteration 2, inertia 1630.7241707160333\n",
      "Iteration 3, inertia 1617.6573253935996\n",
      "Iteration 4, inertia 1613.1832144276696\n",
      "Iteration 5, inertia 1610.5631597242145\n",
      "Iteration 6, inertia 1608.8809445497766\n",
      "Iteration 7, inertia 1607.8728683555116\n",
      "Iteration 8, inertia 1606.8869166634352\n",
      "Iteration 9, inertia 1606.0356633073181\n",
      "Iteration 10, inertia 1605.6300184014758\n",
      "Iteration 11, inertia 1605.320053182122\n",
      "Iteration 12, inertia 1605.1279896489955\n",
      "Iteration 13, inertia 1604.9042381994327\n",
      "Iteration 14, inertia 1604.6538858768035\n",
      "Iteration 15, inertia 1604.2412861885664\n",
      "Iteration 16, inertia 1603.7260510808235\n",
      "Iteration 17, inertia 1603.1977360072233\n",
      "Iteration 18, inertia 1602.3413368434437\n",
      "Iteration 19, inertia 1601.5845015207121\n",
      "Iteration 20, inertia 1600.9215367351126\n",
      "Iteration 21, inertia 1600.255059410737\n",
      "Iteration 22, inertia 1599.655280657512\n",
      "Iteration 23, inertia 1599.3306320777383\n",
      "Iteration 24, inertia 1599.0866416135616\n",
      "Iteration 25, inertia 1598.9097272477184\n",
      "Iteration 26, inertia 1598.6648340282588\n",
      "Iteration 27, inertia 1598.3776315979487\n",
      "Iteration 28, inertia 1598.0872123239021\n",
      "Iteration 29, inertia 1597.9378821231546\n",
      "Iteration 30, inertia 1597.8411324379556\n",
      "Iteration 31, inertia 1597.7718884176372\n",
      "Iteration 32, inertia 1597.6953969364254\n",
      "Iteration 33, inertia 1597.564777052616\n",
      "Iteration 34, inertia 1597.5128637765702\n",
      "Iteration 35, inertia 1597.4530975300067\n",
      "Iteration 36, inertia 1597.3781513637905\n",
      "Iteration 37, inertia 1597.3079744960853\n",
      "Iteration 38, inertia 1597.2144297373861\n",
      "Iteration 39, inertia 1597.0878965038146\n",
      "Iteration 40, inertia 1596.9752600885151\n",
      "Iteration 41, inertia 1596.8599205484115\n",
      "Iteration 42, inertia 1596.7541113648726\n",
      "Iteration 43, inertia 1596.69840758469\n",
      "Iteration 44, inertia 1596.6654564842904\n",
      "Iteration 45, inertia 1596.6297633424447\n",
      "Iteration 46, inertia 1596.6137820752247\n",
      "Iteration 47, inertia 1596.598153917732\n",
      "Iteration 48, inertia 1596.590729050251\n",
      "Iteration 49, inertia 1596.5880454933802\n",
      "Iteration 50, inertia 1596.586485676417\n",
      "Iteration 51, inertia 1596.5810469576188\n",
      "Iteration 52, inertia 1596.5731151061862\n",
      "Iteration 53, inertia 1596.5661914142545\n",
      "Iteration 54, inertia 1596.5594766583079\n",
      "Iteration 55, inertia 1596.5539051372812\n",
      "Iteration 56, inertia 1596.5450591904455\n",
      "Iteration 57, inertia 1596.531882134122\n",
      "Iteration 58, inertia 1596.52361036566\n",
      "Iteration 59, inertia 1596.5093116419796\n",
      "Iteration 60, inertia 1596.4948849260893\n",
      "Iteration 61, inertia 1596.4902723382047\n",
      "Iteration 62, inertia 1596.473985346495\n",
      "Iteration 63, inertia 1596.463555493548\n",
      "Iteration 64, inertia 1596.4387598901315\n",
      "Iteration 65, inertia 1596.4178140116742\n",
      "Iteration 66, inertia 1596.400475917652\n",
      "Iteration 67, inertia 1596.3856245857328\n",
      "Iteration 68, inertia 1596.3754784222879\n",
      "Iteration 69, inertia 1596.3735513438012\n",
      "Iteration 70, inertia 1596.3710601837724\n",
      "Converged at iteration 70: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2348.2755593754678\n",
      "Iteration 1, inertia 1688.170505160257\n",
      "Iteration 2, inertia 1630.0045934609848\n",
      "Iteration 3, inertia 1614.4788587730238\n",
      "Iteration 4, inertia 1611.4965529529334\n",
      "Iteration 5, inertia 1609.8226818858534\n",
      "Iteration 6, inertia 1608.582670270491\n",
      "Iteration 7, inertia 1607.8043647081734\n",
      "Iteration 8, inertia 1607.5587028479213\n",
      "Iteration 9, inertia 1607.3973556011451\n",
      "Iteration 10, inertia 1607.257696772458\n",
      "Iteration 11, inertia 1607.2141509270346\n",
      "Iteration 12, inertia 1607.192417640463\n",
      "Iteration 13, inertia 1607.188906525559\n",
      "Iteration 14, inertia 1607.1864301918413\n",
      "Iteration 15, inertia 1607.1849502118505\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2241.7370703822426\n",
      "Iteration 1, inertia 1749.9783216192864\n",
      "Iteration 2, inertia 1686.9953692729246\n",
      "Iteration 3, inertia 1656.2728058526125\n",
      "Iteration 4, inertia 1630.7561171375241\n",
      "Iteration 5, inertia 1617.2483986716327\n",
      "Iteration 6, inertia 1613.938855276982\n",
      "Iteration 7, inertia 1610.6155108164573\n",
      "Iteration 8, inertia 1608.2189531096667\n",
      "Iteration 9, inertia 1606.6624979640073\n",
      "Iteration 10, inertia 1606.140730063207\n",
      "Iteration 11, inertia 1605.6615223276897\n",
      "Iteration 12, inertia 1605.4371437341822\n",
      "Iteration 13, inertia 1605.3153259516919\n",
      "Iteration 14, inertia 1605.2076024935022\n",
      "Iteration 15, inertia 1605.1675032172425\n",
      "Iteration 16, inertia 1605.1456309685786\n",
      "Iteration 17, inertia 1605.12417905313\n",
      "Iteration 18, inertia 1605.0798477352594\n",
      "Iteration 19, inertia 1604.8977322458836\n",
      "Iteration 20, inertia 1604.73331570666\n",
      "Iteration 21, inertia 1604.6351499686366\n",
      "Iteration 22, inertia 1604.564434134993\n",
      "Iteration 23, inertia 1604.5307213168546\n",
      "Iteration 24, inertia 1604.5126133842937\n",
      "Iteration 25, inertia 1604.5018517645478\n",
      "Iteration 26, inertia 1604.4956043435513\n",
      "Iteration 27, inertia 1604.491789374607\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2250.7039916364934\n",
      "Iteration 1, inertia 1727.445526003174\n",
      "Iteration 2, inertia 1692.2850868706653\n",
      "Iteration 3, inertia 1679.46453941855\n",
      "Iteration 4, inertia 1667.9002201897713\n",
      "Iteration 5, inertia 1657.369360294189\n",
      "Iteration 6, inertia 1650.1734375874362\n",
      "Iteration 7, inertia 1647.4864344446733\n",
      "Iteration 8, inertia 1645.9796973325306\n",
      "Iteration 9, inertia 1644.8672304033603\n",
      "Iteration 10, inertia 1643.54533158314\n",
      "Iteration 11, inertia 1641.8591224734557\n",
      "Iteration 12, inertia 1639.5224556732023\n",
      "Iteration 13, inertia 1635.7445584546685\n",
      "Iteration 14, inertia 1630.533215592326\n",
      "Iteration 15, inertia 1625.8582552803823\n",
      "Iteration 16, inertia 1623.8992024235017\n",
      "Iteration 17, inertia 1622.8541483318127\n",
      "Iteration 18, inertia 1622.044784161357\n",
      "Iteration 19, inertia 1619.9949901360392\n",
      "Iteration 20, inertia 1616.1629944072304\n",
      "Iteration 21, inertia 1613.07050357052\n",
      "Iteration 22, inertia 1612.8989890381467\n",
      "Iteration 23, inertia 1612.8543810952506\n",
      "Iteration 24, inertia 1612.814505976032\n",
      "Iteration 25, inertia 1612.790723070038\n",
      "Iteration 26, inertia 1612.764966904372\n",
      "Iteration 27, inertia 1612.730318064314\n",
      "Iteration 28, inertia 1612.7085895804046\n",
      "Iteration 29, inertia 1612.686748586249\n",
      "Iteration 30, inertia 1612.6666836291024\n",
      "Iteration 31, inertia 1612.659137575636\n",
      "Converged at iteration 31: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2235.5428298009174\n",
      "Iteration 1, inertia 1666.9391281251394\n",
      "Iteration 2, inertia 1632.984248810834\n",
      "Iteration 3, inertia 1623.8527052030431\n",
      "Iteration 4, inertia 1618.2279226218811\n",
      "Iteration 5, inertia 1613.3371409847887\n",
      "Iteration 6, inertia 1608.361571381504\n",
      "Iteration 7, inertia 1605.8489820437032\n",
      "Iteration 8, inertia 1604.879261861956\n",
      "Iteration 9, inertia 1604.6339613467385\n",
      "Iteration 10, inertia 1604.426165630634\n",
      "Iteration 11, inertia 1604.295260520689\n",
      "Iteration 12, inertia 1604.2460091636551\n",
      "Iteration 13, inertia 1604.2281371947681\n",
      "Iteration 14, inertia 1604.2258545467573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2127.405066932857\n",
      "Iteration 1, inertia 1659.624535184386\n",
      "Iteration 2, inertia 1614.7192919221006\n",
      "Iteration 3, inertia 1602.0502942710818\n",
      "Iteration 4, inertia 1596.838492148317\n",
      "Iteration 5, inertia 1594.226764494952\n",
      "Iteration 6, inertia 1592.4248132533025\n",
      "Iteration 7, inertia 1591.3969735434582\n",
      "Iteration 8, inertia 1590.5681727100746\n",
      "Iteration 9, inertia 1589.5713988828534\n",
      "Iteration 10, inertia 1588.0683985013036\n",
      "Iteration 11, inertia 1586.4814758178447\n",
      "Iteration 12, inertia 1585.6026912589812\n",
      "Iteration 13, inertia 1585.4241017447127\n",
      "Iteration 14, inertia 1585.343634842911\n",
      "Iteration 15, inertia 1585.3020663020113\n",
      "Iteration 16, inertia 1585.2937295060656\n",
      "Iteration 17, inertia 1585.2930191897049\n",
      "Converged at iteration 17: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2172.2645612123583\n",
      "Iteration 1, inertia 1679.5852947142703\n",
      "Iteration 2, inertia 1635.4240203327267\n",
      "Iteration 3, inertia 1619.5494862809273\n",
      "Iteration 4, inertia 1610.4260270771777\n",
      "Iteration 5, inertia 1605.1555022105458\n",
      "Iteration 6, inertia 1601.604227635356\n",
      "Iteration 7, inertia 1598.6162620702894\n",
      "Iteration 8, inertia 1596.1574022426043\n",
      "Iteration 9, inertia 1593.564898746839\n",
      "Iteration 10, inertia 1589.9420861676715\n",
      "Iteration 11, inertia 1587.504117650971\n",
      "Iteration 12, inertia 1586.4057665653465\n",
      "Iteration 13, inertia 1586.1544025353091\n",
      "Iteration 14, inertia 1586.024426493738\n",
      "Iteration 15, inertia 1585.964660073989\n",
      "Iteration 16, inertia 1585.934114878342\n",
      "Iteration 17, inertia 1585.919513008427\n",
      "Iteration 18, inertia 1585.9161094005465\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2135.6173226441274\n",
      "Iteration 1, inertia 1718.9232165008566\n",
      "Iteration 2, inertia 1672.6758188014794\n",
      "Iteration 3, inertia 1647.3152602089924\n",
      "Iteration 4, inertia 1633.0874545975796\n",
      "Iteration 5, inertia 1625.4545256202834\n",
      "Iteration 6, inertia 1620.144884659269\n",
      "Iteration 7, inertia 1614.5519352957594\n",
      "Iteration 8, inertia 1607.717341369844\n",
      "Iteration 9, inertia 1599.8712579636158\n",
      "Iteration 10, inertia 1595.6873075951128\n",
      "Iteration 11, inertia 1594.534321598786\n",
      "Iteration 12, inertia 1593.2265766900946\n",
      "Iteration 13, inertia 1591.0227633858365\n",
      "Iteration 14, inertia 1588.9503009208922\n",
      "Iteration 15, inertia 1587.8538656918583\n",
      "Iteration 16, inertia 1587.2440674332272\n",
      "Iteration 17, inertia 1587.0252102143882\n",
      "Iteration 18, inertia 1586.8462434217672\n",
      "Iteration 19, inertia 1586.648707896417\n",
      "Iteration 20, inertia 1586.5527030142287\n",
      "Iteration 21, inertia 1586.4829371119865\n",
      "Iteration 22, inertia 1586.4489490896444\n",
      "Iteration 23, inertia 1586.4370146076978\n",
      "Iteration 24, inertia 1586.4227017000012\n",
      "Iteration 25, inertia 1586.4189879169728\n",
      "Iteration 26, inertia 1586.4150167341188\n",
      "Iteration 27, inertia 1586.4131787543133\n",
      "Iteration 28, inertia 1586.4103266743223\n",
      "Converged at iteration 28: center shift 1.2743987688877183e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2214.9438558305924\n",
      "Iteration 1, inertia 1680.2516357896056\n",
      "Iteration 2, inertia 1627.5098190144897\n",
      "Iteration 3, inertia 1611.737517129697\n",
      "Iteration 4, inertia 1606.0410700776904\n",
      "Iteration 5, inertia 1603.7856889668174\n",
      "Iteration 6, inertia 1602.6465742543655\n",
      "Iteration 7, inertia 1601.6898107485883\n",
      "Iteration 8, inertia 1600.659788511256\n",
      "Iteration 9, inertia 1600.0558315586047\n",
      "Iteration 10, inertia 1599.6090440091705\n",
      "Iteration 11, inertia 1599.3375822900227\n",
      "Iteration 12, inertia 1599.2385999186313\n",
      "Iteration 13, inertia 1599.1297286566605\n",
      "Iteration 14, inertia 1598.939553342027\n",
      "Iteration 15, inertia 1598.8238376401803\n",
      "Iteration 16, inertia 1598.7798659139712\n",
      "Iteration 17, inertia 1598.7564359153002\n",
      "Iteration 18, inertia 1598.747734036523\n",
      "Iteration 19, inertia 1598.7452996160082\n",
      "Iteration 20, inertia 1598.7416365580818\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2102.47263637006\n",
      "Iteration 1, inertia 1721.4001977143305\n",
      "Iteration 2, inertia 1666.6325379982989\n",
      "Iteration 3, inertia 1647.7166731747484\n",
      "Iteration 4, inertia 1639.1895638638202\n",
      "Iteration 5, inertia 1635.543986735223\n",
      "Iteration 6, inertia 1633.6486872540831\n",
      "Iteration 7, inertia 1632.5386169240783\n",
      "Iteration 8, inertia 1631.7472107176436\n",
      "Iteration 9, inertia 1631.17616707753\n",
      "Iteration 10, inertia 1630.8894663231576\n",
      "Iteration 11, inertia 1630.7304973447212\n",
      "Iteration 12, inertia 1630.5927723873174\n",
      "Iteration 13, inertia 1630.445592548037\n",
      "Iteration 14, inertia 1630.2399982511226\n",
      "Iteration 15, inertia 1630.0632921993454\n",
      "Iteration 16, inertia 1629.9403788188717\n",
      "Iteration 17, inertia 1629.816734114521\n",
      "Iteration 18, inertia 1629.6262489976525\n",
      "Iteration 19, inertia 1629.4397003011081\n",
      "Iteration 20, inertia 1629.3358540353092\n",
      "Iteration 21, inertia 1629.2569729325194\n",
      "Iteration 22, inertia 1629.2125156254087\n",
      "Iteration 23, inertia 1629.1449382758706\n",
      "Iteration 24, inertia 1629.079759570379\n",
      "Iteration 25, inertia 1629.0501676614915\n",
      "Iteration 26, inertia 1629.0448714492584\n",
      "Iteration 27, inertia 1629.0361171348882\n",
      "Iteration 28, inertia 1629.0246808149882\n",
      "Iteration 29, inertia 1629.021680994699\n",
      "Iteration 30, inertia 1629.0181987204414\n",
      "Iteration 31, inertia 1629.0104897846936\n",
      "Iteration 32, inertia 1628.9926782111884\n",
      "Converged at iteration 32: strict convergence.\n",
      "Training a K-Means model with 17 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2219.976156799861\n",
      "Iteration 1, inertia 1661.8784877822259\n",
      "Iteration 2, inertia 1615.6686868331453\n",
      "Iteration 3, inertia 1597.5900215864185\n",
      "Iteration 4, inertia 1590.7680306584089\n",
      "Iteration 5, inertia 1587.9525180944815\n",
      "Iteration 6, inertia 1585.4676178223947\n",
      "Iteration 7, inertia 1583.2164413724465\n",
      "Iteration 8, inertia 1581.3144540494513\n",
      "Iteration 9, inertia 1579.075703091432\n",
      "Iteration 10, inertia 1575.6134879907281\n",
      "Iteration 11, inertia 1570.9008370700908\n",
      "Iteration 12, inertia 1567.6330612182603\n",
      "Iteration 13, inertia 1566.3996861276719\n",
      "Iteration 14, inertia 1565.7298120869818\n",
      "Iteration 15, inertia 1565.1906305807263\n",
      "Iteration 16, inertia 1564.9277827638537\n",
      "Iteration 17, inertia 1564.7762236357444\n",
      "Iteration 18, inertia 1564.6980811569672\n",
      "Iteration 19, inertia 1564.6044247754062\n",
      "Iteration 20, inertia 1564.5134422208605\n",
      "Iteration 21, inertia 1564.413158288555\n",
      "Iteration 22, inertia 1564.2542288949094\n",
      "Iteration 23, inertia 1564.1007982106432\n",
      "Iteration 24, inertia 1564.0451158084497\n",
      "Iteration 25, inertia 1564.0060166614521\n",
      "Iteration 26, inertia 1563.9661728679284\n",
      "Iteration 27, inertia 1563.8772565251845\n",
      "Iteration 28, inertia 1563.7848201519225\n",
      "Iteration 29, inertia 1563.7577035124923\n",
      "Iteration 30, inertia 1563.7250123050767\n",
      "Iteration 31, inertia 1563.6691914616808\n",
      "Iteration 32, inertia 1563.5998122831763\n",
      "Iteration 33, inertia 1563.5783094778658\n",
      "Iteration 34, inertia 1563.5549602497476\n",
      "Iteration 35, inertia 1563.5301063571685\n",
      "Iteration 36, inertia 1563.4991656634431\n",
      "Iteration 37, inertia 1563.4877500118462\n",
      "Iteration 38, inertia 1563.4784841262642\n",
      "Iteration 39, inertia 1563.4616834941382\n",
      "Iteration 40, inertia 1563.4577013000542\n",
      "Iteration 41, inertia 1563.4561937881342\n",
      "Iteration 42, inertia 1563.4540358095037\n",
      "Iteration 43, inertia 1563.4523901203004\n",
      "Iteration 44, inertia 1563.4499782880218\n",
      "Iteration 45, inertia 1563.4265168784514\n",
      "Iteration 46, inertia 1563.3753512321757\n",
      "Iteration 47, inertia 1563.2952089233484\n",
      "Iteration 48, inertia 1563.204752586761\n",
      "Iteration 49, inertia 1563.0368508327424\n",
      "Iteration 50, inertia 1562.6332023245093\n",
      "Iteration 51, inertia 1561.9360507927408\n",
      "Iteration 52, inertia 1561.1128243386347\n",
      "Iteration 53, inertia 1560.0509705499408\n",
      "Iteration 54, inertia 1558.7210929678752\n",
      "Iteration 55, inertia 1557.9260237586336\n",
      "Iteration 56, inertia 1557.686244899666\n",
      "Iteration 57, inertia 1557.6479004025373\n",
      "Iteration 58, inertia 1557.6372035949164\n",
      "Iteration 59, inertia 1557.620328547351\n",
      "Iteration 60, inertia 1557.6067891786406\n",
      "Iteration 61, inertia 1557.6017221269112\n",
      "Iteration 62, inertia 1557.6005361795533\n",
      "Converged at iteration 62: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2196.3856805398136\n",
      "Iteration 1, inertia 1697.5658506229242\n",
      "Iteration 2, inertia 1651.2669991951573\n",
      "Iteration 3, inertia 1634.3571704331132\n",
      "Iteration 4, inertia 1620.3344574840564\n",
      "Iteration 5, inertia 1604.5659055087572\n",
      "Iteration 6, inertia 1594.111616324094\n",
      "Iteration 7, inertia 1588.5555751517447\n",
      "Iteration 8, inertia 1585.8480266726594\n",
      "Iteration 9, inertia 1582.9806536003205\n",
      "Iteration 10, inertia 1580.2351067740833\n",
      "Iteration 11, inertia 1576.6740160550805\n",
      "Iteration 12, inertia 1574.4393974094473\n",
      "Iteration 13, inertia 1572.5713052853407\n",
      "Iteration 14, inertia 1571.292993942282\n",
      "Iteration 15, inertia 1570.8059975747135\n",
      "Iteration 16, inertia 1570.443425436324\n",
      "Iteration 17, inertia 1570.174539436797\n",
      "Iteration 18, inertia 1569.9771784769532\n",
      "Iteration 19, inertia 1569.8637471144696\n",
      "Iteration 20, inertia 1569.8135188060014\n",
      "Iteration 21, inertia 1569.7835425489739\n",
      "Iteration 22, inertia 1569.7652361422363\n",
      "Iteration 23, inertia 1569.7331971136487\n",
      "Iteration 24, inertia 1569.7028992718826\n",
      "Iteration 25, inertia 1569.6659164117425\n",
      "Iteration 26, inertia 1569.6241590940663\n",
      "Iteration 27, inertia 1569.545628228594\n",
      "Iteration 28, inertia 1569.4826449401376\n",
      "Iteration 29, inertia 1569.4608548032838\n",
      "Iteration 30, inertia 1569.4413841423848\n",
      "Iteration 31, inertia 1569.422931679298\n",
      "Iteration 32, inertia 1569.4074353798505\n",
      "Iteration 33, inertia 1569.401284914112\n",
      "Iteration 34, inertia 1569.3829092261292\n",
      "Converged at iteration 34: center shift 1.2246640007028912e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2166.6068096516456\n",
      "Iteration 1, inertia 1656.3786515506388\n",
      "Iteration 2, inertia 1615.4967481480217\n",
      "Iteration 3, inertia 1590.4645099825252\n",
      "Iteration 4, inertia 1574.762278629598\n",
      "Iteration 5, inertia 1570.4389812308839\n",
      "Iteration 6, inertia 1568.4820028421118\n",
      "Iteration 7, inertia 1567.825854647158\n",
      "Iteration 8, inertia 1567.4580008410578\n",
      "Iteration 9, inertia 1567.1871329756314\n",
      "Iteration 10, inertia 1567.058256396914\n",
      "Iteration 11, inertia 1566.99300143643\n",
      "Iteration 12, inertia 1566.9551790499947\n",
      "Iteration 13, inertia 1566.9285636826282\n",
      "Iteration 14, inertia 1566.9131139302892\n",
      "Iteration 15, inertia 1566.9073804650968\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2301.01217727285\n",
      "Iteration 1, inertia 1724.682905722965\n",
      "Iteration 2, inertia 1632.7460076091988\n",
      "Iteration 3, inertia 1607.262047561072\n",
      "Iteration 4, inertia 1598.834459458876\n",
      "Iteration 5, inertia 1592.313932819865\n",
      "Iteration 6, inertia 1588.7332832900709\n",
      "Iteration 7, inertia 1586.8241769653318\n",
      "Iteration 8, inertia 1585.9364730281243\n",
      "Iteration 9, inertia 1585.3862760622353\n",
      "Iteration 10, inertia 1585.0693005763887\n",
      "Iteration 11, inertia 1584.9613237944504\n",
      "Iteration 12, inertia 1584.9264698679765\n",
      "Iteration 13, inertia 1584.909045478048\n",
      "Iteration 14, inertia 1584.8982150110905\n",
      "Iteration 15, inertia 1584.8951919689687\n",
      "Iteration 16, inertia 1584.8925529075311\n",
      "Iteration 17, inertia 1584.888508351644\n",
      "Iteration 18, inertia 1584.8696211543195\n",
      "Iteration 19, inertia 1584.8621495050297\n",
      "Iteration 20, inertia 1584.854941824157\n",
      "Iteration 21, inertia 1584.8471404269173\n",
      "Iteration 22, inertia 1584.8434711668758\n",
      "Iteration 23, inertia 1584.8414331873335\n",
      "Iteration 24, inertia 1584.8406354790218\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2226.8180798989542\n",
      "Iteration 1, inertia 1651.2417435164784\n",
      "Iteration 2, inertia 1599.9472905947573\n",
      "Iteration 3, inertia 1583.6027764073194\n",
      "Iteration 4, inertia 1577.7973139125133\n",
      "Iteration 5, inertia 1574.605110532055\n",
      "Iteration 6, inertia 1572.8039535040339\n",
      "Iteration 7, inertia 1571.668317376031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, inertia 1571.0041886802221\n",
      "Iteration 9, inertia 1570.595071894242\n",
      "Iteration 10, inertia 1570.3314662548146\n",
      "Iteration 11, inertia 1570.2011090062194\n",
      "Iteration 12, inertia 1570.0392312813015\n",
      "Iteration 13, inertia 1569.91779282363\n",
      "Iteration 14, inertia 1569.6567773712975\n",
      "Iteration 15, inertia 1569.1033364598975\n",
      "Iteration 16, inertia 1568.0573094495664\n",
      "Iteration 17, inertia 1566.2282200179834\n",
      "Iteration 18, inertia 1561.6189481590386\n",
      "Iteration 19, inertia 1558.6624414528933\n",
      "Iteration 20, inertia 1557.5588428079732\n",
      "Iteration 21, inertia 1557.2438823466603\n",
      "Iteration 22, inertia 1557.122341208989\n",
      "Iteration 23, inertia 1557.0839616662345\n",
      "Iteration 24, inertia 1557.06147591774\n",
      "Iteration 25, inertia 1557.0508575276249\n",
      "Iteration 26, inertia 1557.0455829521948\n",
      "Iteration 27, inertia 1557.0403233767483\n",
      "Iteration 28, inertia 1557.0345233932223\n",
      "Iteration 29, inertia 1557.0317427522073\n",
      "Iteration 30, inertia 1557.0179269873775\n",
      "Iteration 31, inertia 1556.9946170208532\n",
      "Iteration 32, inertia 1556.9832970650389\n",
      "Iteration 33, inertia 1556.9777298009706\n",
      "Iteration 34, inertia 1556.9744426728944\n",
      "Iteration 35, inertia 1556.9715451833288\n",
      "Iteration 36, inertia 1556.9688791675183\n",
      "Iteration 37, inertia 1556.967276488814\n",
      "Iteration 38, inertia 1556.9640783858522\n",
      "Iteration 39, inertia 1556.961811130339\n",
      "Iteration 40, inertia 1556.9598509774553\n",
      "Converged at iteration 40: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2139.7025459236875\n",
      "Iteration 1, inertia 1631.8572391299638\n",
      "Iteration 2, inertia 1597.5431915835738\n",
      "Iteration 3, inertia 1581.9646563188733\n",
      "Iteration 4, inertia 1574.043389617393\n",
      "Iteration 5, inertia 1570.9835876178208\n",
      "Iteration 6, inertia 1569.9880624452705\n",
      "Iteration 7, inertia 1569.4022759510717\n",
      "Iteration 8, inertia 1568.7581385568574\n",
      "Iteration 9, inertia 1567.7473970364772\n",
      "Iteration 10, inertia 1566.409732249318\n",
      "Iteration 11, inertia 1565.5859809665728\n",
      "Iteration 12, inertia 1565.0985250446365\n",
      "Iteration 13, inertia 1564.4997736875841\n",
      "Iteration 14, inertia 1563.976340583226\n",
      "Iteration 15, inertia 1563.7303035722603\n",
      "Iteration 16, inertia 1563.5619491227599\n",
      "Iteration 17, inertia 1563.2187015538768\n",
      "Iteration 18, inertia 1563.0237067774917\n",
      "Iteration 19, inertia 1562.8142913436213\n",
      "Iteration 20, inertia 1562.5605949885555\n",
      "Iteration 21, inertia 1562.399506179337\n",
      "Iteration 22, inertia 1562.2243610863732\n",
      "Iteration 23, inertia 1562.0782777338563\n",
      "Iteration 24, inertia 1561.9919270737923\n",
      "Iteration 25, inertia 1561.9665469150507\n",
      "Iteration 26, inertia 1561.9621336314906\n",
      "Iteration 27, inertia 1561.9576008638182\n",
      "Iteration 28, inertia 1561.9465403034524\n",
      "Iteration 29, inertia 1561.921626682832\n",
      "Iteration 30, inertia 1561.8927290906538\n",
      "Iteration 31, inertia 1561.8650777873877\n",
      "Iteration 32, inertia 1561.8447227879662\n",
      "Iteration 33, inertia 1561.8424778836359\n",
      "Converged at iteration 33: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2148.196676521916\n",
      "Iteration 1, inertia 1674.6332843963512\n",
      "Iteration 2, inertia 1621.5469048813136\n",
      "Iteration 3, inertia 1599.7853197772247\n",
      "Iteration 4, inertia 1588.8999289255532\n",
      "Iteration 5, inertia 1583.5126590105724\n",
      "Iteration 6, inertia 1580.59672890359\n",
      "Iteration 7, inertia 1579.768369318117\n",
      "Iteration 8, inertia 1579.5346060770541\n",
      "Iteration 9, inertia 1579.4099585687138\n",
      "Iteration 10, inertia 1579.3210318039075\n",
      "Iteration 11, inertia 1579.1535660940408\n",
      "Iteration 12, inertia 1578.9408670020734\n",
      "Iteration 13, inertia 1578.7800338359968\n",
      "Iteration 14, inertia 1578.7285173793525\n",
      "Iteration 15, inertia 1578.6449658974932\n",
      "Iteration 16, inertia 1578.4696136555815\n",
      "Iteration 17, inertia 1578.4063979793345\n",
      "Iteration 18, inertia 1578.3431063290818\n",
      "Iteration 19, inertia 1578.2906692503193\n",
      "Iteration 20, inertia 1578.222520878548\n",
      "Iteration 21, inertia 1578.15673817641\n",
      "Iteration 22, inertia 1578.0888752793717\n",
      "Iteration 23, inertia 1578.0264774105074\n",
      "Iteration 24, inertia 1577.9134358822016\n",
      "Iteration 25, inertia 1577.8489685263785\n",
      "Iteration 26, inertia 1577.7609543795188\n",
      "Iteration 27, inertia 1577.662340679883\n",
      "Iteration 28, inertia 1577.5628461066335\n",
      "Iteration 29, inertia 1577.483882462753\n",
      "Iteration 30, inertia 1577.3737268553805\n",
      "Iteration 31, inertia 1577.2160021822685\n",
      "Iteration 32, inertia 1576.9527878390395\n",
      "Iteration 33, inertia 1576.6924781723467\n",
      "Iteration 34, inertia 1576.5015757077615\n",
      "Iteration 35, inertia 1576.18531202501\n",
      "Iteration 36, inertia 1575.72949336912\n",
      "Iteration 37, inertia 1575.0075766593739\n",
      "Iteration 38, inertia 1573.7644440365227\n",
      "Iteration 39, inertia 1572.1222542118423\n",
      "Iteration 40, inertia 1570.750969739609\n",
      "Iteration 41, inertia 1569.9021686049407\n",
      "Iteration 42, inertia 1569.3909328373743\n",
      "Iteration 43, inertia 1569.185605537503\n",
      "Iteration 44, inertia 1568.9213330118864\n",
      "Iteration 45, inertia 1568.6352551634814\n",
      "Iteration 46, inertia 1568.4070976858811\n",
      "Iteration 47, inertia 1568.124345522735\n",
      "Iteration 48, inertia 1567.8629061537245\n",
      "Iteration 49, inertia 1567.6295100689945\n",
      "Iteration 50, inertia 1567.4336362176373\n",
      "Iteration 51, inertia 1567.2578793294842\n",
      "Iteration 52, inertia 1567.1192715997079\n",
      "Iteration 53, inertia 1566.9698708106275\n",
      "Iteration 54, inertia 1566.8882071067296\n",
      "Iteration 55, inertia 1566.8500804190944\n",
      "Iteration 56, inertia 1566.800112634372\n",
      "Iteration 57, inertia 1566.7539123950787\n",
      "Iteration 58, inertia 1566.709803746979\n",
      "Iteration 59, inertia 1566.6656809277977\n",
      "Iteration 60, inertia 1566.6245429155322\n",
      "Iteration 61, inertia 1566.6014736276575\n",
      "Iteration 62, inertia 1566.5515950031995\n",
      "Iteration 63, inertia 1566.4278466590902\n",
      "Iteration 64, inertia 1566.231285570917\n",
      "Iteration 65, inertia 1566.09328859165\n",
      "Iteration 66, inertia 1565.9541646760242\n",
      "Iteration 67, inertia 1565.7735850460865\n",
      "Iteration 68, inertia 1565.643725202177\n",
      "Iteration 69, inertia 1565.4254207738359\n",
      "Iteration 70, inertia 1565.1771784316404\n",
      "Iteration 71, inertia 1564.9731888582664\n",
      "Iteration 72, inertia 1564.865672306321\n",
      "Iteration 73, inertia 1564.7608476049163\n",
      "Iteration 74, inertia 1564.654135011284\n",
      "Iteration 75, inertia 1564.6125101815232\n",
      "Iteration 76, inertia 1564.599809976051\n",
      "Iteration 77, inertia 1564.596531245711\n",
      "Iteration 78, inertia 1564.5933408595868\n",
      "Iteration 79, inertia 1564.585779340422\n",
      "Iteration 80, inertia 1564.5828567748065\n",
      "Iteration 81, inertia 1564.5816646737217\n",
      "Converged at iteration 81: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2227.306515526739\n",
      "Iteration 1, inertia 1719.964274370583\n",
      "Iteration 2, inertia 1639.6015151783915\n",
      "Iteration 3, inertia 1605.757251079999\n",
      "Iteration 4, inertia 1592.9684577626056\n",
      "Iteration 5, inertia 1586.9909086931616\n",
      "Iteration 6, inertia 1583.1042193859394\n",
      "Iteration 7, inertia 1580.626333616826\n",
      "Iteration 8, inertia 1578.567955772528\n",
      "Iteration 9, inertia 1576.0521000164151\n",
      "Iteration 10, inertia 1572.490053781328\n",
      "Iteration 11, inertia 1567.4964977674585\n",
      "Iteration 12, inertia 1564.1532444363875\n",
      "Iteration 13, inertia 1563.432096416396\n",
      "Iteration 14, inertia 1563.0048514346406\n",
      "Iteration 15, inertia 1562.505113217999\n",
      "Iteration 16, inertia 1562.0997287502341\n",
      "Iteration 17, inertia 1561.8055429789629\n",
      "Iteration 18, inertia 1561.6082899695984\n",
      "Iteration 19, inertia 1561.3786746706555\n",
      "Iteration 20, inertia 1561.1755757036408\n",
      "Iteration 21, inertia 1561.025048109475\n",
      "Iteration 22, inertia 1560.9316020502179\n",
      "Iteration 23, inertia 1560.8297342276664\n",
      "Iteration 24, inertia 1560.6542077780978\n",
      "Iteration 25, inertia 1560.4425696804728\n",
      "Iteration 26, inertia 1560.2437259817398\n",
      "Iteration 27, inertia 1560.0638957056158\n",
      "Iteration 28, inertia 1559.9064806743352\n",
      "Iteration 29, inertia 1559.8027039342114\n",
      "Iteration 30, inertia 1559.7570659277706\n",
      "Iteration 31, inertia 1559.741301002813\n",
      "Iteration 32, inertia 1559.7240201446139\n",
      "Iteration 33, inertia 1559.7119645409978\n",
      "Iteration 34, inertia 1559.7056646675398\n",
      "Iteration 35, inertia 1559.6931279736918\n",
      "Iteration 36, inertia 1559.660220115438\n",
      "Iteration 37, inertia 1559.622784168453\n",
      "Iteration 38, inertia 1559.5380370644054\n",
      "Iteration 39, inertia 1559.3612876675556\n",
      "Iteration 40, inertia 1559.1590682660178\n",
      "Iteration 41, inertia 1558.8063889184239\n",
      "Iteration 42, inertia 1558.0632538844925\n",
      "Iteration 43, inertia 1557.2518980948494\n",
      "Iteration 44, inertia 1555.9986320210307\n",
      "Iteration 45, inertia 1554.4725168414097\n",
      "Iteration 46, inertia 1553.5020637315047\n",
      "Iteration 47, inertia 1553.327847751483\n",
      "Iteration 48, inertia 1553.2674752440948\n",
      "Iteration 49, inertia 1553.2452095739297\n",
      "Iteration 50, inertia 1553.2163566066479\n",
      "Iteration 51, inertia 1553.2144417375803\n",
      "Iteration 52, inertia 1553.2136985893337\n",
      "Converged at iteration 52: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2215.4636323346576\n",
      "Iteration 1, inertia 1695.390928349589\n",
      "Iteration 2, inertia 1656.1058847734914\n",
      "Iteration 3, inertia 1643.0567640474462\n",
      "Iteration 4, inertia 1635.347082240605\n",
      "Iteration 5, inertia 1628.4133353821314\n",
      "Iteration 6, inertia 1621.8484672511763\n",
      "Iteration 7, inertia 1614.700603620711\n",
      "Iteration 8, inertia 1610.0970436982923\n",
      "Iteration 9, inertia 1607.2447427411448\n",
      "Iteration 10, inertia 1604.7446827047452\n",
      "Iteration 11, inertia 1602.3565664130338\n",
      "Iteration 12, inertia 1600.889112607043\n",
      "Iteration 13, inertia 1599.231153928629\n",
      "Iteration 14, inertia 1596.2000491092606\n",
      "Iteration 15, inertia 1594.3847913434\n",
      "Iteration 16, inertia 1593.7572252401242\n",
      "Iteration 17, inertia 1593.3747871127593\n",
      "Iteration 18, inertia 1593.0890435863707\n",
      "Iteration 19, inertia 1592.8821980247897\n",
      "Iteration 20, inertia 1592.7050703656198\n",
      "Iteration 21, inertia 1592.4591857335893\n",
      "Iteration 22, inertia 1592.1731079072554\n",
      "Iteration 23, inertia 1591.9192526770498\n",
      "Iteration 24, inertia 1591.689467527607\n",
      "Iteration 25, inertia 1591.5984645725405\n",
      "Iteration 26, inertia 1591.5393280495105\n",
      "Iteration 27, inertia 1591.4925589245101\n",
      "Iteration 28, inertia 1591.4562040388687\n",
      "Iteration 29, inertia 1591.4177049754549\n",
      "Iteration 30, inertia 1591.4070750679787\n",
      "Iteration 31, inertia 1591.4021032139938\n",
      "Iteration 32, inertia 1591.396018982036\n",
      "Iteration 33, inertia 1591.3869705432708\n",
      "Iteration 34, inertia 1591.3780259082719\n",
      "Iteration 35, inertia 1591.3540151603015\n",
      "Iteration 36, inertia 1591.3273703322282\n",
      "Iteration 37, inertia 1591.310346956297\n",
      "Iteration 38, inertia 1591.2768920562585\n",
      "Iteration 39, inertia 1591.2496842573246\n",
      "Iteration 40, inertia 1591.241090062656\n",
      "Iteration 41, inertia 1591.2395081550956\n",
      "Iteration 42, inertia 1591.2321007482435\n",
      "Iteration 43, inertia 1591.2040151686797\n",
      "Iteration 44, inertia 1591.170683778118\n",
      "Iteration 45, inertia 1591.1245761744972\n",
      "Iteration 46, inertia 1591.0975491787897\n",
      "Iteration 47, inertia 1591.073627737754\n",
      "Iteration 48, inertia 1591.0479868313112\n",
      "Iteration 49, inertia 1591.017010100184\n",
      "Iteration 50, inertia 1591.0112561570863\n",
      "Converged at iteration 50: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2245.1420175990934\n",
      "Iteration 1, inertia 1651.7394609135324\n",
      "Iteration 2, inertia 1599.3406859777776\n",
      "Iteration 3, inertia 1588.385612935876\n",
      "Iteration 4, inertia 1583.885951101582\n",
      "Iteration 5, inertia 1581.4002349545756\n",
      "Iteration 6, inertia 1580.0610907814223\n",
      "Iteration 7, inertia 1579.1459814860239\n",
      "Iteration 8, inertia 1578.4461610654196\n",
      "Iteration 9, inertia 1577.874198630167\n",
      "Iteration 10, inertia 1577.4911918457335\n",
      "Iteration 11, inertia 1577.1779697832235\n",
      "Iteration 12, inertia 1576.8046573945935\n",
      "Iteration 13, inertia 1576.2925548890573\n",
      "Iteration 14, inertia 1575.0937343643607\n",
      "Iteration 15, inertia 1573.0496395594948\n",
      "Iteration 16, inertia 1569.798460848828\n",
      "Iteration 17, inertia 1568.5244531493074\n",
      "Iteration 18, inertia 1568.0301132132672\n",
      "Iteration 19, inertia 1567.7299733251689\n",
      "Iteration 20, inertia 1567.5871858103687\n",
      "Iteration 21, inertia 1567.4863322205372\n",
      "Iteration 22, inertia 1567.3050398739113\n",
      "Iteration 23, inertia 1567.1835588385488\n",
      "Iteration 24, inertia 1567.116812629507\n",
      "Iteration 25, inertia 1567.077443901399\n",
      "Iteration 26, inertia 1567.0322042586163\n",
      "Iteration 27, inertia 1566.9979422450576\n",
      "Iteration 28, inertia 1566.9612506874685\n",
      "Iteration 29, inertia 1566.910358105346\n",
      "Iteration 30, inertia 1566.8670586846918\n",
      "Iteration 31, inertia 1566.8528958844395\n",
      "Iteration 32, inertia 1566.8451724353304\n",
      "Iteration 33, inertia 1566.8438618195835\n",
      "Iteration 34, inertia 1566.8424796229685\n",
      "Converged at iteration 34: strict convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 18 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2192.172986397887\n",
      "Iteration 1, inertia 1651.6358805596642\n",
      "Iteration 2, inertia 1604.5967347447504\n",
      "Iteration 3, inertia 1583.250024010726\n",
      "Iteration 4, inertia 1571.0917839444483\n",
      "Iteration 5, inertia 1565.1722259801722\n",
      "Iteration 6, inertia 1560.5578622756875\n",
      "Iteration 7, inertia 1558.4660745438923\n",
      "Iteration 8, inertia 1556.9748765396732\n",
      "Iteration 9, inertia 1556.062147745341\n",
      "Iteration 10, inertia 1555.1828982647405\n",
      "Iteration 11, inertia 1554.0769304415883\n",
      "Iteration 12, inertia 1552.4950485348204\n",
      "Iteration 13, inertia 1550.1319366708803\n",
      "Iteration 14, inertia 1548.91914217044\n",
      "Iteration 15, inertia 1548.2437722030459\n",
      "Iteration 16, inertia 1547.749404429635\n",
      "Iteration 17, inertia 1547.454491186405\n",
      "Iteration 18, inertia 1547.1357472876316\n",
      "Iteration 19, inertia 1546.832320263203\n",
      "Iteration 20, inertia 1546.5809911108631\n",
      "Iteration 21, inertia 1546.2397124723593\n",
      "Iteration 22, inertia 1546.0270310250703\n",
      "Iteration 23, inertia 1545.707089547654\n",
      "Iteration 24, inertia 1545.2520728118454\n",
      "Iteration 25, inertia 1544.0995689219371\n",
      "Iteration 26, inertia 1540.3157286291537\n",
      "Iteration 27, inertia 1537.200668508089\n",
      "Iteration 28, inertia 1534.9769191270907\n",
      "Iteration 29, inertia 1531.352444654859\n",
      "Iteration 30, inertia 1529.92822659279\n",
      "Iteration 31, inertia 1528.885651409338\n",
      "Iteration 32, inertia 1528.071080806008\n",
      "Iteration 33, inertia 1527.491644320916\n",
      "Iteration 34, inertia 1527.2260846626782\n",
      "Iteration 35, inertia 1527.0136349765453\n",
      "Iteration 36, inertia 1526.9197206376668\n",
      "Iteration 37, inertia 1526.7755154177632\n",
      "Iteration 38, inertia 1526.7020897192638\n",
      "Iteration 39, inertia 1526.5879123930488\n",
      "Iteration 40, inertia 1526.445400578155\n",
      "Iteration 41, inertia 1526.3863962299574\n",
      "Iteration 42, inertia 1526.3299011576823\n",
      "Iteration 43, inertia 1526.2760500876063\n",
      "Iteration 44, inertia 1526.2179293920947\n",
      "Iteration 45, inertia 1526.1504112627706\n",
      "Iteration 46, inertia 1526.1165959425523\n",
      "Iteration 47, inertia 1526.0828029450454\n",
      "Iteration 48, inertia 1526.0524073159902\n",
      "Iteration 49, inertia 1526.041033240743\n",
      "Iteration 50, inertia 1526.0369596457067\n",
      "Iteration 51, inertia 1526.0278577517163\n",
      "Iteration 52, inertia 1526.0239167466343\n",
      "Iteration 53, inertia 1526.0105981932447\n",
      "Iteration 54, inertia 1525.9944650445514\n",
      "Iteration 55, inertia 1525.9858280246217\n",
      "Iteration 56, inertia 1525.9771090049476\n",
      "Iteration 57, inertia 1525.9599011973976\n",
      "Iteration 58, inertia 1525.9401779078198\n",
      "Iteration 59, inertia 1525.9258641291015\n",
      "Iteration 60, inertia 1525.9227256490556\n",
      "Iteration 61, inertia 1525.914818824933\n",
      "Converged at iteration 61: center shift 1.3208546963879243e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2149.968834251949\n",
      "Iteration 1, inertia 1645.5454169880609\n",
      "Iteration 2, inertia 1588.5915124068129\n",
      "Iteration 3, inertia 1568.0875414680481\n",
      "Iteration 4, inertia 1557.3736416436016\n",
      "Iteration 5, inertia 1550.7650908122246\n",
      "Iteration 6, inertia 1548.0811466410266\n",
      "Iteration 7, inertia 1547.0088460331629\n",
      "Iteration 8, inertia 1546.369297881628\n",
      "Iteration 9, inertia 1545.8517044023786\n",
      "Iteration 10, inertia 1545.5326558354768\n",
      "Iteration 11, inertia 1545.3584640142844\n",
      "Iteration 12, inertia 1545.1313376469561\n",
      "Iteration 13, inertia 1544.90071188105\n",
      "Iteration 14, inertia 1544.8558810953684\n",
      "Iteration 15, inertia 1544.82983150583\n",
      "Iteration 16, inertia 1544.7933013727127\n",
      "Iteration 17, inertia 1544.7625275569333\n",
      "Iteration 18, inertia 1544.7574882008444\n",
      "Iteration 19, inertia 1544.7541605302895\n",
      "Iteration 20, inertia 1544.7511010183684\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2131.759443093431\n",
      "Iteration 1, inertia 1672.9121060602577\n",
      "Iteration 2, inertia 1603.6655418527282\n",
      "Iteration 3, inertia 1574.2270585828971\n",
      "Iteration 4, inertia 1560.9000150387656\n",
      "Iteration 5, inertia 1551.1621544894213\n",
      "Iteration 6, inertia 1541.9007783869392\n",
      "Iteration 7, inertia 1534.5933444961727\n",
      "Iteration 8, inertia 1531.0154814193934\n",
      "Iteration 9, inertia 1528.6351202895528\n",
      "Iteration 10, inertia 1527.661296771658\n",
      "Iteration 11, inertia 1527.1051259433993\n",
      "Iteration 12, inertia 1526.7471983207363\n",
      "Iteration 13, inertia 1526.4369934282379\n",
      "Iteration 14, inertia 1526.0942156294132\n",
      "Iteration 15, inertia 1525.7117839990892\n",
      "Iteration 16, inertia 1525.568222783269\n",
      "Iteration 17, inertia 1525.3840417629872\n",
      "Iteration 18, inertia 1525.112000278568\n",
      "Iteration 19, inertia 1524.9479092363094\n",
      "Iteration 20, inertia 1524.8527219280913\n",
      "Iteration 21, inertia 1524.7737158628431\n",
      "Iteration 22, inertia 1524.5914662019281\n",
      "Iteration 23, inertia 1524.259809015812\n",
      "Iteration 24, inertia 1524.0840471201577\n",
      "Iteration 25, inertia 1523.9863605258774\n",
      "Iteration 26, inertia 1523.9028858038353\n",
      "Iteration 27, inertia 1523.868379627108\n",
      "Iteration 28, inertia 1523.8428635985001\n",
      "Iteration 29, inertia 1523.7958220817184\n",
      "Iteration 30, inertia 1523.7671270871158\n",
      "Iteration 31, inertia 1523.7465646902351\n",
      "Iteration 32, inertia 1523.7224992962063\n",
      "Iteration 33, inertia 1523.7081605741946\n",
      "Iteration 34, inertia 1523.7055315856992\n",
      "Iteration 35, inertia 1523.6979249992355\n",
      "Iteration 36, inertia 1523.695694902185\n",
      "Iteration 37, inertia 1523.6914727612043\n",
      "Converged at iteration 37: center shift 1.170453971955615e-06 within tolerance 1.436774592781648e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2071.6214756268605\n",
      "Iteration 1, inertia 1642.1596155358416\n",
      "Iteration 2, inertia 1601.8665627465796\n",
      "Iteration 3, inertia 1584.8640913178328\n",
      "Iteration 4, inertia 1573.2306136830707\n",
      "Iteration 5, inertia 1566.2619286515\n",
      "Iteration 6, inertia 1562.414910349353\n",
      "Iteration 7, inertia 1560.6835066504693\n",
      "Iteration 8, inertia 1559.6223436990736\n",
      "Iteration 9, inertia 1558.8909373317567\n",
      "Iteration 10, inertia 1558.4014756635222\n",
      "Iteration 11, inertia 1557.8688073648345\n",
      "Iteration 12, inertia 1557.3930610155946\n",
      "Iteration 13, inertia 1557.0331237976068\n",
      "Iteration 14, inertia 1556.77034335003\n",
      "Iteration 15, inertia 1556.501575533129\n",
      "Iteration 16, inertia 1556.0561858663755\n",
      "Iteration 17, inertia 1555.7032913671098\n",
      "Iteration 18, inertia 1555.4296643956302\n",
      "Iteration 19, inertia 1554.9071583636155\n",
      "Iteration 20, inertia 1554.1608837051792\n",
      "Iteration 21, inertia 1552.9080738723364\n",
      "Iteration 22, inertia 1550.6240138755975\n",
      "Iteration 23, inertia 1548.0908147777268\n",
      "Iteration 24, inertia 1546.5205803267897\n",
      "Iteration 25, inertia 1545.7747658132994\n",
      "Iteration 26, inertia 1545.2915346139666\n",
      "Iteration 27, inertia 1544.954935264094\n",
      "Iteration 28, inertia 1544.7871742167802\n",
      "Iteration 29, inertia 1544.6991200445125\n",
      "Iteration 30, inertia 1544.6310561104242\n",
      "Iteration 31, inertia 1544.5087914654177\n",
      "Iteration 32, inertia 1544.4147660893939\n",
      "Iteration 33, inertia 1544.3458675384586\n",
      "Iteration 34, inertia 1544.3007277080578\n",
      "Iteration 35, inertia 1544.2583264766683\n",
      "Iteration 36, inertia 1544.2236493648154\n",
      "Iteration 37, inertia 1544.1623289807394\n",
      "Iteration 38, inertia 1544.12707133098\n",
      "Iteration 39, inertia 1544.0870416760517\n",
      "Iteration 40, inertia 1544.0381037162133\n",
      "Iteration 41, inertia 1544.0002838345165\n",
      "Iteration 42, inertia 1543.9624434683433\n",
      "Iteration 43, inertia 1543.9266779694494\n",
      "Iteration 44, inertia 1543.8869554096\n",
      "Iteration 45, inertia 1543.8586379614676\n",
      "Iteration 46, inertia 1543.8249827514114\n",
      "Iteration 47, inertia 1543.7867603390798\n",
      "Iteration 48, inertia 1543.7735776934458\n",
      "Iteration 49, inertia 1543.7680351887766\n",
      "Iteration 50, inertia 1543.7496755488348\n",
      "Iteration 51, inertia 1543.68473999556\n",
      "Iteration 52, inertia 1543.5884840463775\n",
      "Iteration 53, inertia 1543.467251058892\n",
      "Iteration 54, inertia 1543.4046156301008\n",
      "Iteration 55, inertia 1543.3732778016874\n",
      "Iteration 56, inertia 1543.3457193379702\n",
      "Iteration 57, inertia 1543.3221218680274\n",
      "Iteration 58, inertia 1543.3053238744933\n",
      "Iteration 59, inertia 1543.3009792689893\n",
      "Iteration 60, inertia 1543.2987068261439\n",
      "Converged at iteration 60: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2288.0344354712215\n",
      "Iteration 1, inertia 1722.184843301212\n",
      "Iteration 2, inertia 1620.6218932344293\n",
      "Iteration 3, inertia 1578.9403177166016\n",
      "Iteration 4, inertia 1565.0941938157584\n",
      "Iteration 5, inertia 1559.0117944880049\n",
      "Iteration 6, inertia 1556.0326886781215\n",
      "Iteration 7, inertia 1554.6944429655234\n",
      "Iteration 8, inertia 1553.6193302507863\n",
      "Iteration 9, inertia 1552.8252878742885\n",
      "Iteration 10, inertia 1552.0480442672042\n",
      "Iteration 11, inertia 1551.3898928582903\n",
      "Iteration 12, inertia 1550.7761920179719\n",
      "Iteration 13, inertia 1550.32288366155\n",
      "Iteration 14, inertia 1549.9610516705725\n",
      "Iteration 15, inertia 1549.834940106391\n",
      "Iteration 16, inertia 1549.7722646710445\n",
      "Iteration 17, inertia 1549.6847662667349\n",
      "Iteration 18, inertia 1549.5240752579423\n",
      "Iteration 19, inertia 1549.2859463111033\n",
      "Iteration 20, inertia 1549.0555533995173\n",
      "Iteration 21, inertia 1548.775812281685\n",
      "Iteration 22, inertia 1548.3048542417514\n",
      "Iteration 23, inertia 1547.9314002568494\n",
      "Iteration 24, inertia 1547.6660100600614\n",
      "Iteration 25, inertia 1547.5426611597372\n",
      "Iteration 26, inertia 1547.488890974242\n",
      "Iteration 27, inertia 1547.479339586489\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2218.9805825243725\n",
      "Iteration 1, inertia 1684.88378540497\n",
      "Iteration 2, inertia 1629.9970901350227\n",
      "Iteration 3, inertia 1604.2344393642763\n",
      "Iteration 4, inertia 1582.868685315142\n",
      "Iteration 5, inertia 1563.337340960256\n",
      "Iteration 6, inertia 1555.488751793291\n",
      "Iteration 7, inertia 1551.2978736320908\n",
      "Iteration 8, inertia 1548.9484494051442\n",
      "Iteration 9, inertia 1546.9600115940943\n",
      "Iteration 10, inertia 1545.3762454166665\n",
      "Iteration 11, inertia 1544.2707550622813\n",
      "Iteration 12, inertia 1543.1002719897717\n",
      "Iteration 13, inertia 1542.1935968684047\n",
      "Iteration 14, inertia 1541.4947573046784\n",
      "Iteration 15, inertia 1541.0455383928506\n",
      "Iteration 16, inertia 1540.549353304499\n",
      "Iteration 17, inertia 1540.1972499085841\n",
      "Iteration 18, inertia 1539.9774913439153\n",
      "Iteration 19, inertia 1539.8575871019934\n",
      "Iteration 20, inertia 1539.7777835962145\n",
      "Iteration 21, inertia 1539.7610493101583\n",
      "Iteration 22, inertia 1539.7520178936331\n",
      "Iteration 23, inertia 1539.7426789810681\n",
      "Iteration 24, inertia 1539.7404725430438\n",
      "Iteration 25, inertia 1539.734514169285\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2210.8925307344507\n",
      "Iteration 1, inertia 1659.2344639482496\n",
      "Iteration 2, inertia 1615.115375636921\n",
      "Iteration 3, inertia 1592.8064577242087\n",
      "Iteration 4, inertia 1582.2008291248878\n",
      "Iteration 5, inertia 1577.1318756715755\n",
      "Iteration 6, inertia 1573.9440597424073\n",
      "Iteration 7, inertia 1571.0972198135762\n",
      "Iteration 8, inertia 1568.7330393060486\n",
      "Iteration 9, inertia 1566.4909805175346\n",
      "Iteration 10, inertia 1564.9454419555905\n",
      "Iteration 11, inertia 1564.2621728692457\n",
      "Iteration 12, inertia 1563.9512192182701\n",
      "Iteration 13, inertia 1563.7668698967823\n",
      "Iteration 14, inertia 1563.5155566007902\n",
      "Iteration 15, inertia 1563.2033334052328\n",
      "Iteration 16, inertia 1563.1235779394547\n",
      "Iteration 17, inertia 1563.0587682931746\n",
      "Iteration 18, inertia 1562.998529289303\n",
      "Iteration 19, inertia 1562.851090374747\n",
      "Iteration 20, inertia 1562.5962783378998\n",
      "Iteration 21, inertia 1562.4048769774245\n",
      "Iteration 22, inertia 1562.0847572323005\n",
      "Iteration 23, inertia 1561.8000060502839\n",
      "Iteration 24, inertia 1561.6368918941291\n",
      "Iteration 25, inertia 1561.557574418914\n",
      "Iteration 26, inertia 1561.5120866291354\n",
      "Iteration 27, inertia 1561.4775014349136\n",
      "Iteration 28, inertia 1561.4675016308172\n",
      "Iteration 29, inertia 1561.4187943745155\n",
      "Iteration 30, inertia 1561.3451114557326\n",
      "Iteration 31, inertia 1561.3163480917108\n",
      "Converged at iteration 31: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2058.922671106866\n",
      "Iteration 1, inertia 1628.2426941696078\n",
      "Iteration 2, inertia 1584.7228953309755\n",
      "Iteration 3, inertia 1573.2130289382933\n",
      "Iteration 4, inertia 1570.052269180283\n",
      "Iteration 5, inertia 1568.559862834893\n",
      "Iteration 6, inertia 1567.8550889853934\n",
      "Iteration 7, inertia 1567.3052358710825\n",
      "Iteration 8, inertia 1566.3879001172838\n",
      "Iteration 9, inertia 1565.2728647855997\n",
      "Iteration 10, inertia 1564.557357857206\n",
      "Iteration 11, inertia 1563.835792399893\n",
      "Iteration 12, inertia 1563.4694726371572\n",
      "Iteration 13, inertia 1563.210655390421\n",
      "Iteration 14, inertia 1563.0563340035656\n",
      "Iteration 15, inertia 1562.8518047913853\n",
      "Iteration 16, inertia 1562.6816477641935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, inertia 1562.6365690835155\n",
      "Iteration 18, inertia 1562.6077523516897\n",
      "Iteration 19, inertia 1562.5746464563113\n",
      "Iteration 20, inertia 1562.5562329760733\n",
      "Iteration 21, inertia 1562.5314594854638\n",
      "Iteration 22, inertia 1562.5090280827317\n",
      "Iteration 23, inertia 1562.4977048922228\n",
      "Iteration 24, inertia 1562.4877803012796\n",
      "Iteration 25, inertia 1562.4730775478304\n",
      "Iteration 26, inertia 1562.4604525086074\n",
      "Iteration 27, inertia 1562.4563883906178\n",
      "Iteration 28, inertia 1562.4470262796217\n",
      "Iteration 29, inertia 1562.4348374971262\n",
      "Iteration 30, inertia 1562.4321898571075\n",
      "Converged at iteration 30: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2098.3754155056436\n",
      "Iteration 1, inertia 1642.5250848001233\n",
      "Iteration 2, inertia 1607.2188044525908\n",
      "Iteration 3, inertia 1595.1740770216895\n",
      "Iteration 4, inertia 1591.1654687270666\n",
      "Iteration 5, inertia 1589.9313657484597\n",
      "Iteration 6, inertia 1589.1019446326352\n",
      "Iteration 7, inertia 1588.6550433242603\n",
      "Iteration 8, inertia 1587.927653668984\n",
      "Iteration 9, inertia 1586.2273514444412\n",
      "Iteration 10, inertia 1581.3592087054071\n",
      "Iteration 11, inertia 1577.1696022941396\n",
      "Iteration 12, inertia 1575.7637138390955\n",
      "Iteration 13, inertia 1575.0678916437912\n",
      "Iteration 14, inertia 1574.9840618088103\n",
      "Iteration 15, inertia 1574.9158209898503\n",
      "Iteration 16, inertia 1574.836510630273\n",
      "Iteration 17, inertia 1574.7821401501737\n",
      "Iteration 18, inertia 1574.7173175135545\n",
      "Iteration 19, inertia 1574.6506828689432\n",
      "Iteration 20, inertia 1574.5272003281175\n",
      "Iteration 21, inertia 1574.3252539256423\n",
      "Iteration 22, inertia 1573.9601790079853\n",
      "Iteration 23, inertia 1573.3953238157062\n",
      "Iteration 24, inertia 1572.9417993391664\n",
      "Iteration 25, inertia 1572.754088049522\n",
      "Iteration 26, inertia 1572.4551634115492\n",
      "Iteration 27, inertia 1572.2183405798023\n",
      "Iteration 28, inertia 1571.9276533539528\n",
      "Iteration 29, inertia 1571.581956224482\n",
      "Iteration 30, inertia 1571.0962445204539\n",
      "Iteration 31, inertia 1570.466813766028\n",
      "Iteration 32, inertia 1569.2946634274872\n",
      "Iteration 33, inertia 1567.096179065866\n",
      "Iteration 34, inertia 1564.291786310285\n",
      "Iteration 35, inertia 1562.5423613647379\n",
      "Iteration 36, inertia 1561.3953261218435\n",
      "Iteration 37, inertia 1560.724197176892\n",
      "Iteration 38, inertia 1560.3274395931542\n",
      "Iteration 39, inertia 1559.94737428601\n",
      "Iteration 40, inertia 1559.726080794719\n",
      "Iteration 41, inertia 1559.5573791539714\n",
      "Iteration 42, inertia 1559.36838448225\n",
      "Iteration 43, inertia 1559.2528342507576\n",
      "Iteration 44, inertia 1559.1844586460973\n",
      "Iteration 45, inertia 1559.140001242041\n",
      "Iteration 46, inertia 1559.1100716924655\n",
      "Iteration 47, inertia 1559.0739052569\n",
      "Iteration 48, inertia 1559.042041788073\n",
      "Iteration 49, inertia 1559.0076738896505\n",
      "Iteration 50, inertia 1558.9900428234823\n",
      "Iteration 51, inertia 1558.9799938039737\n",
      "Iteration 52, inertia 1558.9780805509142\n",
      "Iteration 53, inertia 1558.974167892445\n",
      "Converged at iteration 53: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2152.621812061493\n",
      "Iteration 1, inertia 1649.716306247215\n",
      "Iteration 2, inertia 1600.6840731661982\n",
      "Iteration 3, inertia 1574.7222976908688\n",
      "Iteration 4, inertia 1555.8175228375583\n",
      "Iteration 5, inertia 1546.4354242757258\n",
      "Iteration 6, inertia 1542.767130669898\n",
      "Iteration 7, inertia 1541.6159501194445\n",
      "Iteration 8, inertia 1541.109225500111\n",
      "Iteration 9, inertia 1540.9640961345735\n",
      "Iteration 10, inertia 1540.9048080076784\n",
      "Iteration 11, inertia 1540.830159916311\n",
      "Iteration 12, inertia 1540.7409922642644\n",
      "Iteration 13, inertia 1540.6688960811064\n",
      "Iteration 14, inertia 1540.6180487764987\n",
      "Iteration 15, inertia 1540.5723019766963\n",
      "Iteration 16, inertia 1540.5575057760627\n",
      "Iteration 17, inertia 1540.5362408777871\n",
      "Iteration 18, inertia 1540.5142799210576\n",
      "Iteration 19, inertia 1540.4881459995308\n",
      "Iteration 20, inertia 1540.4599398851303\n",
      "Iteration 21, inertia 1540.4345467016656\n",
      "Iteration 22, inertia 1540.4002452536415\n",
      "Iteration 23, inertia 1540.377848368294\n",
      "Iteration 24, inertia 1540.3533522353341\n",
      "Iteration 25, inertia 1540.3109461260244\n",
      "Iteration 26, inertia 1540.2006221962572\n",
      "Iteration 27, inertia 1539.9902229068634\n",
      "Iteration 28, inertia 1539.7332609817736\n",
      "Iteration 29, inertia 1539.60757042666\n",
      "Iteration 30, inertia 1539.5049544683886\n",
      "Iteration 31, inertia 1539.4150683635312\n",
      "Iteration 32, inertia 1539.3297913102615\n",
      "Iteration 33, inertia 1539.3066030240886\n",
      "Iteration 34, inertia 1539.304070898496\n",
      "Converged at iteration 34: strict convergence.\n",
      "Training a K-Means model with 19 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2151.5755095884188\n",
      "Iteration 1, inertia 1624.2119428861513\n",
      "Iteration 2, inertia 1575.7661160757593\n",
      "Iteration 3, inertia 1553.7944201762045\n",
      "Iteration 4, inertia 1540.983506018915\n",
      "Iteration 5, inertia 1534.432216878074\n",
      "Iteration 6, inertia 1529.8167790591763\n",
      "Iteration 7, inertia 1527.4601803673945\n",
      "Iteration 8, inertia 1525.8014547902837\n",
      "Iteration 9, inertia 1524.767995916224\n",
      "Iteration 10, inertia 1523.918164845596\n",
      "Iteration 11, inertia 1522.9986792762707\n",
      "Iteration 12, inertia 1521.8086241539747\n",
      "Iteration 13, inertia 1519.6881293635236\n",
      "Iteration 14, inertia 1517.1815118932477\n",
      "Iteration 15, inertia 1515.9595959744186\n",
      "Iteration 16, inertia 1515.2412802732008\n",
      "Iteration 17, inertia 1514.5126653835725\n",
      "Iteration 18, inertia 1514.162194017863\n",
      "Iteration 19, inertia 1513.8426667641777\n",
      "Iteration 20, inertia 1513.5623871262621\n",
      "Iteration 21, inertia 1513.222040855309\n",
      "Iteration 22, inertia 1512.9961909727754\n",
      "Iteration 23, inertia 1512.8696759087775\n",
      "Iteration 24, inertia 1512.7456003707089\n",
      "Iteration 25, inertia 1512.5216948098755\n",
      "Iteration 26, inertia 1511.7373931464888\n",
      "Iteration 27, inertia 1508.8841821717954\n",
      "Iteration 28, inertia 1505.3727407710924\n",
      "Iteration 29, inertia 1504.7561900000273\n",
      "Iteration 30, inertia 1504.467142698365\n",
      "Iteration 31, inertia 1504.3205160741782\n",
      "Iteration 32, inertia 1504.2240497179876\n",
      "Iteration 33, inertia 1504.1600816430143\n",
      "Iteration 34, inertia 1504.129287817645\n",
      "Iteration 35, inertia 1504.1143997284642\n",
      "Iteration 36, inertia 1504.1026534650725\n",
      "Iteration 37, inertia 1504.0935664600354\n",
      "Iteration 38, inertia 1504.0925389523559\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2188.030715566665\n",
      "Iteration 1, inertia 1641.545070573401\n",
      "Iteration 2, inertia 1584.5655543253902\n",
      "Iteration 3, inertia 1563.0813537630097\n",
      "Iteration 4, inertia 1548.141107884776\n",
      "Iteration 5, inertia 1539.9522700769191\n",
      "Iteration 6, inertia 1536.3469483611575\n",
      "Iteration 7, inertia 1534.5239345668547\n",
      "Iteration 8, inertia 1532.503794566673\n",
      "Iteration 9, inertia 1530.9685766419575\n",
      "Iteration 10, inertia 1529.708766602108\n",
      "Iteration 11, inertia 1528.5114328740954\n",
      "Iteration 12, inertia 1527.368692098567\n",
      "Iteration 13, inertia 1526.6522150946337\n",
      "Iteration 14, inertia 1526.1479461283136\n",
      "Iteration 15, inertia 1525.8600256311436\n",
      "Iteration 16, inertia 1525.5564945972314\n",
      "Iteration 17, inertia 1525.0560823278302\n",
      "Iteration 18, inertia 1524.4549587284391\n",
      "Iteration 19, inertia 1523.7876397102755\n",
      "Iteration 20, inertia 1522.9073386273626\n",
      "Iteration 21, inertia 1521.266720840568\n",
      "Iteration 22, inertia 1519.4123891604474\n",
      "Iteration 23, inertia 1516.2850858916172\n",
      "Iteration 24, inertia 1510.7404909560944\n",
      "Iteration 25, inertia 1507.7748999881046\n",
      "Iteration 26, inertia 1506.5415396150875\n",
      "Iteration 27, inertia 1505.388900073352\n",
      "Iteration 28, inertia 1504.6937141221813\n",
      "Iteration 29, inertia 1504.255928047413\n",
      "Iteration 30, inertia 1504.056713055163\n",
      "Iteration 31, inertia 1503.9188438347796\n",
      "Iteration 32, inertia 1503.767312024346\n",
      "Iteration 33, inertia 1503.6886752660075\n",
      "Iteration 34, inertia 1503.640303325382\n",
      "Iteration 35, inertia 1503.5951087250332\n",
      "Iteration 36, inertia 1503.5414277185628\n",
      "Iteration 37, inertia 1503.4739480123383\n",
      "Iteration 38, inertia 1503.4359236417101\n",
      "Iteration 39, inertia 1503.3744323049214\n",
      "Iteration 40, inertia 1503.332596951696\n",
      "Iteration 41, inertia 1503.3284521526714\n",
      "Iteration 42, inertia 1503.3233183603538\n",
      "Iteration 43, inertia 1503.2947103403933\n",
      "Iteration 44, inertia 1503.2791081991666\n",
      "Iteration 45, inertia 1503.2773717264358\n",
      "Iteration 46, inertia 1503.274841855317\n",
      "Iteration 47, inertia 1503.2716718502606\n",
      "Iteration 48, inertia 1503.2455148846839\n",
      "Iteration 49, inertia 1503.2231697452505\n",
      "Iteration 50, inertia 1503.1953375545693\n",
      "Iteration 51, inertia 1503.1776379264306\n",
      "Iteration 52, inertia 1503.1607678106825\n",
      "Iteration 53, inertia 1503.136633530405\n",
      "Iteration 54, inertia 1503.1352852461157\n",
      "Converged at iteration 54: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2304.2978288780687\n",
      "Iteration 1, inertia 1680.612773076908\n",
      "Iteration 2, inertia 1605.1154472114852\n",
      "Iteration 3, inertia 1571.4860579087097\n",
      "Iteration 4, inertia 1549.13013145023\n",
      "Iteration 5, inertia 1525.9677379193568\n",
      "Iteration 6, inertia 1513.4749556354914\n",
      "Iteration 7, inertia 1506.758876112544\n",
      "Iteration 8, inertia 1502.9119017265625\n",
      "Iteration 9, inertia 1499.7362193580698\n",
      "Iteration 10, inertia 1497.4163718715995\n",
      "Iteration 11, inertia 1496.5632099496668\n",
      "Iteration 12, inertia 1495.828318296324\n",
      "Iteration 13, inertia 1494.9464001187193\n",
      "Iteration 14, inertia 1494.433556162806\n",
      "Iteration 15, inertia 1493.7643039221455\n",
      "Iteration 16, inertia 1492.8207701208414\n",
      "Iteration 17, inertia 1491.7542542102155\n",
      "Iteration 18, inertia 1491.1101411049112\n",
      "Iteration 19, inertia 1490.6494500873514\n",
      "Iteration 20, inertia 1490.274702196408\n",
      "Iteration 21, inertia 1490.050975971034\n",
      "Iteration 22, inertia 1489.8263865026256\n",
      "Iteration 23, inertia 1489.680730666691\n",
      "Iteration 24, inertia 1489.567684395652\n",
      "Iteration 25, inertia 1489.3919201675717\n",
      "Iteration 26, inertia 1489.0959767496367\n",
      "Iteration 27, inertia 1488.6414427032983\n",
      "Iteration 28, inertia 1488.1195881578246\n",
      "Iteration 29, inertia 1487.7168857326405\n",
      "Iteration 30, inertia 1487.4270430758797\n",
      "Iteration 31, inertia 1487.24470935158\n",
      "Iteration 32, inertia 1487.0348210685354\n",
      "Iteration 33, inertia 1486.7688603282654\n",
      "Iteration 34, inertia 1486.5591196405005\n",
      "Iteration 35, inertia 1486.4914557135435\n",
      "Iteration 36, inertia 1486.445025039925\n",
      "Iteration 37, inertia 1486.3948934217158\n",
      "Iteration 38, inertia 1486.362723955262\n",
      "Iteration 39, inertia 1486.3284592127336\n",
      "Iteration 40, inertia 1486.3002408872571\n",
      "Iteration 41, inertia 1486.273055377866\n",
      "Iteration 42, inertia 1486.2408822616528\n",
      "Iteration 43, inertia 1486.1909922669402\n",
      "Iteration 44, inertia 1486.1523548249934\n",
      "Iteration 45, inertia 1486.107946539792\n",
      "Iteration 46, inertia 1486.0540516952724\n",
      "Iteration 47, inertia 1486.0448171124945\n",
      "Iteration 48, inertia 1486.0428613565164\n",
      "Converged at iteration 48: strict convergence.\n",
      "Initialization complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, inertia 2081.679914352184\n",
      "Iteration 1, inertia 1602.274868383937\n",
      "Iteration 2, inertia 1563.8608525376453\n",
      "Iteration 3, inertia 1551.3925088372825\n",
      "Iteration 4, inertia 1544.5180091982388\n",
      "Iteration 5, inertia 1539.076727410821\n",
      "Iteration 6, inertia 1535.19145515209\n",
      "Iteration 7, inertia 1533.184939901093\n",
      "Iteration 8, inertia 1532.2825095429905\n",
      "Iteration 9, inertia 1531.5515990760455\n",
      "Iteration 10, inertia 1530.6387725805596\n",
      "Iteration 11, inertia 1529.6313099713923\n",
      "Iteration 12, inertia 1528.7935573081065\n",
      "Iteration 13, inertia 1528.0436285237995\n",
      "Iteration 14, inertia 1527.3862280800508\n",
      "Iteration 15, inertia 1526.7480454318238\n",
      "Iteration 16, inertia 1525.841502100488\n",
      "Iteration 17, inertia 1523.7881546368508\n",
      "Iteration 18, inertia 1521.421200999272\n",
      "Iteration 19, inertia 1519.417147906164\n",
      "Iteration 20, inertia 1518.3698211912065\n",
      "Iteration 21, inertia 1518.0918594765678\n",
      "Iteration 22, inertia 1518.0032807551931\n",
      "Iteration 23, inertia 1517.9314612395976\n",
      "Iteration 24, inertia 1517.8769158324774\n",
      "Iteration 25, inertia 1517.8667116380784\n",
      "Iteration 26, inertia 1517.8602900757892\n",
      "Iteration 27, inertia 1517.8553392162428\n",
      "Iteration 28, inertia 1517.8474934104433\n",
      "Iteration 29, inertia 1517.8336379187424\n",
      "Iteration 30, inertia 1517.815834045897\n",
      "Iteration 31, inertia 1517.8101255468457\n",
      "Iteration 32, inertia 1517.8076826879033\n",
      "Iteration 33, inertia 1517.8052674759076\n",
      "Iteration 34, inertia 1517.7968732431457\n",
      "Iteration 35, inertia 1517.7893594625386\n",
      "Iteration 36, inertia 1517.7773593849072\n",
      "Iteration 37, inertia 1517.773442833408\n",
      "Iteration 38, inertia 1517.769493019337\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1965.2313865709102\n",
      "Iteration 1, inertia 1599.898677119024\n",
      "Iteration 2, inertia 1549.6112489017705\n",
      "Iteration 3, inertia 1527.2737549816272\n",
      "Iteration 4, inertia 1517.2828214277592\n",
      "Iteration 5, inertia 1511.8144645727198\n",
      "Iteration 6, inertia 1508.096366654584\n",
      "Iteration 7, inertia 1504.8468747359277\n",
      "Iteration 8, inertia 1502.067109113898\n",
      "Iteration 9, inertia 1500.376764594243\n",
      "Iteration 10, inertia 1499.252422939228\n",
      "Iteration 11, inertia 1498.7303202598102\n",
      "Iteration 12, inertia 1498.3477366399172\n",
      "Iteration 13, inertia 1498.129230522904\n",
      "Iteration 14, inertia 1498.0274905430833\n",
      "Iteration 15, inertia 1497.97377919875\n",
      "Iteration 16, inertia 1497.9250237096721\n",
      "Iteration 17, inertia 1497.8994983211478\n",
      "Iteration 18, inertia 1497.895401228512\n",
      "Iteration 19, inertia 1497.8919758705072\n",
      "Iteration 20, inertia 1497.8901026414253\n",
      "Iteration 21, inertia 1497.889097868041\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2007.1280962543005\n",
      "Iteration 1, inertia 1605.1170888661768\n",
      "Iteration 2, inertia 1555.9524836749401\n",
      "Iteration 3, inertia 1538.1636470621713\n",
      "Iteration 4, inertia 1530.5672279952778\n",
      "Iteration 5, inertia 1527.923460420451\n",
      "Iteration 6, inertia 1526.3675331912825\n",
      "Iteration 7, inertia 1524.8151990755866\n",
      "Iteration 8, inertia 1523.2646498592558\n",
      "Iteration 9, inertia 1521.2844623106012\n",
      "Iteration 10, inertia 1520.110127983153\n",
      "Iteration 11, inertia 1518.9732247295042\n",
      "Iteration 12, inertia 1517.87520643359\n",
      "Iteration 13, inertia 1517.0810632179164\n",
      "Iteration 14, inertia 1516.3107647533195\n",
      "Iteration 15, inertia 1515.5828962271594\n",
      "Iteration 16, inertia 1515.2574007927312\n",
      "Iteration 17, inertia 1515.1344182110072\n",
      "Iteration 18, inertia 1515.0298665886219\n",
      "Iteration 19, inertia 1514.974230033799\n",
      "Iteration 20, inertia 1514.9453187950521\n",
      "Iteration 21, inertia 1514.8846586784537\n",
      "Iteration 22, inertia 1514.7854761704068\n",
      "Iteration 23, inertia 1514.664291426224\n",
      "Iteration 24, inertia 1514.5875712508978\n",
      "Iteration 25, inertia 1514.5703144330191\n",
      "Iteration 26, inertia 1514.5637701778473\n",
      "Iteration 27, inertia 1514.56223773219\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2056.912012603782\n",
      "Iteration 1, inertia 1626.7932581014027\n",
      "Iteration 2, inertia 1576.6214014557481\n",
      "Iteration 3, inertia 1548.1212101462163\n",
      "Iteration 4, inertia 1533.3684612302923\n",
      "Iteration 5, inertia 1526.0521277862572\n",
      "Iteration 6, inertia 1521.5858413025287\n",
      "Iteration 7, inertia 1518.6045887331807\n",
      "Iteration 8, inertia 1516.6343384736472\n",
      "Iteration 9, inertia 1515.1865854684847\n",
      "Iteration 10, inertia 1514.2252512929922\n",
      "Iteration 11, inertia 1513.7298452816935\n",
      "Iteration 12, inertia 1513.3533327888777\n",
      "Iteration 13, inertia 1512.973214709599\n",
      "Iteration 14, inertia 1512.8540584997968\n",
      "Iteration 15, inertia 1512.8071348932244\n",
      "Iteration 16, inertia 1512.7391631311464\n",
      "Iteration 17, inertia 1512.6883127143133\n",
      "Iteration 18, inertia 1512.6777294698525\n",
      "Iteration 19, inertia 1512.663011102832\n",
      "Iteration 20, inertia 1512.6547818800573\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2112.720701815574\n",
      "Iteration 1, inertia 1665.9423573536526\n",
      "Iteration 2, inertia 1598.0929248527768\n",
      "Iteration 3, inertia 1567.6006952174612\n",
      "Iteration 4, inertia 1557.6292910707725\n",
      "Iteration 5, inertia 1552.450142198062\n",
      "Iteration 6, inertia 1549.980698743156\n",
      "Iteration 7, inertia 1548.4167337574422\n",
      "Iteration 8, inertia 1547.4331500898404\n",
      "Iteration 9, inertia 1546.824203354508\n",
      "Iteration 10, inertia 1546.3186081315084\n",
      "Iteration 11, inertia 1545.7842266662506\n",
      "Iteration 12, inertia 1545.6020788832943\n",
      "Iteration 13, inertia 1545.4700368033236\n",
      "Iteration 14, inertia 1545.3534057768115\n",
      "Iteration 15, inertia 1545.2616749767146\n",
      "Iteration 16, inertia 1545.1995254124063\n",
      "Iteration 17, inertia 1545.1436215791146\n",
      "Iteration 18, inertia 1545.0819933251387\n",
      "Iteration 19, inertia 1545.041689514267\n",
      "Iteration 20, inertia 1544.996472358349\n",
      "Iteration 21, inertia 1544.98651396837\n",
      "Iteration 22, inertia 1544.980684593008\n",
      "Iteration 23, inertia 1544.9799843442775\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2090.2637458397617\n",
      "Iteration 1, inertia 1617.2033521236974\n",
      "Iteration 2, inertia 1557.2753677338324\n",
      "Iteration 3, inertia 1531.1390126432132\n",
      "Iteration 4, inertia 1514.956655484475\n",
      "Iteration 5, inertia 1507.0128494881485\n",
      "Iteration 6, inertia 1503.525110001829\n",
      "Iteration 7, inertia 1501.949803394134\n",
      "Iteration 8, inertia 1501.2445287897865\n",
      "Iteration 9, inertia 1500.6348075597941\n",
      "Iteration 10, inertia 1500.2153601939578\n",
      "Iteration 11, inertia 1499.924284459271\n",
      "Iteration 12, inertia 1499.7355700399792\n",
      "Iteration 13, inertia 1499.6101607104747\n",
      "Iteration 14, inertia 1499.5587179664367\n",
      "Iteration 15, inertia 1499.5310068043345\n",
      "Iteration 16, inertia 1499.5075129098193\n",
      "Iteration 17, inertia 1499.4898778053835\n",
      "Iteration 18, inertia 1499.4801007288268\n",
      "Iteration 19, inertia 1499.4782590795176\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1996.3317502488492\n",
      "Iteration 1, inertia 1605.6730821904534\n",
      "Iteration 2, inertia 1576.031344613475\n",
      "Iteration 3, inertia 1564.7329651825787\n",
      "Iteration 4, inertia 1555.1141690172942\n",
      "Iteration 5, inertia 1544.6888310005247\n",
      "Iteration 6, inertia 1536.9794416128518\n",
      "Iteration 7, inertia 1533.1680038148222\n",
      "Iteration 8, inertia 1531.166898908399\n",
      "Iteration 9, inertia 1529.5734796144932\n",
      "Iteration 10, inertia 1528.0234107457836\n",
      "Iteration 11, inertia 1526.570007479784\n",
      "Iteration 12, inertia 1525.172356951237\n",
      "Iteration 13, inertia 1523.962273611147\n",
      "Iteration 14, inertia 1522.2108225728712\n",
      "Iteration 15, inertia 1519.9015876772708\n",
      "Iteration 16, inertia 1518.5945476775805\n",
      "Iteration 17, inertia 1517.8726281677489\n",
      "Iteration 18, inertia 1517.482390142449\n",
      "Iteration 19, inertia 1517.2338945417287\n",
      "Iteration 20, inertia 1517.1334367117747\n",
      "Iteration 21, inertia 1517.0940931658013\n",
      "Iteration 22, inertia 1517.0901556180931\n",
      "Iteration 23, inertia 1517.066745069105\n",
      "Converged at iteration 23: strict convergence.\n",
      "Training a K-Means model with 20 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2116.660236677808\n",
      "Iteration 1, inertia 1586.490131558249\n",
      "Iteration 2, inertia 1538.3021518720798\n",
      "Iteration 3, inertia 1519.1245464085644\n",
      "Iteration 4, inertia 1510.8668706570143\n",
      "Iteration 5, inertia 1506.0937229605759\n",
      "Iteration 6, inertia 1501.6743330569197\n",
      "Iteration 7, inertia 1498.802606588053\n",
      "Iteration 8, inertia 1495.642316321063\n",
      "Iteration 9, inertia 1491.7206770538307\n",
      "Iteration 10, inertia 1487.5818030443463\n",
      "Iteration 11, inertia 1484.518024743607\n",
      "Iteration 12, inertia 1483.1844845688927\n",
      "Iteration 13, inertia 1482.4488476494585\n",
      "Iteration 14, inertia 1481.8335871928837\n",
      "Iteration 15, inertia 1481.5944550055785\n",
      "Iteration 16, inertia 1481.361221103551\n",
      "Iteration 17, inertia 1481.1451776650117\n",
      "Iteration 18, inertia 1481.0498894103157\n",
      "Iteration 19, inertia 1480.9455250489736\n",
      "Iteration 20, inertia 1480.8347151620942\n",
      "Iteration 21, inertia 1480.7076680876019\n",
      "Iteration 22, inertia 1480.655094003436\n",
      "Iteration 23, inertia 1480.6082336825025\n",
      "Iteration 24, inertia 1480.5328084957314\n",
      "Iteration 25, inertia 1480.4759435390565\n",
      "Iteration 26, inertia 1480.4199174437094\n",
      "Iteration 27, inertia 1480.3172012201107\n",
      "Iteration 28, inertia 1480.2185076779856\n",
      "Iteration 29, inertia 1480.1031156533013\n",
      "Iteration 30, inertia 1479.975006506006\n",
      "Iteration 31, inertia 1479.9430317976667\n",
      "Iteration 32, inertia 1479.900847225875\n",
      "Iteration 33, inertia 1479.7911456267425\n",
      "Iteration 34, inertia 1479.6509682941169\n",
      "Iteration 35, inertia 1479.5680734696714\n",
      "Iteration 36, inertia 1479.491174501793\n",
      "Iteration 37, inertia 1479.4505648622926\n",
      "Iteration 38, inertia 1479.409608371599\n",
      "Iteration 39, inertia 1479.365985135447\n",
      "Iteration 40, inertia 1479.3217948433482\n",
      "Iteration 41, inertia 1479.270389755208\n",
      "Iteration 42, inertia 1479.2071356781782\n",
      "Iteration 43, inertia 1479.0134813722325\n",
      "Iteration 44, inertia 1478.6866678836857\n",
      "Iteration 45, inertia 1478.224028479793\n",
      "Iteration 46, inertia 1477.5004690507872\n",
      "Iteration 47, inertia 1476.5079153101458\n",
      "Iteration 48, inertia 1474.8862114324097\n",
      "Iteration 49, inertia 1473.4937222171484\n",
      "Iteration 50, inertia 1473.135461749066\n",
      "Iteration 51, inertia 1473.0521972494557\n",
      "Iteration 52, inertia 1473.0135831279604\n",
      "Iteration 53, inertia 1473.0056039399199\n",
      "Iteration 54, inertia 1473.002952353348\n",
      "Iteration 55, inertia 1473.00191250551\n",
      "Iteration 56, inertia 1473.0005546511256\n",
      "Converged at iteration 56: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2077.2507148693276\n",
      "Iteration 1, inertia 1572.5210892955768\n",
      "Iteration 2, inertia 1519.583920958413\n",
      "Iteration 3, inertia 1498.0568907330435\n",
      "Iteration 4, inertia 1488.6206276391463\n",
      "Iteration 5, inertia 1484.9130143364223\n",
      "Iteration 6, inertia 1482.7682467774841\n",
      "Iteration 7, inertia 1481.4055438591172\n",
      "Iteration 8, inertia 1480.6875952658108\n",
      "Iteration 9, inertia 1480.2566155274503\n",
      "Iteration 10, inertia 1479.8790883444865\n",
      "Iteration 11, inertia 1479.4855526439537\n",
      "Iteration 12, inertia 1479.1828662398823\n",
      "Iteration 13, inertia 1478.9090715519237\n",
      "Iteration 14, inertia 1478.5228731581353\n",
      "Iteration 15, inertia 1478.1208941834334\n",
      "Iteration 16, inertia 1477.8524729002045\n",
      "Iteration 17, inertia 1477.7375374317191\n",
      "Iteration 18, inertia 1477.655989418094\n",
      "Iteration 19, inertia 1477.5622764491463\n",
      "Iteration 20, inertia 1477.5284646095324\n",
      "Iteration 21, inertia 1477.5025067617223\n",
      "Iteration 22, inertia 1477.4828736891943\n",
      "Iteration 23, inertia 1477.4602229816126\n",
      "Iteration 24, inertia 1477.4575973293563\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2023.717169604493\n",
      "Iteration 1, inertia 1616.903297413579\n",
      "Iteration 2, inertia 1576.5366713031012\n",
      "Iteration 3, inertia 1555.9271485669349\n",
      "Iteration 4, inertia 1545.2821030919358\n",
      "Iteration 5, inertia 1539.789503319148\n",
      "Iteration 6, inertia 1536.251440794029\n",
      "Iteration 7, inertia 1532.7775865351955\n",
      "Iteration 8, inertia 1526.512604890287\n",
      "Iteration 9, inertia 1520.5000784574472\n",
      "Iteration 10, inertia 1517.2479498342104\n",
      "Iteration 11, inertia 1515.1574731661797\n",
      "Iteration 12, inertia 1514.2329852954128\n",
      "Iteration 13, inertia 1513.5758162289576\n",
      "Iteration 14, inertia 1512.749233913406\n",
      "Iteration 15, inertia 1511.7951957591129\n",
      "Iteration 16, inertia 1510.5684403349317\n",
      "Iteration 17, inertia 1508.7253071807636\n",
      "Iteration 18, inertia 1506.2553273344927\n",
      "Iteration 19, inertia 1504.595782725462\n",
      "Iteration 20, inertia 1503.7707988716684\n",
      "Iteration 21, inertia 1503.5255392085776\n",
      "Iteration 22, inertia 1503.3327796043775\n",
      "Iteration 23, inertia 1503.1735919651594\n",
      "Iteration 24, inertia 1502.9271012928361\n",
      "Iteration 25, inertia 1502.6787137341053\n",
      "Iteration 26, inertia 1502.5494442960664\n",
      "Iteration 27, inertia 1502.495824652775\n",
      "Iteration 28, inertia 1502.4408274186442\n",
      "Iteration 29, inertia 1502.3467314303466\n",
      "Iteration 30, inertia 1502.296675500468\n",
      "Iteration 31, inertia 1502.2652220831246\n",
      "Iteration 32, inertia 1502.2458313155394\n",
      "Iteration 33, inertia 1502.2057297275971\n",
      "Iteration 34, inertia 1502.1758642384514\n",
      "Iteration 35, inertia 1502.1629778033864\n",
      "Converged at iteration 35: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2017.0622365713996\n",
      "Iteration 1, inertia 1590.784613720467\n",
      "Iteration 2, inertia 1553.5160232454125\n",
      "Iteration 3, inertia 1537.1532217488518\n",
      "Iteration 4, inertia 1528.5642697794449\n",
      "Iteration 5, inertia 1520.9397393433928\n",
      "Iteration 6, inertia 1511.1435684767305\n",
      "Iteration 7, inertia 1499.1206780610007\n",
      "Iteration 8, inertia 1489.9496064330315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, inertia 1485.8631977339955\n",
      "Iteration 10, inertia 1484.2356150971289\n",
      "Iteration 11, inertia 1483.580658046958\n",
      "Iteration 12, inertia 1483.122021027728\n",
      "Iteration 13, inertia 1482.7281122777908\n",
      "Iteration 14, inertia 1482.448547210978\n",
      "Iteration 15, inertia 1482.2610312075617\n",
      "Iteration 16, inertia 1482.122679356511\n",
      "Iteration 17, inertia 1481.9647345239996\n",
      "Iteration 18, inertia 1481.9276722652762\n",
      "Iteration 19, inertia 1481.8938704923949\n",
      "Iteration 20, inertia 1481.8738564178136\n",
      "Iteration 21, inertia 1481.8544162510343\n",
      "Iteration 22, inertia 1481.8373592372209\n",
      "Iteration 23, inertia 1481.8106622400442\n",
      "Iteration 24, inertia 1481.7898218840403\n",
      "Iteration 25, inertia 1481.7765182366936\n",
      "Iteration 26, inertia 1481.763304899683\n",
      "Iteration 27, inertia 1481.756577148588\n",
      "Iteration 28, inertia 1481.750366857887\n",
      "Iteration 29, inertia 1481.74884477292\n",
      "Iteration 30, inertia 1481.7467742407546\n",
      "Converged at iteration 30: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2079.669975616152\n",
      "Iteration 1, inertia 1623.8562270403195\n",
      "Iteration 2, inertia 1582.4710430589319\n",
      "Iteration 3, inertia 1559.3248508496108\n",
      "Iteration 4, inertia 1540.0631096269888\n",
      "Iteration 5, inertia 1527.491295541945\n",
      "Iteration 6, inertia 1521.640954029459\n",
      "Iteration 7, inertia 1517.6945391348822\n",
      "Iteration 8, inertia 1512.6981464575526\n",
      "Iteration 9, inertia 1508.2347079648353\n",
      "Iteration 10, inertia 1506.377821872868\n",
      "Iteration 11, inertia 1505.378266187457\n",
      "Iteration 12, inertia 1505.0587979516672\n",
      "Iteration 13, inertia 1504.9710492919005\n",
      "Iteration 14, inertia 1504.8170684581853\n",
      "Iteration 15, inertia 1504.7361470714297\n",
      "Iteration 16, inertia 1504.5990524772672\n",
      "Iteration 17, inertia 1504.4391750847938\n",
      "Iteration 18, inertia 1504.2584394370438\n",
      "Iteration 19, inertia 1504.0298215630003\n",
      "Iteration 20, inertia 1503.6783330697265\n",
      "Iteration 21, inertia 1503.4477657310113\n",
      "Iteration 22, inertia 1503.301432966566\n",
      "Iteration 23, inertia 1503.2129127167057\n",
      "Iteration 24, inertia 1503.1689876002674\n",
      "Iteration 25, inertia 1503.1426879585904\n",
      "Iteration 26, inertia 1503.128722234674\n",
      "Iteration 27, inertia 1503.1255777199926\n",
      "Iteration 28, inertia 1503.1245877645863\n",
      "Converged at iteration 28: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2111.0578714158146\n",
      "Iteration 1, inertia 1593.526592122757\n",
      "Iteration 2, inertia 1542.1572043614224\n",
      "Iteration 3, inertia 1527.6846426357847\n",
      "Iteration 4, inertia 1520.0755639437143\n",
      "Iteration 5, inertia 1514.2222351989165\n",
      "Iteration 6, inertia 1509.269217012807\n",
      "Iteration 7, inertia 1505.158321818434\n",
      "Iteration 8, inertia 1501.6191268244827\n",
      "Iteration 9, inertia 1499.6498929213508\n",
      "Iteration 10, inertia 1497.811366038126\n",
      "Iteration 11, inertia 1496.2088797439164\n",
      "Iteration 12, inertia 1495.5531679919336\n",
      "Iteration 13, inertia 1495.132638112847\n",
      "Iteration 14, inertia 1494.8345331392622\n",
      "Iteration 15, inertia 1494.6310265282843\n",
      "Iteration 16, inertia 1494.4152699130277\n",
      "Iteration 17, inertia 1494.3127727136098\n",
      "Iteration 18, inertia 1494.1727320986247\n",
      "Iteration 19, inertia 1494.0087321376423\n",
      "Iteration 20, inertia 1493.7333493744322\n",
      "Iteration 21, inertia 1493.5389207577798\n",
      "Iteration 22, inertia 1493.465989100567\n",
      "Iteration 23, inertia 1493.4349858971261\n",
      "Iteration 24, inertia 1493.3997898480325\n",
      "Iteration 25, inertia 1493.3825219172857\n",
      "Iteration 26, inertia 1493.372206886942\n",
      "Iteration 27, inertia 1493.3478546857643\n",
      "Iteration 28, inertia 1493.313398127524\n",
      "Iteration 29, inertia 1493.2420141854655\n",
      "Iteration 30, inertia 1493.2052925575542\n",
      "Iteration 31, inertia 1493.1507063695362\n",
      "Iteration 32, inertia 1493.1446788450487\n",
      "Iteration 33, inertia 1493.1386935494797\n",
      "Iteration 34, inertia 1493.1347399016672\n",
      "Converged at iteration 34: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1987.217889811816\n",
      "Iteration 1, inertia 1593.7332619443603\n",
      "Iteration 2, inertia 1554.071724621705\n",
      "Iteration 3, inertia 1541.0897788592692\n",
      "Iteration 4, inertia 1534.1454015455754\n",
      "Iteration 5, inertia 1527.9405501079252\n",
      "Iteration 6, inertia 1520.1743614428692\n",
      "Iteration 7, inertia 1513.0885206410849\n",
      "Iteration 8, inertia 1506.487108710483\n",
      "Iteration 9, inertia 1502.3066956119321\n",
      "Iteration 10, inertia 1500.1352478483382\n",
      "Iteration 11, inertia 1499.086163486483\n",
      "Iteration 12, inertia 1498.5111793690544\n",
      "Iteration 13, inertia 1498.2341146647111\n",
      "Iteration 14, inertia 1498.0876021674096\n",
      "Iteration 15, inertia 1498.0410350262957\n",
      "Iteration 16, inertia 1497.994339166166\n",
      "Iteration 17, inertia 1497.9053980315682\n",
      "Iteration 18, inertia 1497.808465808119\n",
      "Iteration 19, inertia 1497.6728670730647\n",
      "Iteration 20, inertia 1497.5734441562897\n",
      "Iteration 21, inertia 1497.5231691133472\n",
      "Iteration 22, inertia 1497.4923302001112\n",
      "Iteration 23, inertia 1497.4770185445022\n",
      "Iteration 24, inertia 1497.4541584065482\n",
      "Iteration 25, inertia 1497.4481504187886\n",
      "Iteration 26, inertia 1497.447183498414\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1982.6058076682614\n",
      "Iteration 1, inertia 1552.1479518290803\n",
      "Iteration 2, inertia 1515.4413504171998\n",
      "Iteration 3, inertia 1499.4684455058605\n",
      "Iteration 4, inertia 1490.0743801965473\n",
      "Iteration 5, inertia 1484.615097608107\n",
      "Iteration 6, inertia 1479.920405286981\n",
      "Iteration 7, inertia 1476.5595015292554\n",
      "Iteration 8, inertia 1472.75697934955\n",
      "Iteration 9, inertia 1470.5086621741677\n",
      "Iteration 10, inertia 1469.1229991040116\n",
      "Iteration 11, inertia 1468.2575586426856\n",
      "Iteration 12, inertia 1467.4782832681074\n",
      "Iteration 13, inertia 1466.7487955159215\n",
      "Iteration 14, inertia 1466.124313180067\n",
      "Iteration 15, inertia 1465.3163047253329\n",
      "Iteration 16, inertia 1464.1122919462239\n",
      "Iteration 17, inertia 1462.8577276721217\n",
      "Iteration 18, inertia 1462.0391253132561\n",
      "Iteration 19, inertia 1461.3842085564356\n",
      "Iteration 20, inertia 1460.9422435535434\n",
      "Iteration 21, inertia 1460.6272997509027\n",
      "Iteration 22, inertia 1460.284884002719\n",
      "Iteration 23, inertia 1460.1126100623778\n",
      "Iteration 24, inertia 1459.9940039059297\n",
      "Iteration 25, inertia 1459.9365228941774\n",
      "Iteration 26, inertia 1459.9080487057174\n",
      "Iteration 27, inertia 1459.88708697999\n",
      "Iteration 28, inertia 1459.8659490636555\n",
      "Iteration 29, inertia 1459.8440904278063\n",
      "Iteration 30, inertia 1459.804809903831\n",
      "Iteration 31, inertia 1459.7858041566997\n",
      "Iteration 32, inertia 1459.7840178332162\n",
      "Converged at iteration 32: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2029.1420245211802\n",
      "Iteration 1, inertia 1573.8882350085005\n",
      "Iteration 2, inertia 1528.4086225762176\n",
      "Iteration 3, inertia 1513.805521065399\n",
      "Iteration 4, inertia 1507.6457163681675\n",
      "Iteration 5, inertia 1504.919243962039\n",
      "Iteration 6, inertia 1503.1420739188118\n",
      "Iteration 7, inertia 1501.9689935550268\n",
      "Iteration 8, inertia 1501.322977665318\n",
      "Iteration 9, inertia 1500.974306572082\n",
      "Iteration 10, inertia 1500.6994842655993\n",
      "Iteration 11, inertia 1500.5204520948735\n",
      "Iteration 12, inertia 1500.3993263355642\n",
      "Iteration 13, inertia 1500.2242314189134\n",
      "Iteration 14, inertia 1499.798569023987\n",
      "Iteration 15, inertia 1498.4138177965249\n",
      "Iteration 16, inertia 1496.5205222352697\n",
      "Iteration 17, inertia 1493.8231923577887\n",
      "Iteration 18, inertia 1493.121819957123\n",
      "Iteration 19, inertia 1493.0477379449248\n",
      "Iteration 20, inertia 1492.9817793378438\n",
      "Iteration 21, inertia 1492.8659063241294\n",
      "Iteration 22, inertia 1492.7671717159928\n",
      "Iteration 23, inertia 1492.7127396546894\n",
      "Iteration 24, inertia 1492.6645573131545\n",
      "Iteration 25, inertia 1492.6224176059923\n",
      "Iteration 26, inertia 1492.5588418192194\n",
      "Iteration 27, inertia 1492.4831479214547\n",
      "Iteration 28, inertia 1492.3725416030402\n",
      "Iteration 29, inertia 1492.176051272066\n",
      "Iteration 30, inertia 1491.8640721945208\n",
      "Iteration 31, inertia 1491.6688072964712\n",
      "Iteration 32, inertia 1491.565833465101\n",
      "Iteration 33, inertia 1491.505988944169\n",
      "Iteration 34, inertia 1491.4554139736924\n",
      "Iteration 35, inertia 1491.3725146875288\n",
      "Iteration 36, inertia 1491.3471815458963\n",
      "Iteration 37, inertia 1491.2770031446987\n",
      "Iteration 38, inertia 1491.2086553434872\n",
      "Iteration 39, inertia 1491.1598727856895\n",
      "Iteration 40, inertia 1491.1443053852797\n",
      "Converged at iteration 40: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2091.8516313183613\n",
      "Iteration 1, inertia 1586.5680603512744\n",
      "Iteration 2, inertia 1542.6275259356353\n",
      "Iteration 3, inertia 1526.8522414942433\n",
      "Iteration 4, inertia 1515.780269188078\n",
      "Iteration 5, inertia 1508.2101799049221\n",
      "Iteration 6, inertia 1502.601028985268\n",
      "Iteration 7, inertia 1495.1078162685271\n",
      "Iteration 8, inertia 1490.0935530699426\n",
      "Iteration 9, inertia 1487.6983651890305\n",
      "Iteration 10, inertia 1486.5759249424755\n",
      "Iteration 11, inertia 1486.055484508348\n",
      "Iteration 12, inertia 1485.6762885084354\n",
      "Iteration 13, inertia 1485.5084085163937\n",
      "Iteration 14, inertia 1485.4340037155393\n",
      "Iteration 15, inertia 1485.3849547297182\n",
      "Iteration 16, inertia 1485.3678871083932\n",
      "Iteration 17, inertia 1485.3654128182877\n",
      "Iteration 18, inertia 1485.3564493187955\n",
      "Iteration 19, inertia 1485.3497828084232\n",
      "Iteration 20, inertia 1485.3456847310292\n",
      "Iteration 21, inertia 1485.3382937814226\n",
      "Iteration 22, inertia 1485.3297004964386\n",
      "Converged at iteration 22: strict convergence.\n"
     ]
    }
   ],
   "source": [
    "K = range(2, 21)\n",
    "\n",
    "inertia = []\n",
    "silhouette = []\n",
    "\n",
    "for k in K:\n",
    "    print(\"Training a K-Means model with {} clusters! \".format(k))\n",
    "    print()\n",
    "    kmeans = KMeans(n_clusters=k,\n",
    "                    random_state=1234,\n",
    "                    verbose=1)\n",
    "    kmeans.fit(No_bankrupcies)\n",
    "    \n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette.append(silhouette_score(No_bankrupcies, kmeans.predict(No_bankrupcies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32facd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Silhouette Method showing the optimal k')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "ax[0].plot(K, inertia, 'bx-')\n",
    "ax[0].set_xlabel('k')\n",
    "ax[0].set_ylabel('inertia')\n",
    "ax[0].set_xticks(np.arange(min(K), max(K)+1, 1.0))\n",
    "ax[0].set_title('Elbow Method showing the optimal k')\n",
    "ax[1].plot(K, silhouette, 'bx-')\n",
    "ax[1].set_xlabel('k')\n",
    "ax[1].set_ylabel('silhouette score')\n",
    "ax[1].set_xticks(np.arange(min(K), max(K)+1, 1.0))\n",
    "ax[1].set_title('Silhouette Method showing the optimal k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bedee1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=13, random_state=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=13, random_state=1)\n",
    "kmeans.fit(No_bankrupcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a1f701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     327\n",
       "1     476\n",
       "2     621\n",
       "3     516\n",
       "4     397\n",
       "5     450\n",
       "6     306\n",
       "7     382\n",
       "8     240\n",
       "9     378\n",
       "10    385\n",
       "11    133\n",
       "12    668\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = kmeans.predict(No_bankrupcies)\n",
    "\n",
    "elem_in_cluster = pd.Series(clusters).value_counts().sort_index() # Number of values in each cluster\n",
    "elem_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d935479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_weight = []\n",
    "for j in range(len(elem_in_cluster)):\n",
    "    weight = elem_in_cluster[j]/len(No_bankrupcies)\n",
    "    clusters_weight.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a88f98c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Research and development expense rate</th>\n",
       "      <th>Interest-bearing debt interest rate</th>\n",
       "      <th>Tax rate (A)</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Net Value Growth Rate</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Total debt/Total net worth</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Contingent liabilities/Net worth</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Total Asset Turnover</th>\n",
       "      <th>Average Collection Days</th>\n",
       "      <th>Fixed Assets Turnover Frequency</th>\n",
       "      <th>Revenue per person</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Current Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Cash/Current Liability</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Current Asset Turnover Rate</th>\n",
       "      <th>Quick Asset Turnover Rate</th>\n",
       "      <th>Cash Turnover Rate</th>\n",
       "      <th>Fixed Assets to Assets</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Liability-Assets Flag</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Equity to Liability</th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>0.517184</td>\n",
       "      <td>0.595181</td>\n",
       "      <td>0.561647</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.717944e-13</td>\n",
       "      <td>0.186355</td>\n",
       "      <td>0.192954</td>\n",
       "      <td>0.192954</td>\n",
       "      <td>0.192954</td>\n",
       "      <td>0.230595</td>\n",
       "      <td>0.108135</td>\n",
       "      <td>0.192815</td>\n",
       "      <td>0.066466</td>\n",
       "      <td>5.244967e-14</td>\n",
       "      <td>5.562521e-13</td>\n",
       "      <td>3.929320e-13</td>\n",
       "      <td>0.088430</td>\n",
       "      <td>0.911570</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.371884</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.108139</td>\n",
       "      <td>0.191878</td>\n",
       "      <td>0.335832</td>\n",
       "      <td>1.909667e-13</td>\n",
       "      <td>4.482950e-14</td>\n",
       "      <td>8.952817e-12</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>0.751356</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.184907</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>1.896346e-13</td>\n",
       "      <td>0.057453</td>\n",
       "      <td>0.367288</td>\n",
       "      <td>0.603629</td>\n",
       "      <td>0.951720</td>\n",
       "      <td>0.052945</td>\n",
       "      <td>9.140000e-03</td>\n",
       "      <td>1.290000e-01</td>\n",
       "      <td>6.540000e-02</td>\n",
       "      <td>5.803651e-11</td>\n",
       "      <td>0.115561</td>\n",
       "      <td>0.642290</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.830270</td>\n",
       "      <td>5.814095e-13</td>\n",
       "      <td>0.277612</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>0.530249</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.570962</td>\n",
       "      <td>3.363384e-14</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173992</td>\n",
       "      <td>0.173992</td>\n",
       "      <td>0.173992</td>\n",
       "      <td>0.215184</td>\n",
       "      <td>0.095106</td>\n",
       "      <td>0.171349</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>4.841899e-14</td>\n",
       "      <td>3.687891e-12</td>\n",
       "      <td>1.744937e-13</td>\n",
       "      <td>0.047042</td>\n",
       "      <td>0.952958</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.095111</td>\n",
       "      <td>0.170357</td>\n",
       "      <td>0.085457</td>\n",
       "      <td>6.101105e-13</td>\n",
       "      <td>2.067306e-14</td>\n",
       "      <td>8.636050e-13</td>\n",
       "      <td>0.391895</td>\n",
       "      <td>0.921106</td>\n",
       "      <td>0.787858</td>\n",
       "      <td>0.754803</td>\n",
       "      <td>0.131380</td>\n",
       "      <td>1.698972e-12</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.376574</td>\n",
       "      <td>0.806740</td>\n",
       "      <td>0.922722</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>3.752457e-14</td>\n",
       "      <td>3.989659e-14</td>\n",
       "      <td>1.746463e-14</td>\n",
       "      <td>2.246573e-11</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.622632</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800269</td>\n",
       "      <td>3.848267e-14</td>\n",
       "      <td>0.276037</td>\n",
       "      <td>0.081620</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>0.514649</td>\n",
       "      <td>0.579427</td>\n",
       "      <td>0.564484</td>\n",
       "      <td>5.941884e-02</td>\n",
       "      <td>3.778156e-13</td>\n",
       "      <td>0.118129</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.242980</td>\n",
       "      <td>0.119941</td>\n",
       "      <td>0.195601</td>\n",
       "      <td>0.596597</td>\n",
       "      <td>4.994086e-14</td>\n",
       "      <td>9.100095e-13</td>\n",
       "      <td>6.394434e-13</td>\n",
       "      <td>0.121293</td>\n",
       "      <td>0.878707</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.119954</td>\n",
       "      <td>0.194691</td>\n",
       "      <td>0.340330</td>\n",
       "      <td>5.084839e-13</td>\n",
       "      <td>9.289247e-13</td>\n",
       "      <td>7.915490e-12</td>\n",
       "      <td>0.402535</td>\n",
       "      <td>0.849637</td>\n",
       "      <td>0.563323</td>\n",
       "      <td>0.744841</td>\n",
       "      <td>0.164218</td>\n",
       "      <td>7.561791e-13</td>\n",
       "      <td>0.119202</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.929519</td>\n",
       "      <td>0.948818</td>\n",
       "      <td>0.047951</td>\n",
       "      <td>7.470000e-01</td>\n",
       "      <td>5.870000e-01</td>\n",
       "      <td>5.090000e-01</td>\n",
       "      <td>3.423857e-12</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.602408</td>\n",
       "      <td>0.024939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.825276</td>\n",
       "      <td>2.129843e-13</td>\n",
       "      <td>0.279389</td>\n",
       "      <td>0.030795</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>0.505484</td>\n",
       "      <td>0.557894</td>\n",
       "      <td>0.548156</td>\n",
       "      <td>5.284102e-14</td>\n",
       "      <td>1.220324e-12</td>\n",
       "      <td>0.306610</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.227664</td>\n",
       "      <td>0.106995</td>\n",
       "      <td>0.184228</td>\n",
       "      <td>0.669670</td>\n",
       "      <td>4.829061e-14</td>\n",
       "      <td>2.203583e-12</td>\n",
       "      <td>2.077044e-13</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.945642</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.369760</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.106958</td>\n",
       "      <td>0.183244</td>\n",
       "      <td>0.028486</td>\n",
       "      <td>6.069511e-13</td>\n",
       "      <td>1.219181e-14</td>\n",
       "      <td>1.258729e-12</td>\n",
       "      <td>0.403903</td>\n",
       "      <td>0.826426</td>\n",
       "      <td>0.409265</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>0.221474</td>\n",
       "      <td>3.264634e-12</td>\n",
       "      <td>0.036908</td>\n",
       "      <td>0.363156</td>\n",
       "      <td>0.615800</td>\n",
       "      <td>0.938185</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>5.703645e-14</td>\n",
       "      <td>6.032370e-14</td>\n",
       "      <td>8.428286e-14</td>\n",
       "      <td>1.408309e-11</td>\n",
       "      <td>0.111261</td>\n",
       "      <td>0.606112</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.811530</td>\n",
       "      <td>5.463042e-13</td>\n",
       "      <td>0.276277</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>0.504168</td>\n",
       "      <td>0.561928</td>\n",
       "      <td>0.549708</td>\n",
       "      <td>2.615230e-01</td>\n",
       "      <td>6.293559e-13</td>\n",
       "      <td>0.216115</td>\n",
       "      <td>0.174413</td>\n",
       "      <td>0.174413</td>\n",
       "      <td>0.174413</td>\n",
       "      <td>0.223598</td>\n",
       "      <td>0.116440</td>\n",
       "      <td>0.179257</td>\n",
       "      <td>0.672673</td>\n",
       "      <td>5.010134e-14</td>\n",
       "      <td>6.159326e-13</td>\n",
       "      <td>1.008155e-12</td>\n",
       "      <td>0.154881</td>\n",
       "      <td>0.845119</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.378281</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.116453</td>\n",
       "      <td>0.178275</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>8.715488e-13</td>\n",
       "      <td>2.896500e-13</td>\n",
       "      <td>1.675194e-12</td>\n",
       "      <td>0.396373</td>\n",
       "      <td>0.844396</td>\n",
       "      <td>0.623509</td>\n",
       "      <td>0.824909</td>\n",
       "      <td>0.081422</td>\n",
       "      <td>2.988600e-13</td>\n",
       "      <td>0.149663</td>\n",
       "      <td>0.347908</td>\n",
       "      <td>0.920141</td>\n",
       "      <td>0.935566</td>\n",
       "      <td>0.068622</td>\n",
       "      <td>1.556919e-14</td>\n",
       "      <td>1.202105e-14</td>\n",
       "      <td>4.340000e-01</td>\n",
       "      <td>5.522568e-12</td>\n",
       "      <td>0.112961</td>\n",
       "      <td>0.600923</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808609</td>\n",
       "      <td>3.058725e-14</td>\n",
       "      <td>0.282046</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.609454</td>\n",
       "      <td>1.022044e-02</td>\n",
       "      <td>3.616523e-13</td>\n",
       "      <td>0.189898</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.247424</td>\n",
       "      <td>0.110170</td>\n",
       "      <td>0.219703</td>\n",
       "      <td>0.955956</td>\n",
       "      <td>8.881406e-14</td>\n",
       "      <td>8.021385e-13</td>\n",
       "      <td>3.743083e-13</td>\n",
       "      <td>0.085437</td>\n",
       "      <td>0.914563</td>\n",
       "      <td>0.006786</td>\n",
       "      <td>0.372422</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.110167</td>\n",
       "      <td>0.201259</td>\n",
       "      <td>0.151424</td>\n",
       "      <td>6.706652e-13</td>\n",
       "      <td>6.421522e-14</td>\n",
       "      <td>1.516244e-11</td>\n",
       "      <td>0.436184</td>\n",
       "      <td>0.816919</td>\n",
       "      <td>0.298744</td>\n",
       "      <td>0.463892</td>\n",
       "      <td>0.036543</td>\n",
       "      <td>2.873165e-13</td>\n",
       "      <td>0.069614</td>\n",
       "      <td>0.328416</td>\n",
       "      <td>0.758802</td>\n",
       "      <td>0.946399</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>1.355168e-14</td>\n",
       "      <td>8.850000e-01</td>\n",
       "      <td>3.030000e-01</td>\n",
       "      <td>1.216929e-11</td>\n",
       "      <td>0.112060</td>\n",
       "      <td>0.530002</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841345</td>\n",
       "      <td>2.700800e-13</td>\n",
       "      <td>0.277477</td>\n",
       "      <td>0.044851</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.506752</td>\n",
       "      <td>0.563399</td>\n",
       "      <td>0.563360</td>\n",
       "      <td>5.320641e-01</td>\n",
       "      <td>1.505201e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179133</td>\n",
       "      <td>0.179133</td>\n",
       "      <td>0.179133</td>\n",
       "      <td>0.223315</td>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.176094</td>\n",
       "      <td>0.667668</td>\n",
       "      <td>4.936046e-14</td>\n",
       "      <td>2.341531e-12</td>\n",
       "      <td>9.027436e-14</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>0.973668</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.369654</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.096316</td>\n",
       "      <td>0.174391</td>\n",
       "      <td>0.058471</td>\n",
       "      <td>5.886970e-13</td>\n",
       "      <td>1.014722e-14</td>\n",
       "      <td>8.309281e-13</td>\n",
       "      <td>0.392777</td>\n",
       "      <td>0.802107</td>\n",
       "      <td>0.223893</td>\n",
       "      <td>0.241770</td>\n",
       "      <td>0.041121</td>\n",
       "      <td>1.225378e-12</td>\n",
       "      <td>0.018016</td>\n",
       "      <td>0.374684</td>\n",
       "      <td>0.581476</td>\n",
       "      <td>0.937618</td>\n",
       "      <td>0.011049</td>\n",
       "      <td>1.663976e-14</td>\n",
       "      <td>1.595049e-14</td>\n",
       "      <td>7.780000e-01</td>\n",
       "      <td>3.389565e-11</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.597414</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814818</td>\n",
       "      <td>2.548259e-13</td>\n",
       "      <td>0.275430</td>\n",
       "      <td>0.139579</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510</th>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.537942</td>\n",
       "      <td>0.546228</td>\n",
       "      <td>8.847695e-02</td>\n",
       "      <td>2.283057e-13</td>\n",
       "      <td>0.260350</td>\n",
       "      <td>0.191901</td>\n",
       "      <td>0.191901</td>\n",
       "      <td>0.191901</td>\n",
       "      <td>0.216791</td>\n",
       "      <td>0.099910</td>\n",
       "      <td>0.172855</td>\n",
       "      <td>0.669670</td>\n",
       "      <td>4.778510e-14</td>\n",
       "      <td>8.182245e-13</td>\n",
       "      <td>5.352242e-13</td>\n",
       "      <td>0.108746</td>\n",
       "      <td>0.891254</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.371420</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.099818</td>\n",
       "      <td>0.171872</td>\n",
       "      <td>0.122939</td>\n",
       "      <td>6.901480e-13</td>\n",
       "      <td>2.279338e-14</td>\n",
       "      <td>1.031658e-12</td>\n",
       "      <td>0.392924</td>\n",
       "      <td>0.799142</td>\n",
       "      <td>0.317485</td>\n",
       "      <td>0.417248</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>6.955153e-14</td>\n",
       "      <td>0.075222</td>\n",
       "      <td>0.346042</td>\n",
       "      <td>0.648311</td>\n",
       "      <td>0.939319</td>\n",
       "      <td>0.010702</td>\n",
       "      <td>1.312155e-14</td>\n",
       "      <td>1.026691e-14</td>\n",
       "      <td>8.550000e-02</td>\n",
       "      <td>3.127477e-11</td>\n",
       "      <td>0.115051</td>\n",
       "      <td>0.583741</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799665</td>\n",
       "      <td>1.167399e-12</td>\n",
       "      <td>0.278637</td>\n",
       "      <td>0.034702</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>0.493004</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>1.583166e-01</td>\n",
       "      <td>4.293359e-13</td>\n",
       "      <td>0.220873</td>\n",
       "      <td>0.193081</td>\n",
       "      <td>0.193081</td>\n",
       "      <td>0.193081</td>\n",
       "      <td>0.226057</td>\n",
       "      <td>0.108949</td>\n",
       "      <td>0.186639</td>\n",
       "      <td>0.741742</td>\n",
       "      <td>5.062022e-14</td>\n",
       "      <td>1.112301e-12</td>\n",
       "      <td>9.247618e-13</td>\n",
       "      <td>0.148442</td>\n",
       "      <td>0.851558</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.371939</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.108953</td>\n",
       "      <td>0.185687</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>6.164293e-13</td>\n",
       "      <td>1.119223e-13</td>\n",
       "      <td>9.356277e-12</td>\n",
       "      <td>0.399471</td>\n",
       "      <td>0.859163</td>\n",
       "      <td>0.740799</td>\n",
       "      <td>0.823791</td>\n",
       "      <td>0.164228</td>\n",
       "      <td>6.792483e-13</td>\n",
       "      <td>0.132765</td>\n",
       "      <td>0.332666</td>\n",
       "      <td>0.849657</td>\n",
       "      <td>0.937350</td>\n",
       "      <td>0.026386</td>\n",
       "      <td>9.050000e-03</td>\n",
       "      <td>8.570000e-01</td>\n",
       "      <td>5.420000e-01</td>\n",
       "      <td>1.903661e-11</td>\n",
       "      <td>0.113265</td>\n",
       "      <td>0.526209</td>\n",
       "      <td>0.025129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813044</td>\n",
       "      <td>1.583772e-13</td>\n",
       "      <td>0.281446</td>\n",
       "      <td>0.024543</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>0.504558</td>\n",
       "      <td>0.558820</td>\n",
       "      <td>0.549708</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.657231e-13</td>\n",
       "      <td>0.237372</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0.222558</td>\n",
       "      <td>0.101050</td>\n",
       "      <td>0.178128</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>4.679281e-14</td>\n",
       "      <td>4.775928e-12</td>\n",
       "      <td>5.073346e-14</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>0.984612</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.370001</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>0.101015</td>\n",
       "      <td>0.177174</td>\n",
       "      <td>0.050975</td>\n",
       "      <td>7.436819e-13</td>\n",
       "      <td>1.991581e-14</td>\n",
       "      <td>2.242039e-12</td>\n",
       "      <td>0.401574</td>\n",
       "      <td>0.876058</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.508408</td>\n",
       "      <td>0.246222</td>\n",
       "      <td>7.770864e-12</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.388888</td>\n",
       "      <td>0.859788</td>\n",
       "      <td>0.946818</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>3.944611e-14</td>\n",
       "      <td>3.276767e-14</td>\n",
       "      <td>5.075770e-14</td>\n",
       "      <td>1.733027e-11</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.594555</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812539</td>\n",
       "      <td>5.690372e-13</td>\n",
       "      <td>0.275145</td>\n",
       "      <td>0.219073</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5279 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "2542                                           0.517184          \n",
       "1809                                           0.530249          \n",
       "4322                                           0.514649          \n",
       "3933                                           0.505484          \n",
       "1784                                           0.504168          \n",
       "...                                                 ...          \n",
       "1337                                           0.569688          \n",
       "406                                            0.506752          \n",
       "5510                                           0.493151          \n",
       "2191                                           0.493004          \n",
       "2671                                           0.504558          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "2542                                 0.595181   \n",
       "1809                                 0.538432   \n",
       "4322                                 0.579427   \n",
       "3933                                 0.557894   \n",
       "1784                                 0.561928   \n",
       "...                                       ...   \n",
       "1337                                 0.625600   \n",
       "406                                  0.563399   \n",
       "5510                                 0.537942   \n",
       "2191                                 0.562800   \n",
       "2671                                 0.558820   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "2542                                           0.561647    \n",
       "1809                                           0.570962    \n",
       "4322                                           0.564484    \n",
       "3933                                           0.548156    \n",
       "1784                                           0.549708    \n",
       "...                                                 ...    \n",
       "1337                                           0.609454    \n",
       "406                                            0.563360    \n",
       "5510                                           0.546228    \n",
       "2191                                           0.539804    \n",
       "2671                                           0.549708    \n",
       "\n",
       "       Research and development expense rate  \\\n",
       "2542                            0.000000e+00   \n",
       "1809                            3.363384e-14   \n",
       "4322                            5.941884e-02   \n",
       "3933                            5.284102e-14   \n",
       "1784                            2.615230e-01   \n",
       "...                                      ...   \n",
       "1337                            1.022044e-02   \n",
       "406                             5.320641e-01   \n",
       "5510                            8.847695e-02   \n",
       "2191                            1.583166e-01   \n",
       "2671                            0.000000e+00   \n",
       "\n",
       "       Interest-bearing debt interest rate   Tax rate (A)  \\\n",
       "2542                          7.717944e-13       0.186355   \n",
       "1809                          0.000000e+00       0.000000   \n",
       "4322                          3.778156e-13       0.118129   \n",
       "3933                          1.220324e-12       0.306610   \n",
       "1784                          6.293559e-13       0.216115   \n",
       "...                                    ...            ...   \n",
       "1337                          3.616523e-13       0.189898   \n",
       "406                           1.505201e-13       0.000000   \n",
       "5510                          2.283057e-13       0.260350   \n",
       "2191                          4.293359e-13       0.220873   \n",
       "2671                          6.657231e-13       0.237372   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "2542                  0.192954                  0.192954   \n",
       "1809                  0.173992                  0.173992   \n",
       "4322                  0.206565                  0.206565   \n",
       "3933                  0.217564                  0.217564   \n",
       "1784                  0.174413                  0.174413   \n",
       "...                        ...                       ...   \n",
       "1337                  0.198053                  0.198053   \n",
       "406                   0.179133                  0.179133   \n",
       "5510                  0.191901                  0.191901   \n",
       "2191                  0.193081                  0.193081   \n",
       "2671                  0.188825                  0.188825   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "2542                  0.192954                                  0.230595   \n",
       "1809                  0.173992                                  0.215184   \n",
       "4322                  0.206565                                  0.242980   \n",
       "3933                  0.217564                                  0.227664   \n",
       "1784                  0.174413                                  0.223598   \n",
       "...                        ...                                       ...   \n",
       "1337                  0.198053                                  0.247424   \n",
       "406                   0.179133                                  0.223315   \n",
       "5510                  0.191901                                  0.216791   \n",
       "2191                  0.193081                                  0.226057   \n",
       "2671                  0.188825                                  0.222558   \n",
       "\n",
       "       Operating Profit Per Share (Yuan ¥)  \\\n",
       "2542                              0.108135   \n",
       "1809                              0.095106   \n",
       "4322                              0.119941   \n",
       "3933                              0.106995   \n",
       "1784                              0.116440   \n",
       "...                                    ...   \n",
       "1337                              0.110170   \n",
       "406                               0.096491   \n",
       "5510                              0.099910   \n",
       "2191                              0.108949   \n",
       "2671                              0.101050   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)   Total Asset Growth Rate  \\\n",
       "2542                                   0.192815                  0.066466   \n",
       "1809                                   0.171349                  0.720721   \n",
       "4322                                   0.195601                  0.596597   \n",
       "3933                                   0.184228                  0.669670   \n",
       "1784                                   0.179257                  0.672673   \n",
       "...                                         ...                       ...   \n",
       "1337                                   0.219703                  0.955956   \n",
       "406                                    0.176094                  0.667668   \n",
       "5510                                   0.172855                  0.669670   \n",
       "2191                                   0.186639                  0.741742   \n",
       "2671                                   0.178128                  0.006076   \n",
       "\n",
       "       Net Value Growth Rate   Quick Ratio   Total debt/Total net worth  \\\n",
       "2542            5.244967e-14  5.562521e-13                 3.929320e-13   \n",
       "1809            4.841899e-14  3.687891e-12                 1.744937e-13   \n",
       "4322            4.994086e-14  9.100095e-13                 6.394434e-13   \n",
       "3933            4.829061e-14  2.203583e-12                 2.077044e-13   \n",
       "1784            5.010134e-14  6.159326e-13                 1.008155e-12   \n",
       "...                      ...           ...                          ...   \n",
       "1337            8.881406e-14  8.021385e-13                 3.743083e-13   \n",
       "406             4.936046e-14  2.341531e-12                 9.027436e-14   \n",
       "5510            4.778510e-14  8.182245e-13                 5.352242e-13   \n",
       "2191            5.062022e-14  1.112301e-12                 9.247618e-13   \n",
       "2671            4.679281e-14  4.775928e-12                 5.073346e-14   \n",
       "\n",
       "       Debt ratio %   Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "2542       0.088430           0.911570                               0.005192   \n",
       "1809       0.047042           0.952958                               0.005991   \n",
       "4322       0.121293           0.878707                               0.010607   \n",
       "3933       0.054358           0.945642                               0.006713   \n",
       "1784       0.154881           0.845119                               0.007876   \n",
       "...             ...                ...                                    ...   \n",
       "1337       0.085437           0.914563                               0.006786   \n",
       "406        0.026332           0.973668                               0.005631   \n",
       "5510       0.108746           0.891254                               0.005502   \n",
       "2191       0.148442           0.851558                               0.005683   \n",
       "2671       0.015388           0.984612                               0.006578   \n",
       "\n",
       "       Borrowing dependency   Contingent liabilities/Net worth  \\\n",
       "2542               0.371884                           0.007290   \n",
       "1809               0.369637                           0.005366   \n",
       "4322               0.369637                           0.005754   \n",
       "3933               0.369760                           0.005587   \n",
       "1784               0.378281                           0.005366   \n",
       "...                     ...                                ...   \n",
       "1337               0.372422                           0.007237   \n",
       "406                0.369654                           0.006165   \n",
       "5510               0.371420                           0.006567   \n",
       "2191               0.371939                           0.005752   \n",
       "2671               0.370001                           0.005582   \n",
       "\n",
       "       Operating profit/Paid-in capital  \\\n",
       "2542                           0.108139   \n",
       "1809                           0.095111   \n",
       "4322                           0.119954   \n",
       "3933                           0.106958   \n",
       "1784                           0.116453   \n",
       "...                                 ...   \n",
       "1337                           0.110167   \n",
       "406                            0.096316   \n",
       "5510                           0.099818   \n",
       "2191                           0.108953   \n",
       "2671                           0.101015   \n",
       "\n",
       "       Net profit before tax/Paid-in capital   Total Asset Turnover  \\\n",
       "2542                                0.191878               0.335832   \n",
       "1809                                0.170357               0.085457   \n",
       "4322                                0.194691               0.340330   \n",
       "3933                                0.183244               0.028486   \n",
       "1784                                0.178275               0.206897   \n",
       "...                                      ...                    ...   \n",
       "1337                                0.201259               0.151424   \n",
       "406                                 0.174391               0.058471   \n",
       "5510                                0.171872               0.122939   \n",
       "2191                                0.185687               0.347826   \n",
       "2671                                0.177174               0.050975   \n",
       "\n",
       "       Average Collection Days   Fixed Assets Turnover Frequency  \\\n",
       "2542              1.909667e-13                      4.482950e-14   \n",
       "1809              6.101105e-13                      2.067306e-14   \n",
       "4322              5.084839e-13                      9.289247e-13   \n",
       "3933              6.069511e-13                      1.219181e-14   \n",
       "1784              8.715488e-13                      2.896500e-13   \n",
       "...                        ...                               ...   \n",
       "1337              6.706652e-13                      6.421522e-14   \n",
       "406               5.886970e-13                      1.014722e-14   \n",
       "5510              6.901480e-13                      2.279338e-14   \n",
       "2191              6.164293e-13                      1.119223e-13   \n",
       "2671              7.436819e-13                      1.991581e-14   \n",
       "\n",
       "       Revenue per person   Operating profit per person  \\\n",
       "2542         8.952817e-12                      0.401404   \n",
       "1809         8.636050e-13                      0.391895   \n",
       "4322         7.915490e-12                      0.402535   \n",
       "3933         1.258729e-12                      0.403903   \n",
       "1784         1.675194e-12                      0.396373   \n",
       "...                   ...                           ...   \n",
       "1337         1.516244e-11                      0.436184   \n",
       "406          8.309281e-13                      0.392777   \n",
       "5510         1.031658e-12                      0.392924   \n",
       "2191         9.356277e-12                      0.399471   \n",
       "2671         2.242039e-12                      0.401574   \n",
       "\n",
       "       Working Capital to Total Assets   Quick Assets/Total Assets  \\\n",
       "2542                          0.751356                    0.171034   \n",
       "1809                          0.921106                    0.787858   \n",
       "4322                          0.849637                    0.563323   \n",
       "3933                          0.826426                    0.409265   \n",
       "1784                          0.844396                    0.623509   \n",
       "...                                ...                         ...   \n",
       "1337                          0.816919                    0.298744   \n",
       "406                           0.802107                    0.223893   \n",
       "5510                          0.799142                    0.317485   \n",
       "2191                          0.859163                    0.740799   \n",
       "2671                          0.876058                    0.415983   \n",
       "\n",
       "       Current Assets/Total Assets   Cash/Total Assets  \\\n",
       "2542                      0.184907            0.019934   \n",
       "1809                      0.754803            0.131380   \n",
       "4322                      0.744841            0.164218   \n",
       "3933                      0.392087            0.221474   \n",
       "1784                      0.824909            0.081422   \n",
       "...                            ...                 ...   \n",
       "1337                      0.463892            0.036543   \n",
       "406                       0.241770            0.041121   \n",
       "5510                      0.417248            0.009554   \n",
       "2191                      0.823791            0.164228   \n",
       "2671                      0.508408            0.246222   \n",
       "\n",
       "       Cash/Current Liability   Current Liability to Assets  \\\n",
       "2542             1.896346e-13                      0.057453   \n",
       "1809             1.698972e-12                      0.042138   \n",
       "4322             7.561791e-13                      0.119202   \n",
       "3933             3.264634e-12                      0.036908   \n",
       "1784             2.988600e-13                      0.149663   \n",
       "...                       ...                           ...   \n",
       "1337             2.873165e-13                      0.069614   \n",
       "406              1.225378e-12                      0.018016   \n",
       "5510             6.955153e-14                      0.075222   \n",
       "2191             6.792483e-13                      0.132765   \n",
       "2671             7.770864e-12                      0.016984   \n",
       "\n",
       "       Operating Funds to Liability   Current Liabilities/Liability  \\\n",
       "2542                       0.367288                        0.603629   \n",
       "1809                       0.376574                        0.806740   \n",
       "4322                       0.350225                        0.929519   \n",
       "3933                       0.363156                        0.615800   \n",
       "1784                       0.347908                        0.920141   \n",
       "...                             ...                             ...   \n",
       "1337                       0.328416                        0.758802   \n",
       "406                        0.374684                        0.581476   \n",
       "5510                       0.346042                        0.648311   \n",
       "2191                       0.332666                        0.849657   \n",
       "2671                       0.388888                        0.859788   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total expense/Assets  \\\n",
       "2542                            0.951720               0.052945   \n",
       "1809                            0.922722               0.032573   \n",
       "4322                            0.948818               0.047951   \n",
       "3933                            0.938185               0.029146   \n",
       "1784                            0.935566               0.068622   \n",
       "...                                  ...                    ...   \n",
       "1337                            0.946399               0.008681   \n",
       "406                             0.937618               0.011049   \n",
       "5510                            0.939319               0.010702   \n",
       "2191                            0.937350               0.026386   \n",
       "2671                            0.946818               0.010999   \n",
       "\n",
       "       Current Asset Turnover Rate   Quick Asset Turnover Rate  \\\n",
       "2542                  9.140000e-03                1.290000e-01   \n",
       "1809                  3.752457e-14                3.989659e-14   \n",
       "4322                  7.470000e-01                5.870000e-01   \n",
       "3933                  5.703645e-14                6.032370e-14   \n",
       "1784                  1.556919e-14                1.202105e-14   \n",
       "...                            ...                         ...   \n",
       "1337                  1.355168e-14                8.850000e-01   \n",
       "406                   1.663976e-14                1.595049e-14   \n",
       "5510                  1.312155e-14                1.026691e-14   \n",
       "2191                  9.050000e-03                8.570000e-01   \n",
       "2671                  3.944611e-14                3.276767e-14   \n",
       "\n",
       "       Cash Turnover Rate   Fixed Assets to Assets  \\\n",
       "2542         6.540000e-02             5.803651e-11   \n",
       "1809         1.746463e-14             2.246573e-11   \n",
       "4322         5.090000e-01             3.423857e-12   \n",
       "3933         8.428286e-14             1.408309e-11   \n",
       "1784         4.340000e-01             5.522568e-12   \n",
       "...                   ...                      ...   \n",
       "1337         3.030000e-01             1.216929e-11   \n",
       "406          7.780000e-01             3.389565e-11   \n",
       "5510         8.550000e-02             3.127477e-11   \n",
       "2191         5.420000e-01             1.903661e-11   \n",
       "2671         5.075770e-14             1.733027e-11   \n",
       "\n",
       "       Equity to Long-term Liability   CFO to Assets  \\\n",
       "2542                        0.115561        0.642290   \n",
       "1809                        0.110933        0.622632   \n",
       "4322                        0.110933        0.602408   \n",
       "3933                        0.111261        0.606112   \n",
       "1784                        0.112961        0.600923   \n",
       "...                              ...             ...   \n",
       "1337                        0.112060        0.530002   \n",
       "406                         0.110933        0.597414   \n",
       "5510                        0.115051        0.583741   \n",
       "2191                        0.113265        0.526209   \n",
       "2671                        0.110933        0.594555   \n",
       "\n",
       "       Current Liability to Current Assets   Liability-Assets Flag  \\\n",
       "2542                              0.047557                     0.0   \n",
       "1809                              0.008567                     0.0   \n",
       "4322                              0.024939                     0.0   \n",
       "3933                              0.014551                     0.0   \n",
       "1784                              0.028316                     0.0   \n",
       "...                                    ...                     ...   \n",
       "1337                              0.023299                     0.0   \n",
       "406                               0.011483                     0.0   \n",
       "5510                              0.027990                     0.0   \n",
       "2191                              0.025129                     0.0   \n",
       "2671                              0.005068                     0.0   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price  \\\n",
       "2542                     0.830270                5.814095e-13   \n",
       "1809                     0.800269                3.848267e-14   \n",
       "4322                     0.825276                2.129843e-13   \n",
       "3933                     0.811530                5.463042e-13   \n",
       "1784                     0.808609                3.058725e-14   \n",
       "...                           ...                         ...   \n",
       "1337                     0.841345                2.700800e-13   \n",
       "406                      0.814818                2.548259e-13   \n",
       "5510                     0.799665                1.167399e-12   \n",
       "2191                     0.813044                1.583772e-13   \n",
       "2671                     0.812539                5.690372e-13   \n",
       "\n",
       "       Liability to Equity   Equity to Liability  Bankrupt?  cluster  \n",
       "2542              0.277612              0.043263          0        6  \n",
       "1809              0.276037              0.081620          0        0  \n",
       "4322              0.279389              0.030795          0        9  \n",
       "3933              0.276277              0.070907          0        5  \n",
       "1784              0.282046              0.023375          0        3  \n",
       "...                    ...                   ...        ...      ...  \n",
       "1337              0.277477              0.044851          0        2  \n",
       "406               0.275430              0.139579          0        8  \n",
       "5510              0.278637              0.034702          0        5  \n",
       "2191              0.281446              0.024543          0        2  \n",
       "2671              0.275145              0.219073          0        6  \n",
       "\n",
       "[5279 rows x 52 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "No_bankrupcies[\"cluster\"] = clusters\n",
    "No_bankrupcies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f0592c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_No_bankrupcies = pd.DataFrame(columns=No_bankrupcies.columns, dtype=float)\n",
    " \n",
    "for cluster, weight in enumerate(clusters_weight):\n",
    "    new_No_bankrupcies = pd.concat([new_No_bankrupcies, No_bankrupcies[No_bankrupcies[\"cluster\"]==cluster].sample(round(400*weight))], axis=0)\n",
    "    \n",
    "new_No_bankrupcies.drop(columns=['cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "073490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "final1_df = pd.concat([new_No_bankrupcies, upsample], axis=0)\n",
    "final1_df = final1_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21d0641b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 51)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e14d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = final1_df['Bankrupt?'].astype('int')\n",
    "X_train = final1_df.drop(['Bankrupt?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada9393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b661c99d",
   "metadata": {},
   "source": [
    "# Predicting with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "604cad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model n_neighbors=2 is: 0.98\n",
      "The kappa of the model n_neighbors=2 is: 0.96\n",
      "The accuracy of the model n_neighbors=5 is: 0.89\n",
      "The kappa of the model n_neighbors=5 is: 0.77\n",
      "The accuracy of the model n_neighbors=8 is: 0.86\n",
      "The kappa of the model n_neighbors=8 is: 0.73\n",
      "The accuracy of the model n_neighbors=11 is: 0.83\n",
      "The kappa of the model n_neighbors=11 is: 0.67\n"
     ]
    }
   ],
   "source": [
    "# entrenar distintos modelos con distintos valores de k\n",
    "K = range(2, 14, 3)\n",
    "accuracies = []\n",
    "models = []\n",
    "\n",
    "for k in K:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights = 'uniform') \n",
    "    knn.fit(X_train, y_train)\n",
    "    models.append(knn)\n",
    "    ypred_train = knn.predict(X_train)\n",
    "    accuracies.append(accuracy_score(y_train, ypred_train))\n",
    "    print(\"The accuracy of the model n_neighbors={} is: {:.2f}\".format(k, accuracy_score(y_train, ypred_train)))\n",
    "    print(\"The kappa of the model n_neighbors={} is: {:.2f}\".format(k, cohen_kappa_score(y_train, ypred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "278eebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model n_neighbors=2 is: 0.88\n",
      "The kappa score of the model n_neighbors=2 is: 0.22\n",
      "The accuracy of the model n_neighbors=5 is: 0.74\n",
      "The kappa score of the model n_neighbors=5 is: 0.13\n",
      "The accuracy of the model n_neighbors=8 is: 0.80\n",
      "The kappa score of the model n_neighbors=8 is: 0.15\n",
      "The accuracy of the model n_neighbors=11 is: 0.74\n",
      "The kappa score of the model n_neighbors=11 is: 0.12\n"
     ]
    }
   ],
   "source": [
    "K = range(2, 14, 3)\n",
    "#accuracies = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    #knn = KNeighborsClassifier(n_neighbors=k) \n",
    "    knn = models[i]\n",
    "    ypred_test = knn.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, ypred_test))\n",
    "    print(\"The accuracy of the model n_neighbors={} is: {:.2f}\".format(list(K)[i], accuracy_score(y_test, ypred_test)))\n",
    "    print(\"The kappa score of the model n_neighbors={} is: {:.2f}\".format(list(K)[i],cohen_kappa_score(y_test, ypred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80f961f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2) \n",
    "knn.fit(X_train, y_train)\n",
    "ypred_train = knn.predict(X_train)\n",
    "ypred_test = knn.predict(X_test)\n",
    "display(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9aa8ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[400,  17],\n",
       "       [  0, 383]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(confusion_matrix(ypred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0ad5f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1164,   14],\n",
       "       [ 156,   30]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(confusion_matrix(ypred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28443f5",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8353638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the Random Forest in the TRAIN set is 0.93\n",
      "The accuracy for the Random Forest in the TEST set is 0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    435\n",
       "0    365\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[353,  47],\n",
       "       [ 12, 388]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1123\n",
       "1     241\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1120,  200],\n",
       "       [   3,   41]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2466179815635101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=4,\n",
    "                             min_samples_split=6,\n",
    "                             min_samples_leaf =3,\n",
    "                             max_samples=0.8, random_state=8)\n",
    "                            \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"The accuracy for the Random Forest in the TRAIN set is {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"The accuracy for the Random Forest in the TEST set is {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18a517",
   "metadata": {},
   "source": [
    "### Grid Search-Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2b0bb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8400000000000001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "max_depth_choices= [2, 3, 5,7,9] \n",
    "min_samples_split_choices = [2,4,6,8,9]  \n",
    "min_samples_leaf_choices = [1,3] \n",
    "max_samples=[0.8,0.5]\n",
    "#n_jobs = [-1]\n",
    "\n",
    "grid = {'max_depth': max_depth_choices,\n",
    "        'min_samples_split': min_samples_split_choices,\n",
    "        'min_samples_leaf': min_samples_leaf_choices,\n",
    "        'max_samples':max_samples}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = grid, cv = 5) \n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96fef021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5,\n",
       " 'max_samples': 0.5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 6}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "767875ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the Random Forest in the TRAIN set is 0.94\n",
      "The accuracy for the Random Forest in the TEST set is 0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    436\n",
       "0    364\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[357,  43],\n",
       "       [  7, 393]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1128\n",
       "1     236\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1126,  194],\n",
       "       [   2,   42]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.25974658869395706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(**grid_search.best_params_, random_state =8)\n",
    "                          \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"The accuracy for the Random Forest in the TRAIN set is {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"The accuracy for the Random Forest in the TEST set is {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdeb88",
   "metadata": {},
   "source": [
    "### Grid Search-Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14ca0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_kap = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "grid1 = GridSearchCV(estimator=clf, param_grid=grid, scoring=coh_kap, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddd56256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1.fit(X_train, y_train)\n",
    "grid1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ed4f719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7,\n",
       " 'max_samples': 0.8,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 9}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed71c8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the Random Forest in the TRAIN set is 0.97\n",
      "The accuracy for the Random Forest in the TEST set is 0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    416\n",
       "0    384\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[380,  20],\n",
       "       [  4, 396]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1150\n",
       "1     214\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1147,  173],\n",
       "       [   3,   41]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2792602377807133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(**grid1.best_params_, random_state =8)\n",
    "                          \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"The accuracy for the Random Forest in the TRAIN set is {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"The accuracy for the Random Forest in the TEST set is {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3913b",
   "metadata": {},
   "source": [
    "## Feature Extraction-SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12509063",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e5551a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.dependence_plot(' Persistent EPS in the Last Four Seasons', shap_values[0], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X)\n",
    "plt.savefig(\"Kbest50RandomForestSHAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60332121",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da8614df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f2ac396",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cl = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdb823d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73d7ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the XGB in the TRAIN set is 1.00\n",
      "The accuracy for the XGB in the TEST set is 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    400\n",
       "1    400\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[400,   0],\n",
       "       [  0, 400]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1198\n",
       "1     166\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1194,  126],\n",
       "       [   4,   40]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.34768533505988997"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The accuracy for the XGB in the TRAIN set is {:.2f}\".format(xgb_cl.score(X_train, y_train)))\n",
    "print(\"The accuracy for the XGB in the TEST set is {:.2f}\".format(xgb_cl.score(X_test, y_test)))\n",
    "\n",
    "y_pred = pd.Series(xgb_cl.predict(X_train))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = pd.Series(xgb_cl.predict(X_test))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b27be1",
   "metadata": {},
   "source": [
    "Learning_rate: also called eta, it specifies how quickly the model fits the residual errors by using additional base learners.typical values: 0.01–0.2\n",
    "\n",
    "Gamma, reg_alpha, reg_lambda: these 3 parameters specify the values for 3 types of regularization done by XGBoost - minimum loss reduction to create a new split, L1 reg on leaf weights, L2 reg leaf weights respectively.Typical values for gamma: 0 - 0.5 but highly dependent on the data. Typical values for reg_alpha and reg_lambda: 0 - 1 is a good starting point but again, depends on the data.\n",
    "\n",
    "Max_depth - how deep the tree's decision nodes can go. Must be a positive integer. typical values: 1–10\n",
    "\n",
    "Subsample - fraction of the training set that can be used to train each tree. If this value is low, it may lead to underfitting or if it is too high, it may lead to overfitting. typical values: 0.5–0.9\n",
    "\n",
    "Colsample_bytree- fraction of the features that can be used to train each tree. A large value means almost all features can be used to build the decision tree. typical values: 0.5–0.9\n",
    "\n",
    "The above are the main hyperparameters people often tune. It is perfectly OK if you don’t understand them all completely (like me) but you can refer to this post which gives a thorough overview of how each of the above parameters works and how to tune them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1df97f",
   "metadata": {},
   "source": [
    "### Grid Search-Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.2],\n",
    "    \"gamma\": [0, 0.5, 1],\n",
    "    \"reg_lambda\": [0, 0.5, 1, 10],\n",
    "    \"scale_pos_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.5],\n",
    "    \"reg_alpha\": [0, 0.5, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c051e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "# Init Grid Search\n",
    "grid_cv = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring=\"roc_auc\")\n",
    "\n",
    "# Fit\n",
    "_ = grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b171da",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f841bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cbc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cl = xgb.XGBClassifier(\n",
    "    **grid_cv.best_params_,\n",
    "    objective=\"binary:logistic\")\n",
    "_ = final_cl.fit(X_train, y_train)\n",
    "preds = final_cl.predict(X_test)\n",
    "\n",
    "print(\"The accuracy for the XGB in the TRAIN set is {:.2f}\".format(final_cl.score(X_train, y_train)))\n",
    "print(\"The accuracy for the XGB in the TEST set is {:.2f}\".format(final_cl.score(X_test, y_test)))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_train))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_test))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc62370",
   "metadata": {},
   "source": [
    "### Grid Search-Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1f4b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_kap = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "grid2 = GridSearchCV(estimator=xgb_cl, param_grid=grid, scoring=coh_kap, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b6d7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:24:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:25:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6625"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.fit(X_train, y_train)\n",
    "grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b58531a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9,\n",
       " 'max_samples': 0.8,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d24254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "The accuracy for the XGB in the TRAIN set is 1.00\n",
      "The accuracy for the XGB in the TEST set is 0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    400\n",
       "1    400\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[400,   0],\n",
       "       [  0, 400]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1205\n",
       "1     159\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1201,  119],\n",
       "       [   4,   40]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3618410041841005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_cl = xgb.XGBClassifier(\n",
    "    **grid2.best_params_,\n",
    "    objective=\"binary:logistic\")\n",
    "_ = final_cl.fit(X_train, y_train)\n",
    "preds = final_cl.predict(X_test)\n",
    "\n",
    "print(\"The accuracy for the XGB in the TRAIN set is {:.2f}\".format(final_cl.score(X_train, y_train)))\n",
    "print(\"The accuracy for the XGB in the TEST set is {:.2f}\".format(final_cl.score(X_test, y_test)))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_train))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_test))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d132c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6e126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e025f688",
   "metadata": {},
   "source": [
    "## Feature extraction-SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0862ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(final_cl)\n",
    "shap_values = explainer.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470aa899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35438ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.dependence_plot( ' Quick Ratio', shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35107792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x684 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values, X)\n",
    "plt.savefig(\"Kbest50XGBSHAPimpact on model output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d19bf",
   "metadata": {},
   "source": [
    "# K-fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(shape=(4, 12))\n",
    "\n",
    "pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos CV-10 a todos los modelos para tner una mejor estimacion del accuracy y menor varianza\n",
    "nfolds = 10\n",
    "models = [knn, clf, xgb_cl]\n",
    "cv_scores = np.zeros((len(models), nfolds))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    scores = cross_val_score(model, X, y, scoring='roc', cv=nfolds)\n",
    "    cv_scores[i] = scores\n",
    "    \n",
    "cv_df = pd.DataFrame(cv_scores, index=[models])\n",
    "cv_df['mean_score'] = cv_df.mean(1)\n",
    "cv_df['std_score'] = cv_df.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea41b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
