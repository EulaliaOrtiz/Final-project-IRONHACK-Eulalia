{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f53b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Javascript\n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e65af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bec8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b74375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88bed74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>Continuous interest rate (after tax)</th>\n",
       "      <th>Operating Expense Rate</th>\n",
       "      <th>Research and development expense rate</th>\n",
       "      <th>Cash flow rate</th>\n",
       "      <th>Interest-bearing debt interest rate</th>\n",
       "      <th>Tax rate (A)</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Cash Flow Per Share</th>\n",
       "      <th>Revenue Per Share (Yuan ¥)</th>\n",
       "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
       "      <th>Operating Profit Growth Rate</th>\n",
       "      <th>After-tax Net Profit Growth Rate</th>\n",
       "      <th>Regular Net Profit Growth Rate</th>\n",
       "      <th>Continuous Net Profit Growth Rate</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Net Value Growth Rate</th>\n",
       "      <th>Total Asset Return Growth Rate Ratio</th>\n",
       "      <th>Cash Reinvestment %</th>\n",
       "      <th>Current Ratio</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Interest Expense Ratio</th>\n",
       "      <th>Total debt/Total net worth</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Contingent liabilities/Net worth</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Inventory and accounts receivable/Net value</th>\n",
       "      <th>Total Asset Turnover</th>\n",
       "      <th>Accounts Receivable Turnover</th>\n",
       "      <th>Average Collection Days</th>\n",
       "      <th>Inventory Turnover Rate (times)</th>\n",
       "      <th>Fixed Assets Turnover Frequency</th>\n",
       "      <th>Net Worth Turnover Rate (times)</th>\n",
       "      <th>Revenue per person</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Allocation rate per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Current Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Quick Assets/Current Liability</th>\n",
       "      <th>Cash/Current Liability</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Inventory/Working Capital</th>\n",
       "      <th>Inventory/Current Liability</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Working Capital/Equity</th>\n",
       "      <th>Current Liabilities/Equity</th>\n",
       "      <th>Long-term Liability to Current Assets</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total income/Total expense</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Current Asset Turnover Rate</th>\n",
       "      <th>Quick Asset Turnover Rate</th>\n",
       "      <th>Working capitcal Turnover Rate</th>\n",
       "      <th>Cash Turnover Rate</th>\n",
       "      <th>Cash Flow to Sales</th>\n",
       "      <th>Fixed Assets to Assets</th>\n",
       "      <th>Current Liability to Liability</th>\n",
       "      <th>Current Liability to Equity</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>Cash Flow to Total Assets</th>\n",
       "      <th>Cash Flow to Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Cash Flow to Equity</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Liability-Assets Flag</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>0.780985</td>\n",
       "      <td>1.256969e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>7.250725e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.169141</td>\n",
       "      <td>0.311664</td>\n",
       "      <td>0.017560</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.138736</td>\n",
       "      <td>0.022102</td>\n",
       "      <td>0.848195</td>\n",
       "      <td>0.688979</td>\n",
       "      <td>0.688979</td>\n",
       "      <td>0.217535</td>\n",
       "      <td>4.980000e+09</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.363725</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.629951</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.207576</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.390284</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.095885</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>0.398036</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>1.820926e-04</td>\n",
       "      <td>1.165007e-04</td>\n",
       "      <td>0.032903</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.672775</td>\n",
       "      <td>0.166673</td>\n",
       "      <td>0.190643</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>1.473360e-04</td>\n",
       "      <td>0.147308</td>\n",
       "      <td>0.334015</td>\n",
       "      <td>0.276920</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.676269</td>\n",
       "      <td>0.721275</td>\n",
       "      <td>0.339077</td>\n",
       "      <td>2.559237e-02</td>\n",
       "      <td>0.903225</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>7.010000e+08</td>\n",
       "      <td>6.550000e+09</td>\n",
       "      <td>0.593831</td>\n",
       "      <td>4.580000e+08</td>\n",
       "      <td>0.671568</td>\n",
       "      <td>0.424206</td>\n",
       "      <td>0.676269</td>\n",
       "      <td>0.339077</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>0.637555</td>\n",
       "      <td>0.458609</td>\n",
       "      <td>0.520382</td>\n",
       "      <td>0.312905</td>\n",
       "      <td>0.118250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>0.781506</td>\n",
       "      <td>2.897851e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.461867</td>\n",
       "      <td>6.470647e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.318137</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>0.093722</td>\n",
       "      <td>0.169918</td>\n",
       "      <td>0.022080</td>\n",
       "      <td>0.848088</td>\n",
       "      <td>0.689693</td>\n",
       "      <td>0.689702</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>6.110000e+09</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.376709</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.635172</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.828824</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.093743</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.397725</td>\n",
       "      <td>0.064468</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>9.360000e+09</td>\n",
       "      <td>7.190000e+08</td>\n",
       "      <td>0.025484</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.391590</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.127236</td>\n",
       "      <td>0.182419</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>1.383910e-03</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>0.341106</td>\n",
       "      <td>0.289642</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.731975</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>2.394682e-02</td>\n",
       "      <td>0.931065</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>1.065198e-04</td>\n",
       "      <td>7.700000e+09</td>\n",
       "      <td>0.593916</td>\n",
       "      <td>2.490000e+09</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.468828</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.120916</td>\n",
       "      <td>0.641100</td>\n",
       "      <td>0.459001</td>\n",
       "      <td>0.567101</td>\n",
       "      <td>0.314163</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>0.780284</td>\n",
       "      <td>2.361297e-04</td>\n",
       "      <td>2.550000e+07</td>\n",
       "      <td>0.458521</td>\n",
       "      <td>7.900790e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.193713</td>\n",
       "      <td>0.180581</td>\n",
       "      <td>0.307102</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.092338</td>\n",
       "      <td>0.142803</td>\n",
       "      <td>0.022760</td>\n",
       "      <td>0.848094</td>\n",
       "      <td>0.689463</td>\n",
       "      <td>0.689470</td>\n",
       "      <td>0.217601</td>\n",
       "      <td>7.280000e+09</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.264184</td>\n",
       "      <td>0.368913</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.629631</td>\n",
       "      <td>0.021248</td>\n",
       "      <td>0.207516</td>\n",
       "      <td>0.792484</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.379093</td>\n",
       "      <td>0.006562</td>\n",
       "      <td>0.092318</td>\n",
       "      <td>0.148036</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>6.500000e+07</td>\n",
       "      <td>2.650000e+09</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.381968</td>\n",
       "      <td>0.141016</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.340201</td>\n",
       "      <td>0.602806</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.006302</td>\n",
       "      <td>5.340000e+09</td>\n",
       "      <td>0.098162</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.277456</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>0.446027</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.334777</td>\n",
       "      <td>3.715116e-03</td>\n",
       "      <td>0.909903</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.021387</td>\n",
       "      <td>1.791094e-03</td>\n",
       "      <td>1.022676e-03</td>\n",
       "      <td>0.594502</td>\n",
       "      <td>7.610000e+08</td>\n",
       "      <td>0.671571</td>\n",
       "      <td>0.276179</td>\n",
       "      <td>0.446027</td>\n",
       "      <td>0.334777</td>\n",
       "      <td>0.117922</td>\n",
       "      <td>0.642765</td>\n",
       "      <td>0.459254</td>\n",
       "      <td>0.538491</td>\n",
       "      <td>0.314515</td>\n",
       "      <td>0.025346</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.781241</td>\n",
       "      <td>1.078888e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.465705</td>\n",
       "      <td>4.490449e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.321674</td>\n",
       "      <td>0.014368</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.148603</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.217568</td>\n",
       "      <td>4.880000e+09</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.263371</td>\n",
       "      <td>0.384077</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.630228</td>\n",
       "      <td>0.009572</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.397925</td>\n",
       "      <td>0.089955</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>7.130000e+09</td>\n",
       "      <td>9.150000e+09</td>\n",
       "      <td>0.028065</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>0.378497</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.725754</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.225815</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>1.010646e-03</td>\n",
       "      <td>0.098715</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.276580</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.729825</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>2.216520e-02</td>\n",
       "      <td>0.906902</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>8.140000e+09</td>\n",
       "      <td>6.050000e+09</td>\n",
       "      <td>0.593889</td>\n",
       "      <td>2.030000e+09</td>\n",
       "      <td>0.671519</td>\n",
       "      <td>0.559144</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>0.120760</td>\n",
       "      <td>0.579039</td>\n",
       "      <td>0.448518</td>\n",
       "      <td>0.604105</td>\n",
       "      <td>0.302382</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>0.781550</td>\n",
       "      <td>7.890000e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.462746</td>\n",
       "      <td>6.860686e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.319162</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.096898</td>\n",
       "      <td>0.168412</td>\n",
       "      <td>0.022096</td>\n",
       "      <td>0.848258</td>\n",
       "      <td>0.689697</td>\n",
       "      <td>0.689697</td>\n",
       "      <td>0.217626</td>\n",
       "      <td>5.510000e+09</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.265218</td>\n",
       "      <td>0.379690</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.636055</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>0.375025</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>0.096927</td>\n",
       "      <td>0.167461</td>\n",
       "      <td>0.400079</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>1.633674e-04</td>\n",
       "      <td>2.935211e-04</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.058111</td>\n",
       "      <td>0.394371</td>\n",
       "      <td>0.023988</td>\n",
       "      <td>0.751822</td>\n",
       "      <td>0.260330</td>\n",
       "      <td>0.358380</td>\n",
       "      <td>0.014161</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>6.804636e-04</td>\n",
       "      <td>0.110195</td>\n",
       "      <td>0.344639</td>\n",
       "      <td>0.287913</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.330726</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.913850</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>6.680000e+09</td>\n",
       "      <td>5.050000e+09</td>\n",
       "      <td>0.593915</td>\n",
       "      <td>8.240000e+08</td>\n",
       "      <td>0.671563</td>\n",
       "      <td>0.309555</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.330726</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.622374</td>\n",
       "      <td>0.454411</td>\n",
       "      <td>0.578469</td>\n",
       "      <td>0.311567</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.539468</td>\n",
       "      <td>0.543230</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.604462</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797409</td>\n",
       "      <td>0.809331</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>1.510213e-04</td>\n",
       "      <td>4.500000e+09</td>\n",
       "      <td>0.463734</td>\n",
       "      <td>1.790179e-04</td>\n",
       "      <td>0.113372</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.216602</td>\n",
       "      <td>0.320966</td>\n",
       "      <td>0.020766</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.172102</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.848205</td>\n",
       "      <td>0.689778</td>\n",
       "      <td>0.689778</td>\n",
       "      <td>0.217635</td>\n",
       "      <td>7.070000e+09</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.264517</td>\n",
       "      <td>0.380155</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.631415</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.124618</td>\n",
       "      <td>0.875382</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.373823</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.098222</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.404804</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>4.030000e+07</td>\n",
       "      <td>1.429781e-04</td>\n",
       "      <td>0.027903</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.392596</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>0.817769</td>\n",
       "      <td>0.312840</td>\n",
       "      <td>0.578455</td>\n",
       "      <td>0.099481</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>5.071548e-03</td>\n",
       "      <td>0.103838</td>\n",
       "      <td>0.346224</td>\n",
       "      <td>0.277543</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.786888</td>\n",
       "      <td>0.736716</td>\n",
       "      <td>0.330914</td>\n",
       "      <td>1.792237e-03</td>\n",
       "      <td>0.925611</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>2.294154e-04</td>\n",
       "      <td>1.244230e-04</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>1.077940e-04</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.400338</td>\n",
       "      <td>0.786888</td>\n",
       "      <td>0.330914</td>\n",
       "      <td>0.112622</td>\n",
       "      <td>0.639806</td>\n",
       "      <td>0.458639</td>\n",
       "      <td>0.587178</td>\n",
       "      <td>0.314063</td>\n",
       "      <td>0.027951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.623620</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.279606</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.566193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0</td>\n",
       "      <td>0.475162</td>\n",
       "      <td>0.538269</td>\n",
       "      <td>0.524172</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.809327</td>\n",
       "      <td>0.303520</td>\n",
       "      <td>0.781586</td>\n",
       "      <td>5.220000e+09</td>\n",
       "      <td>1.440000e+09</td>\n",
       "      <td>0.461978</td>\n",
       "      <td>2.370237e-04</td>\n",
       "      <td>0.371596</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.216697</td>\n",
       "      <td>0.318278</td>\n",
       "      <td>0.023050</td>\n",
       "      <td>0.098608</td>\n",
       "      <td>0.172780</td>\n",
       "      <td>0.022159</td>\n",
       "      <td>0.848245</td>\n",
       "      <td>0.689734</td>\n",
       "      <td>0.689734</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>5.220000e+09</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.264730</td>\n",
       "      <td>0.377389</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>0.631489</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>0.900747</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.372505</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>0.098572</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.399926</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>9.940000e+09</td>\n",
       "      <td>6.051982e-04</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.016083</td>\n",
       "      <td>0.393625</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.793387</td>\n",
       "      <td>0.335085</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.080337</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>4.727181e-03</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>0.342166</td>\n",
       "      <td>0.277368</td>\n",
       "      <td>0.006730</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.734584</td>\n",
       "      <td>0.329753</td>\n",
       "      <td>2.204673e-03</td>\n",
       "      <td>0.932629</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>1.517299e-04</td>\n",
       "      <td>1.173396e-04</td>\n",
       "      <td>0.593954</td>\n",
       "      <td>7.710000e+09</td>\n",
       "      <td>0.671572</td>\n",
       "      <td>0.096136</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.329753</td>\n",
       "      <td>0.112329</td>\n",
       "      <td>0.642072</td>\n",
       "      <td>0.459058</td>\n",
       "      <td>0.569498</td>\n",
       "      <td>0.314446</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.566018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0</td>\n",
       "      <td>0.472725</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.520638</td>\n",
       "      <td>0.610444</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>0.781546</td>\n",
       "      <td>2.509312e-04</td>\n",
       "      <td>1.039086e-04</td>\n",
       "      <td>0.472189</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.490839</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.210929</td>\n",
       "      <td>0.324857</td>\n",
       "      <td>0.044255</td>\n",
       "      <td>0.100073</td>\n",
       "      <td>0.173232</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.847978</td>\n",
       "      <td>0.689202</td>\n",
       "      <td>0.689202</td>\n",
       "      <td>0.217547</td>\n",
       "      <td>5.990000e+09</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.263858</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.038424</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>0.630612</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.961061</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.100103</td>\n",
       "      <td>0.172287</td>\n",
       "      <td>0.395592</td>\n",
       "      <td>0.106447</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>2.797309e-04</td>\n",
       "      <td>1.024298e-03</td>\n",
       "      <td>0.022419</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.393693</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.866047</td>\n",
       "      <td>0.476747</td>\n",
       "      <td>0.496053</td>\n",
       "      <td>0.412885</td>\n",
       "      <td>0.035531</td>\n",
       "      <td>8.821248e-02</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.358847</td>\n",
       "      <td>0.277022</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.553964</td>\n",
       "      <td>0.737432</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.035446</td>\n",
       "      <td>1.762272e-04</td>\n",
       "      <td>1.749713e-04</td>\n",
       "      <td>0.594025</td>\n",
       "      <td>4.074263e-04</td>\n",
       "      <td>0.671564</td>\n",
       "      <td>0.055509</td>\n",
       "      <td>0.553964</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.631678</td>\n",
       "      <td>0.452465</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.313353</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0</td>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.610441</td>\n",
       "      <td>0.840138</td>\n",
       "      <td>0.275789</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0</td>\n",
       "      <td>0.506264</td>\n",
       "      <td>0.559911</td>\n",
       "      <td>0.554045</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.809399</td>\n",
       "      <td>0.303498</td>\n",
       "      <td>0.781663</td>\n",
       "      <td>1.236154e-04</td>\n",
       "      <td>2.510000e+09</td>\n",
       "      <td>0.476123</td>\n",
       "      <td>2.110211e-04</td>\n",
       "      <td>0.181294</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.228326</td>\n",
       "      <td>0.346573</td>\n",
       "      <td>0.031535</td>\n",
       "      <td>0.111799</td>\n",
       "      <td>0.185584</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>0.854064</td>\n",
       "      <td>0.696113</td>\n",
       "      <td>0.696113</td>\n",
       "      <td>0.218006</td>\n",
       "      <td>7.250000e+09</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.264409</td>\n",
       "      <td>0.401028</td>\n",
       "      <td>0.012782</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.630731</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.086979</td>\n",
       "      <td>0.913021</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.369649</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.111722</td>\n",
       "      <td>0.182498</td>\n",
       "      <td>0.401540</td>\n",
       "      <td>0.109445</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>4.550000e+09</td>\n",
       "      <td>2.330013e-04</td>\n",
       "      <td>0.027258</td>\n",
       "      <td>0.012749</td>\n",
       "      <td>0.396735</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.832340</td>\n",
       "      <td>0.353624</td>\n",
       "      <td>0.564439</td>\n",
       "      <td>0.112238</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>7.133218e-03</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>0.380251</td>\n",
       "      <td>0.277353</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.893241</td>\n",
       "      <td>0.736713</td>\n",
       "      <td>0.329294</td>\n",
       "      <td>3.200000e+09</td>\n",
       "      <td>0.939613</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>2.135940e-04</td>\n",
       "      <td>1.351937e-04</td>\n",
       "      <td>0.593997</td>\n",
       "      <td>1.165392e-04</td>\n",
       "      <td>0.671606</td>\n",
       "      <td>0.246805</td>\n",
       "      <td>0.893241</td>\n",
       "      <td>0.329294</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.684857</td>\n",
       "      <td>0.471313</td>\n",
       "      <td>0.678338</td>\n",
       "      <td>0.320118</td>\n",
       "      <td>0.022916</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.607846</td>\n",
       "      <td>0.841084</td>\n",
       "      <td>0.277547</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493053</td>\n",
       "      <td>0.570105</td>\n",
       "      <td>0.549548</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.998080</td>\n",
       "      <td>0.801987</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.786079</td>\n",
       "      <td>1.431695e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.427721</td>\n",
       "      <td>5.900000e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.227758</td>\n",
       "      <td>0.305793</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.092501</td>\n",
       "      <td>0.182119</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.848053</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.217605</td>\n",
       "      <td>9.350000e+09</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.264186</td>\n",
       "      <td>0.360102</td>\n",
       "      <td>0.051348</td>\n",
       "      <td>0.040897</td>\n",
       "      <td>0.630618</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.985851</td>\n",
       "      <td>0.058476</td>\n",
       "      <td>0.370049</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.092465</td>\n",
       "      <td>0.179911</td>\n",
       "      <td>0.393883</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>1.910000e+07</td>\n",
       "      <td>2.995731e-04</td>\n",
       "      <td>0.009194</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.385767</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.873759</td>\n",
       "      <td>0.527136</td>\n",
       "      <td>0.505010</td>\n",
       "      <td>0.238147</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>6.667354e-02</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.239585</td>\n",
       "      <td>0.276975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737286</td>\n",
       "      <td>0.326690</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.938005</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>7.863781e-03</td>\n",
       "      <td>8.238471e-03</td>\n",
       "      <td>0.598674</td>\n",
       "      <td>9.505992e-03</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.326690</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.659917</td>\n",
       "      <td>0.483285</td>\n",
       "      <td>0.505531</td>\n",
       "      <td>0.316238</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.626680</td>\n",
       "      <td>0.627408</td>\n",
       "      <td>0.841019</td>\n",
       "      <td>0.275114</td>\n",
       "      <td>0.026793</td>\n",
       "      <td>0.565167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.233902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "0             1                                           0.370594          \n",
       "1             1                                           0.464291          \n",
       "2             1                                           0.426071          \n",
       "3             1                                           0.399844          \n",
       "4             1                                           0.465022          \n",
       "...         ...                                                ...          \n",
       "6814          0                                           0.493687          \n",
       "6815          0                                           0.475162          \n",
       "6816          0                                           0.472725          \n",
       "6817          0                                           0.506264          \n",
       "6818          0                                           0.493053          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "0                                    0.424389   \n",
       "1                                    0.538214   \n",
       "2                                    0.499019   \n",
       "3                                    0.451265   \n",
       "4                                    0.538432   \n",
       "...                                       ...   \n",
       "6814                                 0.539468   \n",
       "6815                                 0.538269   \n",
       "6816                                 0.533744   \n",
       "6817                                 0.559911   \n",
       "6818                                 0.570105   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "0                                              0.405750    \n",
       "1                                              0.516730    \n",
       "2                                              0.472295    \n",
       "3                                              0.457733    \n",
       "4                                              0.522298    \n",
       "...                                                 ...    \n",
       "6814                                           0.543230    \n",
       "6815                                           0.524172    \n",
       "6816                                           0.520638    \n",
       "6817                                           0.554045    \n",
       "6818                                           0.549548    \n",
       "\n",
       "       Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "0                    0.601457                      0.601457   \n",
       "1                    0.610235                      0.610235   \n",
       "2                    0.601450                      0.601364   \n",
       "3                    0.583541                      0.583541   \n",
       "4                    0.598783                      0.598783   \n",
       "...                       ...                           ...   \n",
       "6814                 0.604455                      0.604462   \n",
       "6815                 0.598308                      0.598308   \n",
       "6816                 0.610444                      0.610213   \n",
       "6817                 0.607850                      0.607850   \n",
       "6818                 0.627409                      0.627409   \n",
       "\n",
       "       Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "0                   0.998969                    0.796887   \n",
       "1                   0.998946                    0.797380   \n",
       "2                   0.998857                    0.796403   \n",
       "3                   0.998700                    0.796967   \n",
       "4                   0.998973                    0.797366   \n",
       "...                      ...                         ...   \n",
       "6814                0.998992                    0.797409   \n",
       "6815                0.998992                    0.797414   \n",
       "6816                0.998984                    0.797401   \n",
       "6817                0.999074                    0.797500   \n",
       "6818                0.998080                    0.801987   \n",
       "\n",
       "       After-tax net Interest Rate  \\\n",
       "0                         0.808809   \n",
       "1                         0.809301   \n",
       "2                         0.808388   \n",
       "3                         0.808966   \n",
       "4                         0.809304   \n",
       "...                            ...   \n",
       "6814                      0.809331   \n",
       "6815                      0.809327   \n",
       "6816                      0.809317   \n",
       "6817                      0.809399   \n",
       "6818                      0.813800   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  \\\n",
       "0                                         0.302646   \n",
       "1                                         0.303556   \n",
       "2                                         0.302035   \n",
       "3                                         0.303350   \n",
       "4                                         0.303475   \n",
       "...                                            ...   \n",
       "6814                                      0.303510   \n",
       "6815                                      0.303520   \n",
       "6816                                      0.303512   \n",
       "6817                                      0.303498   \n",
       "6818                                      0.313415   \n",
       "\n",
       "       Continuous interest rate (after tax)   Operating Expense Rate  \\\n",
       "0                                  0.780985             1.256969e-04   \n",
       "1                                  0.781506             2.897851e-04   \n",
       "2                                  0.780284             2.361297e-04   \n",
       "3                                  0.781241             1.078888e-04   \n",
       "4                                  0.781550             7.890000e+09   \n",
       "...                                     ...                      ...   \n",
       "6814                               0.781588             1.510213e-04   \n",
       "6815                               0.781586             5.220000e+09   \n",
       "6816                               0.781546             2.509312e-04   \n",
       "6817                               0.781663             1.236154e-04   \n",
       "6818                               0.786079             1.431695e-03   \n",
       "\n",
       "       Research and development expense rate   Cash flow rate  \\\n",
       "0                               0.000000e+00         0.458143   \n",
       "1                               0.000000e+00         0.461867   \n",
       "2                               2.550000e+07         0.458521   \n",
       "3                               0.000000e+00         0.465705   \n",
       "4                               0.000000e+00         0.462746   \n",
       "...                                      ...              ...   \n",
       "6814                            4.500000e+09         0.463734   \n",
       "6815                            1.440000e+09         0.461978   \n",
       "6816                            1.039086e-04         0.472189   \n",
       "6817                            2.510000e+09         0.476123   \n",
       "6818                            0.000000e+00         0.427721   \n",
       "\n",
       "       Interest-bearing debt interest rate   Tax rate (A)  \\\n",
       "0                             7.250725e-04       0.000000   \n",
       "1                             6.470647e-04       0.000000   \n",
       "2                             7.900790e-04       0.000000   \n",
       "3                             4.490449e-04       0.000000   \n",
       "4                             6.860686e-04       0.000000   \n",
       "...                                    ...            ...   \n",
       "6814                          1.790179e-04       0.113372   \n",
       "6815                          2.370237e-04       0.371596   \n",
       "6816                          0.000000e+00       0.490839   \n",
       "6817                          2.110211e-04       0.181294   \n",
       "6818                          5.900000e+08       0.000000   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "0                     0.147950                  0.147950   \n",
       "1                     0.182251                  0.182251   \n",
       "2                     0.177911                  0.177911   \n",
       "3                     0.154187                  0.154187   \n",
       "4                     0.167502                  0.167502   \n",
       "...                        ...                       ...   \n",
       "6814                  0.175045                  0.175045   \n",
       "6815                  0.181324                  0.181324   \n",
       "6816                  0.269521                  0.269521   \n",
       "6817                  0.213392                  0.213392   \n",
       "6818                  0.220766                  0.220766   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "0                     0.147950                                  0.169141   \n",
       "1                     0.182251                                  0.208944   \n",
       "2                     0.193713                                  0.180581   \n",
       "3                     0.154187                                  0.193722   \n",
       "4                     0.167502                                  0.212537   \n",
       "...                        ...                                       ...   \n",
       "6814                  0.175045                                  0.216602   \n",
       "6815                  0.181324                                  0.216697   \n",
       "6816                  0.269521                                  0.210929   \n",
       "6817                  0.213392                                  0.228326   \n",
       "6818                  0.220766                                  0.227758   \n",
       "\n",
       "       Cash Flow Per Share   Revenue Per Share (Yuan ¥)  \\\n",
       "0                 0.311664                     0.017560   \n",
       "1                 0.318137                     0.021144   \n",
       "2                 0.307102                     0.005944   \n",
       "3                 0.321674                     0.014368   \n",
       "4                 0.319162                     0.029690   \n",
       "...                    ...                          ...   \n",
       "6814              0.320966                     0.020766   \n",
       "6815              0.318278                     0.023050   \n",
       "6816              0.324857                     0.044255   \n",
       "6817              0.346573                     0.031535   \n",
       "6818              0.305793                     0.000665   \n",
       "\n",
       "       Operating Profit Per Share (Yuan ¥)  \\\n",
       "0                                 0.095921   \n",
       "1                                 0.093722   \n",
       "2                                 0.092338   \n",
       "3                                 0.077762   \n",
       "4                                 0.096898   \n",
       "...                                    ...   \n",
       "6814                              0.098200   \n",
       "6815                              0.098608   \n",
       "6816                              0.100073   \n",
       "6817                              0.111799   \n",
       "6818                              0.092501   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)  \\\n",
       "0                                      0.138736   \n",
       "1                                      0.169918   \n",
       "2                                      0.142803   \n",
       "3                                      0.148603   \n",
       "4                                      0.168412   \n",
       "...                                         ...   \n",
       "6814                                   0.172102   \n",
       "6815                                   0.172780   \n",
       "6816                                   0.173232   \n",
       "6817                                   0.185584   \n",
       "6818                                   0.182119   \n",
       "\n",
       "       Realized Sales Gross Profit Growth Rate   Operating Profit Growth Rate  \\\n",
       "0                                     0.022102                       0.848195   \n",
       "1                                     0.022080                       0.848088   \n",
       "2                                     0.022760                       0.848094   \n",
       "3                                     0.022046                       0.848005   \n",
       "4                                     0.022096                       0.848258   \n",
       "...                                        ...                            ...   \n",
       "6814                                  0.022374                       0.848205   \n",
       "6815                                  0.022159                       0.848245   \n",
       "6816                                  0.022068                       0.847978   \n",
       "6817                                  0.022350                       0.854064   \n",
       "6818                                  0.025316                       0.848053   \n",
       "\n",
       "       After-tax Net Profit Growth Rate   Regular Net Profit Growth Rate  \\\n",
       "0                              0.688979                         0.688979   \n",
       "1                              0.689693                         0.689702   \n",
       "2                              0.689463                         0.689470   \n",
       "3                              0.689110                         0.689110   \n",
       "4                              0.689697                         0.689697   \n",
       "...                                 ...                              ...   \n",
       "6814                           0.689778                         0.689778   \n",
       "6815                           0.689734                         0.689734   \n",
       "6816                           0.689202                         0.689202   \n",
       "6817                           0.696113                         0.696113   \n",
       "6818                           0.689527                         0.689527   \n",
       "\n",
       "       Continuous Net Profit Growth Rate   Total Asset Growth Rate  \\\n",
       "0                               0.217535              4.980000e+09   \n",
       "1                               0.217620              6.110000e+09   \n",
       "2                               0.217601              7.280000e+09   \n",
       "3                               0.217568              4.880000e+09   \n",
       "4                               0.217626              5.510000e+09   \n",
       "...                                  ...                       ...   \n",
       "6814                            0.217635              7.070000e+09   \n",
       "6815                            0.217631              5.220000e+09   \n",
       "6816                            0.217547              5.990000e+09   \n",
       "6817                            0.218006              7.250000e+09   \n",
       "6818                            0.217605              9.350000e+09   \n",
       "\n",
       "       Net Value Growth Rate   Total Asset Return Growth Rate Ratio  \\\n",
       "0                   0.000327                               0.263100   \n",
       "1                   0.000443                               0.264516   \n",
       "2                   0.000396                               0.264184   \n",
       "3                   0.000382                               0.263371   \n",
       "4                   0.000439                               0.265218   \n",
       "...                      ...                                    ...   \n",
       "6814                0.000450                               0.264517   \n",
       "6815                0.000445                               0.264730   \n",
       "6816                0.000435                               0.263858   \n",
       "6817                0.000529                               0.264409   \n",
       "6818                0.000519                               0.264186   \n",
       "\n",
       "       Cash Reinvestment %   Current Ratio   Quick Ratio  \\\n",
       "0                 0.363725        0.002259      0.001208   \n",
       "1                 0.376709        0.006016      0.004039   \n",
       "2                 0.368913        0.011543      0.005348   \n",
       "3                 0.384077        0.004194      0.002896   \n",
       "4                 0.379690        0.006022      0.003727   \n",
       "...                    ...             ...           ...   \n",
       "6814              0.380155        0.010451      0.005457   \n",
       "6815              0.377389        0.009259      0.006741   \n",
       "6816              0.379392        0.038424      0.035112   \n",
       "6817              0.401028        0.012782      0.007256   \n",
       "6818              0.360102        0.051348      0.040897   \n",
       "\n",
       "       Interest Expense Ratio   Total debt/Total net worth   Debt ratio %  \\\n",
       "0                    0.629951                     0.021266       0.207576   \n",
       "1                    0.635172                     0.012502       0.171176   \n",
       "2                    0.629631                     0.021248       0.207516   \n",
       "3                    0.630228                     0.009572       0.151465   \n",
       "4                    0.636055                     0.005150       0.106509   \n",
       "...                       ...                          ...            ...   \n",
       "6814                 0.631415                     0.006655       0.124618   \n",
       "6815                 0.631489                     0.004623       0.099253   \n",
       "6816                 0.630612                     0.001392       0.038939   \n",
       "6817                 0.630731                     0.003816       0.086979   \n",
       "6818                 0.630618                     0.000461       0.014149   \n",
       "\n",
       "       Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "0              0.792424                               0.005024   \n",
       "1              0.828824                               0.005059   \n",
       "2              0.792484                               0.005100   \n",
       "3              0.848535                               0.005047   \n",
       "4              0.893491                               0.005303   \n",
       "...                 ...                                    ...   \n",
       "6814           0.875382                               0.005150   \n",
       "6815           0.900747                               0.006772   \n",
       "6816           0.961061                               0.009149   \n",
       "6817           0.913021                               0.005529   \n",
       "6818           0.985851                               0.058476   \n",
       "\n",
       "       Borrowing dependency   Contingent liabilities/Net worth  \\\n",
       "0                  0.390284                           0.006479   \n",
       "1                  0.376760                           0.005835   \n",
       "2                  0.379093                           0.006562   \n",
       "3                  0.379743                           0.005366   \n",
       "4                  0.375025                           0.006624   \n",
       "...                     ...                                ...   \n",
       "6814               0.373823                           0.005366   \n",
       "6815               0.372505                           0.008619   \n",
       "6816               0.369637                           0.005366   \n",
       "6817               0.369649                           0.007068   \n",
       "6818               0.370049                           0.006368   \n",
       "\n",
       "       Operating profit/Paid-in capital  \\\n",
       "0                              0.095885   \n",
       "1                              0.093743   \n",
       "2                              0.092318   \n",
       "3                              0.077727   \n",
       "4                              0.096927   \n",
       "...                                 ...   \n",
       "6814                           0.098222   \n",
       "6815                           0.098572   \n",
       "6816                           0.100103   \n",
       "6817                           0.111722   \n",
       "6818                           0.092465   \n",
       "\n",
       "       Net profit before tax/Paid-in capital  \\\n",
       "0                                   0.137757   \n",
       "1                                   0.168962   \n",
       "2                                   0.148036   \n",
       "3                                   0.147561   \n",
       "4                                   0.167461   \n",
       "...                                      ...   \n",
       "6814                                0.171111   \n",
       "6815                                0.171805   \n",
       "6816                                0.172287   \n",
       "6817                                0.182498   \n",
       "6818                                0.179911   \n",
       "\n",
       "       Inventory and accounts receivable/Net value   Total Asset Turnover  \\\n",
       "0                                         0.398036               0.086957   \n",
       "1                                         0.397725               0.064468   \n",
       "2                                         0.406580               0.014993   \n",
       "3                                         0.397925               0.089955   \n",
       "4                                         0.400079               0.175412   \n",
       "...                                            ...                    ...   \n",
       "6814                                      0.404804               0.103448   \n",
       "6815                                      0.399926               0.103448   \n",
       "6816                                      0.395592               0.106447   \n",
       "6817                                      0.401540               0.109445   \n",
       "6818                                      0.393883               0.002999   \n",
       "\n",
       "       Accounts Receivable Turnover   Average Collection Days  \\\n",
       "0                          0.001814                  0.003487   \n",
       "1                          0.001286                  0.004917   \n",
       "2                          0.001495                  0.004227   \n",
       "3                          0.001966                  0.003215   \n",
       "4                          0.001449                  0.004367   \n",
       "...                             ...                       ...   \n",
       "6814                       0.000690                  0.009177   \n",
       "6815                       0.000655                  0.009652   \n",
       "6816                       0.001510                  0.004188   \n",
       "6817                       0.000716                  0.008829   \n",
       "6818                       0.000325                  0.019474   \n",
       "\n",
       "       Inventory Turnover Rate (times)   Fixed Assets Turnover Frequency  \\\n",
       "0                         1.820926e-04                      1.165007e-04   \n",
       "1                         9.360000e+09                      7.190000e+08   \n",
       "2                         6.500000e+07                      2.650000e+09   \n",
       "3                         7.130000e+09                      9.150000e+09   \n",
       "4                         1.633674e-04                      2.935211e-04   \n",
       "...                                ...                               ...   \n",
       "6814                      4.030000e+07                      1.429781e-04   \n",
       "6815                      9.940000e+09                      6.051982e-04   \n",
       "6816                      2.797309e-04                      1.024298e-03   \n",
       "6817                      4.550000e+09                      2.330013e-04   \n",
       "6818                      1.910000e+07                      2.995731e-04   \n",
       "\n",
       "       Net Worth Turnover Rate (times)   Revenue per person  \\\n",
       "0                             0.032903             0.034164   \n",
       "1                             0.025484             0.006889   \n",
       "2                             0.013387             0.028997   \n",
       "3                             0.028065             0.015463   \n",
       "4                             0.040161             0.058111   \n",
       "...                                ...                  ...   \n",
       "6814                          0.027903             0.006348   \n",
       "6815                          0.027419             0.016083   \n",
       "6816                          0.022419             0.022097   \n",
       "6817                          0.027258             0.012749   \n",
       "6818                          0.009194             0.002097   \n",
       "\n",
       "       Operating profit per person   Allocation rate per person  \\\n",
       "0                         0.392913                     0.037135   \n",
       "1                         0.391590                     0.012335   \n",
       "2                         0.381968                     0.141016   \n",
       "3                         0.378497                     0.021320   \n",
       "4                         0.394371                     0.023988   \n",
       "...                            ...                          ...   \n",
       "6814                      0.392596                     0.006312   \n",
       "6815                      0.393625                     0.003401   \n",
       "6816                      0.393693                     0.002774   \n",
       "6817                      0.396735                     0.007489   \n",
       "6818                      0.385767                     0.000963   \n",
       "\n",
       "       Working Capital to Total Assets   Quick Assets/Total Assets  \\\n",
       "0                             0.672775                    0.166673   \n",
       "1                             0.751111                    0.127236   \n",
       "2                             0.829502                    0.340201   \n",
       "3                             0.725754                    0.161575   \n",
       "4                             0.751822                    0.260330   \n",
       "...                                ...                         ...   \n",
       "6814                          0.817769                    0.312840   \n",
       "6815                          0.793387                    0.335085   \n",
       "6816                          0.866047                    0.476747   \n",
       "6817                          0.832340                    0.353624   \n",
       "6818                          0.873759                    0.527136   \n",
       "\n",
       "       Current Assets/Total Assets   Cash/Total Assets  \\\n",
       "0                         0.190643            0.004094   \n",
       "1                         0.182419            0.014948   \n",
       "2                         0.602806            0.000991   \n",
       "3                         0.225815            0.018851   \n",
       "4                         0.358380            0.014161   \n",
       "...                            ...                 ...   \n",
       "6814                      0.578455            0.099481   \n",
       "6815                      0.444043            0.080337   \n",
       "6816                      0.496053            0.412885   \n",
       "6817                      0.564439            0.112238   \n",
       "6818                      0.505010            0.238147   \n",
       "\n",
       "       Quick Assets/Current Liability   Cash/Current Liability  \\\n",
       "0                            0.001997             1.473360e-04   \n",
       "1                            0.004136             1.383910e-03   \n",
       "2                            0.006302             5.340000e+09   \n",
       "3                            0.002961             1.010646e-03   \n",
       "4                            0.004275             6.804636e-04   \n",
       "...                               ...                      ...   \n",
       "6814                         0.005469             5.071548e-03   \n",
       "6815                         0.006790             4.727181e-03   \n",
       "6816                         0.035531             8.821248e-02   \n",
       "6817                         0.007753             7.133218e-03   \n",
       "6818                         0.051481             6.667354e-02   \n",
       "\n",
       "       Current Liability to Assets   Operating Funds to Liability  \\\n",
       "0                         0.147308                       0.334015   \n",
       "1                         0.056963                       0.341106   \n",
       "2                         0.098162                       0.336731   \n",
       "3                         0.098715                       0.348716   \n",
       "4                         0.110195                       0.344639   \n",
       "...                            ...                            ...   \n",
       "6814                      0.103838                       0.346224   \n",
       "6815                      0.089901                       0.342166   \n",
       "6816                      0.024414                       0.358847   \n",
       "6817                      0.083199                       0.380251   \n",
       "6818                      0.018517                       0.239585   \n",
       "\n",
       "       Inventory/Working Capital   Inventory/Current Liability  \\\n",
       "0                       0.276920                      0.001036   \n",
       "1                       0.289642                      0.005210   \n",
       "2                       0.277456                      0.013879   \n",
       "3                       0.276580                      0.003540   \n",
       "4                       0.287913                      0.004869   \n",
       "...                          ...                           ...   \n",
       "6814                    0.277543                      0.013212   \n",
       "6815                    0.277368                      0.006730   \n",
       "6816                    0.277022                      0.007810   \n",
       "6817                    0.277353                      0.013334   \n",
       "6818                    0.276975                      0.000000   \n",
       "\n",
       "       Current Liabilities/Liability   Working Capital/Equity  \\\n",
       "0                           0.676269                 0.721275   \n",
       "1                           0.308589                 0.731975   \n",
       "2                           0.446027                 0.742729   \n",
       "3                           0.615848                 0.729825   \n",
       "4                           0.975007                 0.732000   \n",
       "...                              ...                      ...   \n",
       "6814                        0.786888                 0.736716   \n",
       "6815                        0.849898                 0.734584   \n",
       "6816                        0.553964                 0.737432   \n",
       "6817                        0.893241                 0.736713   \n",
       "6818                        1.000000                 0.737286   \n",
       "\n",
       "       Current Liabilities/Equity   Long-term Liability to Current Assets  \\\n",
       "0                        0.339077                            2.559237e-02   \n",
       "1                        0.329740                            2.394682e-02   \n",
       "2                        0.334777                            3.715116e-03   \n",
       "3                        0.331509                            2.216520e-02   \n",
       "4                        0.330726                            0.000000e+00   \n",
       "...                           ...                                     ...   \n",
       "6814                     0.330914                            1.792237e-03   \n",
       "6815                     0.329753                            2.204673e-03   \n",
       "6816                     0.326921                            0.000000e+00   \n",
       "6817                     0.329294                            3.200000e+09   \n",
       "6818                     0.326690                            0.000000e+00   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total income/Total expense  \\\n",
       "0                               0.903225                     0.002022   \n",
       "1                               0.931065                     0.002226   \n",
       "2                               0.909903                     0.002060   \n",
       "3                               0.906902                     0.001831   \n",
       "4                               0.913850                     0.002224   \n",
       "...                                  ...                          ...   \n",
       "6814                            0.925611                     0.002266   \n",
       "6815                            0.932629                     0.002288   \n",
       "6816                            0.932000                     0.002239   \n",
       "6817                            0.939613                     0.002395   \n",
       "6818                            0.938005                     0.002791   \n",
       "\n",
       "       Total expense/Assets   Current Asset Turnover Rate  \\\n",
       "0                  0.064856                  7.010000e+08   \n",
       "1                  0.025516                  1.065198e-04   \n",
       "2                  0.021387                  1.791094e-03   \n",
       "3                  0.024161                  8.140000e+09   \n",
       "4                  0.026385                  6.680000e+09   \n",
       "...                     ...                           ...   \n",
       "6814               0.019060                  2.294154e-04   \n",
       "6815               0.011118                  1.517299e-04   \n",
       "6816               0.035446                  1.762272e-04   \n",
       "6817               0.016443                  2.135940e-04   \n",
       "6818               0.006089                  7.863781e-03   \n",
       "\n",
       "       Quick Asset Turnover Rate   Working capitcal Turnover Rate  \\\n",
       "0                   6.550000e+09                         0.593831   \n",
       "1                   7.700000e+09                         0.593916   \n",
       "2                   1.022676e-03                         0.594502   \n",
       "3                   6.050000e+09                         0.593889   \n",
       "4                   5.050000e+09                         0.593915   \n",
       "...                          ...                              ...   \n",
       "6814                1.244230e-04                         0.593985   \n",
       "6815                1.173396e-04                         0.593954   \n",
       "6816                1.749713e-04                         0.594025   \n",
       "6817                1.351937e-04                         0.593997   \n",
       "6818                8.238471e-03                         0.598674   \n",
       "\n",
       "       Cash Turnover Rate   Cash Flow to Sales   Fixed Assets to Assets  \\\n",
       "0            4.580000e+08             0.671568                 0.424206   \n",
       "1            2.490000e+09             0.671570                 0.468828   \n",
       "2            7.610000e+08             0.671571                 0.276179   \n",
       "3            2.030000e+09             0.671519                 0.559144   \n",
       "4            8.240000e+08             0.671563                 0.309555   \n",
       "...                   ...                  ...                      ...   \n",
       "6814         1.077940e-04             0.671570                 0.400338   \n",
       "6815         7.710000e+09             0.671572                 0.096136   \n",
       "6816         4.074263e-04             0.671564                 0.055509   \n",
       "6817         1.165392e-04             0.671606                 0.246805   \n",
       "6818         9.505992e-03             0.672096                 0.005016   \n",
       "\n",
       "       Current Liability to Liability   Current Liability to Equity  \\\n",
       "0                            0.676269                      0.339077   \n",
       "1                            0.308589                      0.329740   \n",
       "2                            0.446027                      0.334777   \n",
       "3                            0.615848                      0.331509   \n",
       "4                            0.975007                      0.330726   \n",
       "...                               ...                           ...   \n",
       "6814                         0.786888                      0.330914   \n",
       "6815                         0.849898                      0.329753   \n",
       "6816                         0.553964                      0.326921   \n",
       "6817                         0.893241                      0.329294   \n",
       "6818                         1.000000                      0.326690   \n",
       "\n",
       "       Equity to Long-term Liability   Cash Flow to Total Assets  \\\n",
       "0                           0.126549                    0.637555   \n",
       "1                           0.120916                    0.641100   \n",
       "2                           0.117922                    0.642765   \n",
       "3                           0.120760                    0.579039   \n",
       "4                           0.110933                    0.622374   \n",
       "...                              ...                         ...   \n",
       "6814                        0.112622                    0.639806   \n",
       "6815                        0.112329                    0.642072   \n",
       "6816                        0.110933                    0.631678   \n",
       "6817                        0.110957                    0.684857   \n",
       "6818                        0.110933                    0.659917   \n",
       "\n",
       "       Cash Flow to Liability   CFO to Assets   Cash Flow to Equity  \\\n",
       "0                    0.458609        0.520382              0.312905   \n",
       "1                    0.459001        0.567101              0.314163   \n",
       "2                    0.459254        0.538491              0.314515   \n",
       "3                    0.448518        0.604105              0.302382   \n",
       "4                    0.454411        0.578469              0.311567   \n",
       "...                       ...             ...                   ...   \n",
       "6814                 0.458639        0.587178              0.314063   \n",
       "6815                 0.459058        0.569498              0.314446   \n",
       "6816                 0.452465        0.589341              0.313353   \n",
       "6817                 0.471313        0.678338              0.320118   \n",
       "6818                 0.483285        0.505531              0.316238   \n",
       "\n",
       "       Current Liability to Current Assets   Liability-Assets Flag  \\\n",
       "0                                 0.118250                       0   \n",
       "1                                 0.047775                       0   \n",
       "2                                 0.025346                       0   \n",
       "3                                 0.067250                       0   \n",
       "4                                 0.047725                       0   \n",
       "...                                    ...                     ...   \n",
       "6814                              0.027951                       0   \n",
       "6815                              0.031470                       0   \n",
       "6816                              0.007542                       0   \n",
       "6817                              0.022916                       0   \n",
       "6818                              0.005579                       0   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0                        0.716845                    0.009219   \n",
       "1                        0.795297                    0.008323   \n",
       "2                        0.774670                    0.040003   \n",
       "3                        0.739555                    0.003252   \n",
       "4                        0.795016                    0.003878   \n",
       "...                           ...                         ...   \n",
       "6814                     0.799927                    0.000466   \n",
       "6815                     0.799748                    0.001959   \n",
       "6816                     0.797778                    0.002840   \n",
       "6817                     0.811808                    0.002837   \n",
       "6818                     0.815956                    0.000707   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "0                0.622879                0.601453   \n",
       "1                0.623652                0.610237   \n",
       "2                0.623841                0.601449   \n",
       "3                0.622929                0.583538   \n",
       "4                0.623521                0.598782   \n",
       "...                   ...                     ...   \n",
       "6814             0.623620                0.604455   \n",
       "6815             0.623931                0.598306   \n",
       "6816             0.624156                0.610441   \n",
       "6817             0.623957                0.607846   \n",
       "6818             0.626680                0.627408   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                                0.827890              0.290202   \n",
       "1                                0.839969              0.283846   \n",
       "2                                0.836774              0.290189   \n",
       "3                                0.834697              0.281721   \n",
       "4                                0.839973              0.278514   \n",
       "...                                   ...                   ...   \n",
       "6814                             0.840359              0.279606   \n",
       "6815                             0.840306              0.278132   \n",
       "6816                             0.840138              0.275789   \n",
       "6817                             0.841084              0.277547   \n",
       "6818                             0.841019              0.275114   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "0                                0.026601   \n",
       "1                                0.264577   \n",
       "2                                0.026555   \n",
       "3                                0.026697   \n",
       "4                                0.024752   \n",
       "...                                   ...   \n",
       "6814                             0.027064   \n",
       "6815                             0.027009   \n",
       "6816                             0.026791   \n",
       "6817                             0.026822   \n",
       "6818                             0.026793   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "0                                              0.564050                   1   \n",
       "1                                              0.570175                   1   \n",
       "2                                              0.563706                   1   \n",
       "3                                              0.564663                   1   \n",
       "4                                              0.575617                   1   \n",
       "...                                                 ...                 ...   \n",
       "6814                                           0.566193                   1   \n",
       "6815                                           0.566018                   1   \n",
       "6816                                           0.565158                   1   \n",
       "6817                                           0.565302                   1   \n",
       "6818                                           0.565167                   1   \n",
       "\n",
       "       Equity to Liability  \n",
       "0                 0.016469  \n",
       "1                 0.020794  \n",
       "2                 0.016474  \n",
       "3                 0.023982  \n",
       "4                 0.035490  \n",
       "...                    ...  \n",
       "6814              0.029890  \n",
       "6815              0.038284  \n",
       "6816              0.097649  \n",
       "6817              0.044009  \n",
       "6818              0.233902  \n",
       "\n",
       "[6819 rows x 96 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f357ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([' Current Liability to Liability', ' Net Income Flag'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfa0052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6819, 94)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30477381",
   "metadata": {},
   "source": [
    "# Separating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c45973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Bankrupt?'], axis = 1)\n",
    "y = df['Bankrupt?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea246009",
   "metadata": {},
   "source": [
    "# Selecting features with the Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0b6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X1) \n",
    "scaler.transform(X1)\n",
    "X1_sc = pd.DataFrame(scaler.transform(X1), columns=X1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec224fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "rfe = RFE(clf, n_features_to_select=60, verbose=False)\n",
    "sel = rfe.fit(X1_sc, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4109c4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Column_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ROA(C) before interest and depreciation befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ROA(A) before interest and % after tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ROA(B) before interest and depreciation after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating Gross Margin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating Profit Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Pre-tax net Interest Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>After-tax net Interest Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Non-industry income and expenditure/revenue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Continuous interest rate (after tax)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Cash flow rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Net Value Per Share (B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>Net Value Per Share (A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Net Value Per Share (C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>Persistent EPS in the Last Four Seasons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>Per Share Net profit before tax (Yuan ¥)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Realized Sales Gross Profit Growth Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating Profit Growth Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>Regular Net Profit Growth Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Continuous Net Profit Growth Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Total Asset Growth Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>Total Asset Return Growth Rate Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>Cash Reinvestment %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>Interest Expense Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>Debt ratio %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>Net worth/Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>Long-term fund suitability ratio (A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>Borrowing dependency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating profit/Paid-in capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>Net profit before tax/Paid-in capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>Inventory and accounts receivable/Net value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating profit per person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>Working Capital to Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>Quick Assets/Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>Cash/Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>Current Liability to Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating Funds to Liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>Inventory/Working Capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>Current Liabilities/Liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>Working Capital/Equity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>Current Liabilities/Equity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>Retained Earnings to Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>Total income/Total expense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>Total expense/Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>Working capitcal Turnover Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>Cash Flow to Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>Current Liability to Equity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>Equity to Long-term Liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>Cash Flow to Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>Cash Flow to Liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>CFO to Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>Cash Flow to Equity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>Current Liability to Current Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>Net Income to Total Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>No-credit Interval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>Gross Profit to Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>Net Income to Stockholder's Equity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>Liability to Equity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>Degree of Financial Leverage (DFL)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>Interest Coverage Ratio (Interest expense to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>Equity to Liability</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Column_name\n",
       "0      1   ROA(C) before interest and depreciation befor...\n",
       "1      1             ROA(A) before interest and % after tax\n",
       "2      1   ROA(B) before interest and depreciation after...\n",
       "3      1                             Operating Gross Margin\n",
       "5      1                              Operating Profit Rate\n",
       "6      1                          Pre-tax net Interest Rate\n",
       "7      1                        After-tax net Interest Rate\n",
       "8      1        Non-industry income and expenditure/revenue\n",
       "9      1               Continuous interest rate (after tax)\n",
       "12     1                                     Cash flow rate\n",
       "15     1                            Net Value Per Share (B)\n",
       "16     1                            Net Value Per Share (A)\n",
       "17     1                            Net Value Per Share (C)\n",
       "18     1            Persistent EPS in the Last Four Seasons\n",
       "22     1           Per Share Net profit before tax (Yuan ¥)\n",
       "23     1            Realized Sales Gross Profit Growth Rate\n",
       "24     1                       Operating Profit Growth Rate\n",
       "26     1                     Regular Net Profit Growth Rate\n",
       "27     1                  Continuous Net Profit Growth Rate\n",
       "28     1                            Total Asset Growth Rate\n",
       "30     1               Total Asset Return Growth Rate Ratio\n",
       "31     1                                Cash Reinvestment %\n",
       "34     1                             Interest Expense Ratio\n",
       "36     1                                       Debt ratio %\n",
       "37     1                                   Net worth/Assets\n",
       "38     1               Long-term fund suitability ratio (A)\n",
       "39     1                               Borrowing dependency\n",
       "41     1                   Operating profit/Paid-in capital\n",
       "42     1              Net profit before tax/Paid-in capital\n",
       "43     1        Inventory and accounts receivable/Net value\n",
       "51     1                        Operating profit per person\n",
       "53     1                    Working Capital to Total Assets\n",
       "54     1                          Quick Assets/Total Assets\n",
       "56     1                                  Cash/Total Assets\n",
       "59     1                        Current Liability to Assets\n",
       "60     1                       Operating Funds to Liability\n",
       "61     1                          Inventory/Working Capital\n",
       "63     1                      Current Liabilities/Liability\n",
       "64     1                             Working Capital/Equity\n",
       "65     1                         Current Liabilities/Equity\n",
       "67     1                  Retained Earnings to Total Assets\n",
       "68     1                         Total income/Total expense\n",
       "69     1                               Total expense/Assets\n",
       "72     1                     Working capitcal Turnover Rate\n",
       "74     1                                 Cash Flow to Sales\n",
       "76     1                        Current Liability to Equity\n",
       "77     1                      Equity to Long-term Liability\n",
       "78     1                          Cash Flow to Total Assets\n",
       "79     1                             Cash Flow to Liability\n",
       "80     1                                      CFO to Assets\n",
       "81     1                                Cash Flow to Equity\n",
       "82     1                Current Liability to Current Assets\n",
       "84     1                         Net Income to Total Assets\n",
       "86     1                                 No-credit Interval\n",
       "87     1                              Gross Profit to Sales\n",
       "88     1                 Net Income to Stockholder's Equity\n",
       "89     1                                Liability to Equity\n",
       "90     1                 Degree of Financial Leverage (DFL)\n",
       "91     1   Interest Coverage Ratio (Interest expense to ...\n",
       "92     1                                Equity to Liability"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After we run the algorithm, it labels the top features as 1 and the rest are marked in an increasing order of importance.\n",
    "df_RFE = pd.DataFrame(data = rfe.ranking_, columns=['Rank'])\n",
    "#pd.DataFrame(data = rfe.ranking_, columns=['Rank'])\n",
    "df_RFE['Column_name'] = pd.DataFrame(X1_sc).columns\n",
    "#df.head()\n",
    "df_RFE[df_RFE['Rank']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367f038c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>Continuous interest rate (after tax)</th>\n",
       "      <th>Cash flow rate</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
       "      <th>Operating Profit Growth Rate</th>\n",
       "      <th>Regular Net Profit Growth Rate</th>\n",
       "      <th>Continuous Net Profit Growth Rate</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Total Asset Return Growth Rate Ratio</th>\n",
       "      <th>Cash Reinvestment %</th>\n",
       "      <th>Interest Expense Ratio</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Inventory and accounts receivable/Net value</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Inventory/Working Capital</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Working Capital/Equity</th>\n",
       "      <th>Current Liabilities/Equity</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total income/Total expense</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Working capitcal Turnover Rate</th>\n",
       "      <th>Cash Flow to Sales</th>\n",
       "      <th>Current Liability to Equity</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>Cash Flow to Total Assets</th>\n",
       "      <th>Cash Flow to Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Cash Flow to Equity</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>0.780985</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.169141</td>\n",
       "      <td>0.138736</td>\n",
       "      <td>0.022102</td>\n",
       "      <td>0.848195</td>\n",
       "      <td>0.688979</td>\n",
       "      <td>0.217535</td>\n",
       "      <td>4.980000e+09</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.363725</td>\n",
       "      <td>0.629951</td>\n",
       "      <td>0.207576</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.390284</td>\n",
       "      <td>0.095885</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>0.398036</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>0.672775</td>\n",
       "      <td>0.166673</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.147308</td>\n",
       "      <td>0.334015</td>\n",
       "      <td>0.276920</td>\n",
       "      <td>0.676269</td>\n",
       "      <td>0.721275</td>\n",
       "      <td>0.339077</td>\n",
       "      <td>0.903225</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.593831</td>\n",
       "      <td>0.671568</td>\n",
       "      <td>0.339077</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>0.637555</td>\n",
       "      <td>0.458609</td>\n",
       "      <td>0.520382</td>\n",
       "      <td>0.312905</td>\n",
       "      <td>0.118250</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>0.781506</td>\n",
       "      <td>0.461867</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.169918</td>\n",
       "      <td>0.022080</td>\n",
       "      <td>0.848088</td>\n",
       "      <td>0.689702</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>6.110000e+09</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.376709</td>\n",
       "      <td>0.635172</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.828824</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.093743</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.397725</td>\n",
       "      <td>0.391590</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.127236</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>0.341106</td>\n",
       "      <td>0.289642</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.731975</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.931065</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.593916</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.120916</td>\n",
       "      <td>0.641100</td>\n",
       "      <td>0.459001</td>\n",
       "      <td>0.567101</td>\n",
       "      <td>0.314163</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>0.780284</td>\n",
       "      <td>0.458521</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>0.193713</td>\n",
       "      <td>0.180581</td>\n",
       "      <td>0.142803</td>\n",
       "      <td>0.022760</td>\n",
       "      <td>0.848094</td>\n",
       "      <td>0.689470</td>\n",
       "      <td>0.217601</td>\n",
       "      <td>7.280000e+09</td>\n",
       "      <td>0.264184</td>\n",
       "      <td>0.368913</td>\n",
       "      <td>0.629631</td>\n",
       "      <td>0.207516</td>\n",
       "      <td>0.792484</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.379093</td>\n",
       "      <td>0.092318</td>\n",
       "      <td>0.148036</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>0.381968</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.340201</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.098162</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.277456</td>\n",
       "      <td>0.446027</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.334777</td>\n",
       "      <td>0.909903</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.021387</td>\n",
       "      <td>0.594502</td>\n",
       "      <td>0.671571</td>\n",
       "      <td>0.334777</td>\n",
       "      <td>0.117922</td>\n",
       "      <td>0.642765</td>\n",
       "      <td>0.459254</td>\n",
       "      <td>0.538491</td>\n",
       "      <td>0.314515</td>\n",
       "      <td>0.025346</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.781241</td>\n",
       "      <td>0.465705</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.148603</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.217568</td>\n",
       "      <td>4.880000e+09</td>\n",
       "      <td>0.263371</td>\n",
       "      <td>0.384077</td>\n",
       "      <td>0.630228</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.397925</td>\n",
       "      <td>0.378497</td>\n",
       "      <td>0.725754</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>0.098715</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.276580</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.729825</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>0.906902</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>0.593889</td>\n",
       "      <td>0.671519</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>0.120760</td>\n",
       "      <td>0.579039</td>\n",
       "      <td>0.448518</td>\n",
       "      <td>0.604105</td>\n",
       "      <td>0.302382</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>0.781550</td>\n",
       "      <td>0.462746</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.168412</td>\n",
       "      <td>0.022096</td>\n",
       "      <td>0.848258</td>\n",
       "      <td>0.689697</td>\n",
       "      <td>0.217626</td>\n",
       "      <td>5.510000e+09</td>\n",
       "      <td>0.265218</td>\n",
       "      <td>0.379690</td>\n",
       "      <td>0.636055</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>0.375025</td>\n",
       "      <td>0.096927</td>\n",
       "      <td>0.167461</td>\n",
       "      <td>0.400079</td>\n",
       "      <td>0.394371</td>\n",
       "      <td>0.751822</td>\n",
       "      <td>0.260330</td>\n",
       "      <td>0.014161</td>\n",
       "      <td>0.110195</td>\n",
       "      <td>0.344639</td>\n",
       "      <td>0.287913</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.330726</td>\n",
       "      <td>0.913850</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>0.593915</td>\n",
       "      <td>0.671563</td>\n",
       "      <td>0.330726</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.622374</td>\n",
       "      <td>0.454411</td>\n",
       "      <td>0.578469</td>\n",
       "      <td>0.311567</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.539468</td>\n",
       "      <td>0.543230</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797409</td>\n",
       "      <td>0.809331</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>0.463734</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.175045</td>\n",
       "      <td>0.216602</td>\n",
       "      <td>0.172102</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.848205</td>\n",
       "      <td>0.689778</td>\n",
       "      <td>0.217635</td>\n",
       "      <td>7.070000e+09</td>\n",
       "      <td>0.264517</td>\n",
       "      <td>0.380155</td>\n",
       "      <td>0.631415</td>\n",
       "      <td>0.124618</td>\n",
       "      <td>0.875382</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.373823</td>\n",
       "      <td>0.098222</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.404804</td>\n",
       "      <td>0.392596</td>\n",
       "      <td>0.817769</td>\n",
       "      <td>0.312840</td>\n",
       "      <td>0.099481</td>\n",
       "      <td>0.103838</td>\n",
       "      <td>0.346224</td>\n",
       "      <td>0.277543</td>\n",
       "      <td>0.786888</td>\n",
       "      <td>0.736716</td>\n",
       "      <td>0.330914</td>\n",
       "      <td>0.925611</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.593985</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.330914</td>\n",
       "      <td>0.112622</td>\n",
       "      <td>0.639806</td>\n",
       "      <td>0.458639</td>\n",
       "      <td>0.587178</td>\n",
       "      <td>0.314063</td>\n",
       "      <td>0.027951</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>0.623620</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.279606</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.566193</td>\n",
       "      <td>0.029890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0.475162</td>\n",
       "      <td>0.538269</td>\n",
       "      <td>0.524172</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.809327</td>\n",
       "      <td>0.303520</td>\n",
       "      <td>0.781586</td>\n",
       "      <td>0.461978</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.216697</td>\n",
       "      <td>0.172780</td>\n",
       "      <td>0.022159</td>\n",
       "      <td>0.848245</td>\n",
       "      <td>0.689734</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>5.220000e+09</td>\n",
       "      <td>0.264730</td>\n",
       "      <td>0.377389</td>\n",
       "      <td>0.631489</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>0.900747</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.372505</td>\n",
       "      <td>0.098572</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.399926</td>\n",
       "      <td>0.393625</td>\n",
       "      <td>0.793387</td>\n",
       "      <td>0.335085</td>\n",
       "      <td>0.080337</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>0.342166</td>\n",
       "      <td>0.277368</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.734584</td>\n",
       "      <td>0.329753</td>\n",
       "      <td>0.932629</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.593954</td>\n",
       "      <td>0.671572</td>\n",
       "      <td>0.329753</td>\n",
       "      <td>0.112329</td>\n",
       "      <td>0.642072</td>\n",
       "      <td>0.459058</td>\n",
       "      <td>0.569498</td>\n",
       "      <td>0.314446</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.566018</td>\n",
       "      <td>0.038284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0.472725</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.520638</td>\n",
       "      <td>0.610444</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>0.781546</td>\n",
       "      <td>0.472189</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.269521</td>\n",
       "      <td>0.210929</td>\n",
       "      <td>0.173232</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.847978</td>\n",
       "      <td>0.689202</td>\n",
       "      <td>0.217547</td>\n",
       "      <td>5.990000e+09</td>\n",
       "      <td>0.263858</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.630612</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.961061</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.100103</td>\n",
       "      <td>0.172287</td>\n",
       "      <td>0.395592</td>\n",
       "      <td>0.393693</td>\n",
       "      <td>0.866047</td>\n",
       "      <td>0.476747</td>\n",
       "      <td>0.412885</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.358847</td>\n",
       "      <td>0.277022</td>\n",
       "      <td>0.553964</td>\n",
       "      <td>0.737432</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.035446</td>\n",
       "      <td>0.594025</td>\n",
       "      <td>0.671564</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.631678</td>\n",
       "      <td>0.452465</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.313353</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.610441</td>\n",
       "      <td>0.840138</td>\n",
       "      <td>0.275789</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>0.097649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0.506264</td>\n",
       "      <td>0.559911</td>\n",
       "      <td>0.554045</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.809399</td>\n",
       "      <td>0.303498</td>\n",
       "      <td>0.781663</td>\n",
       "      <td>0.476123</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.213392</td>\n",
       "      <td>0.228326</td>\n",
       "      <td>0.185584</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>0.854064</td>\n",
       "      <td>0.696113</td>\n",
       "      <td>0.218006</td>\n",
       "      <td>7.250000e+09</td>\n",
       "      <td>0.264409</td>\n",
       "      <td>0.401028</td>\n",
       "      <td>0.630731</td>\n",
       "      <td>0.086979</td>\n",
       "      <td>0.913021</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.369649</td>\n",
       "      <td>0.111722</td>\n",
       "      <td>0.182498</td>\n",
       "      <td>0.401540</td>\n",
       "      <td>0.396735</td>\n",
       "      <td>0.832340</td>\n",
       "      <td>0.353624</td>\n",
       "      <td>0.112238</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>0.380251</td>\n",
       "      <td>0.277353</td>\n",
       "      <td>0.893241</td>\n",
       "      <td>0.736713</td>\n",
       "      <td>0.329294</td>\n",
       "      <td>0.939613</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>0.593997</td>\n",
       "      <td>0.671606</td>\n",
       "      <td>0.329294</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.684857</td>\n",
       "      <td>0.471313</td>\n",
       "      <td>0.678338</td>\n",
       "      <td>0.320118</td>\n",
       "      <td>0.022916</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.607846</td>\n",
       "      <td>0.841084</td>\n",
       "      <td>0.277547</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>0.044009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0.493053</td>\n",
       "      <td>0.570105</td>\n",
       "      <td>0.549548</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.998080</td>\n",
       "      <td>0.801987</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.786079</td>\n",
       "      <td>0.427721</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.220766</td>\n",
       "      <td>0.227758</td>\n",
       "      <td>0.182119</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.848053</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.217605</td>\n",
       "      <td>9.350000e+09</td>\n",
       "      <td>0.264186</td>\n",
       "      <td>0.360102</td>\n",
       "      <td>0.630618</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.985851</td>\n",
       "      <td>0.058476</td>\n",
       "      <td>0.370049</td>\n",
       "      <td>0.092465</td>\n",
       "      <td>0.179911</td>\n",
       "      <td>0.393883</td>\n",
       "      <td>0.385767</td>\n",
       "      <td>0.873759</td>\n",
       "      <td>0.527136</td>\n",
       "      <td>0.238147</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.239585</td>\n",
       "      <td>0.276975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737286</td>\n",
       "      <td>0.326690</td>\n",
       "      <td>0.938005</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.598674</td>\n",
       "      <td>0.672096</td>\n",
       "      <td>0.326690</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.659917</td>\n",
       "      <td>0.483285</td>\n",
       "      <td>0.505531</td>\n",
       "      <td>0.316238</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.626680</td>\n",
       "      <td>0.627408</td>\n",
       "      <td>0.841019</td>\n",
       "      <td>0.275114</td>\n",
       "      <td>0.026793</td>\n",
       "      <td>0.565167</td>\n",
       "      <td>0.233902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "0                                              0.370594          \n",
       "1                                              0.464291          \n",
       "2                                              0.426071          \n",
       "3                                              0.399844          \n",
       "4                                              0.465022          \n",
       "...                                                 ...          \n",
       "6814                                           0.493687          \n",
       "6815                                           0.475162          \n",
       "6816                                           0.472725          \n",
       "6817                                           0.506264          \n",
       "6818                                           0.493053          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "0                                    0.424389   \n",
       "1                                    0.538214   \n",
       "2                                    0.499019   \n",
       "3                                    0.451265   \n",
       "4                                    0.538432   \n",
       "...                                       ...   \n",
       "6814                                 0.539468   \n",
       "6815                                 0.538269   \n",
       "6816                                 0.533744   \n",
       "6817                                 0.559911   \n",
       "6818                                 0.570105   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "0                                              0.405750    \n",
       "1                                              0.516730    \n",
       "2                                              0.472295    \n",
       "3                                              0.457733    \n",
       "4                                              0.522298    \n",
       "...                                                 ...    \n",
       "6814                                           0.543230    \n",
       "6815                                           0.524172    \n",
       "6816                                           0.520638    \n",
       "6817                                           0.554045    \n",
       "6818                                           0.549548    \n",
       "\n",
       "       Operating Gross Margin   Operating Profit Rate  \\\n",
       "0                    0.601457                0.998969   \n",
       "1                    0.610235                0.998946   \n",
       "2                    0.601450                0.998857   \n",
       "3                    0.583541                0.998700   \n",
       "4                    0.598783                0.998973   \n",
       "...                       ...                     ...   \n",
       "6814                 0.604455                0.998992   \n",
       "6815                 0.598308                0.998992   \n",
       "6816                 0.610444                0.998984   \n",
       "6817                 0.607850                0.999074   \n",
       "6818                 0.627409                0.998080   \n",
       "\n",
       "       Pre-tax net Interest Rate   After-tax net Interest Rate  \\\n",
       "0                       0.796887                      0.808809   \n",
       "1                       0.797380                      0.809301   \n",
       "2                       0.796403                      0.808388   \n",
       "3                       0.796967                      0.808966   \n",
       "4                       0.797366                      0.809304   \n",
       "...                          ...                           ...   \n",
       "6814                    0.797409                      0.809331   \n",
       "6815                    0.797414                      0.809327   \n",
       "6816                    0.797401                      0.809317   \n",
       "6817                    0.797500                      0.809399   \n",
       "6818                    0.801987                      0.813800   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  \\\n",
       "0                                         0.302646   \n",
       "1                                         0.303556   \n",
       "2                                         0.302035   \n",
       "3                                         0.303350   \n",
       "4                                         0.303475   \n",
       "...                                            ...   \n",
       "6814                                      0.303510   \n",
       "6815                                      0.303520   \n",
       "6816                                      0.303512   \n",
       "6817                                      0.303498   \n",
       "6818                                      0.313415   \n",
       "\n",
       "       Continuous interest rate (after tax)   Cash flow rate  \\\n",
       "0                                  0.780985         0.458143   \n",
       "1                                  0.781506         0.461867   \n",
       "2                                  0.780284         0.458521   \n",
       "3                                  0.781241         0.465705   \n",
       "4                                  0.781550         0.462746   \n",
       "...                                     ...              ...   \n",
       "6814                               0.781588         0.463734   \n",
       "6815                               0.781586         0.461978   \n",
       "6816                               0.781546         0.472189   \n",
       "6817                               0.781663         0.476123   \n",
       "6818                               0.786079         0.427721   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "0                     0.147950                  0.147950   \n",
       "1                     0.182251                  0.182251   \n",
       "2                     0.177911                  0.177911   \n",
       "3                     0.154187                  0.154187   \n",
       "4                     0.167502                  0.167502   \n",
       "...                        ...                       ...   \n",
       "6814                  0.175045                  0.175045   \n",
       "6815                  0.181324                  0.181324   \n",
       "6816                  0.269521                  0.269521   \n",
       "6817                  0.213392                  0.213392   \n",
       "6818                  0.220766                  0.220766   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "0                     0.147950                                  0.169141   \n",
       "1                     0.182251                                  0.208944   \n",
       "2                     0.193713                                  0.180581   \n",
       "3                     0.154187                                  0.193722   \n",
       "4                     0.167502                                  0.212537   \n",
       "...                        ...                                       ...   \n",
       "6814                  0.175045                                  0.216602   \n",
       "6815                  0.181324                                  0.216697   \n",
       "6816                  0.269521                                  0.210929   \n",
       "6817                  0.213392                                  0.228326   \n",
       "6818                  0.220766                                  0.227758   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)  \\\n",
       "0                                      0.138736   \n",
       "1                                      0.169918   \n",
       "2                                      0.142803   \n",
       "3                                      0.148603   \n",
       "4                                      0.168412   \n",
       "...                                         ...   \n",
       "6814                                   0.172102   \n",
       "6815                                   0.172780   \n",
       "6816                                   0.173232   \n",
       "6817                                   0.185584   \n",
       "6818                                   0.182119   \n",
       "\n",
       "       Realized Sales Gross Profit Growth Rate   Operating Profit Growth Rate  \\\n",
       "0                                     0.022102                       0.848195   \n",
       "1                                     0.022080                       0.848088   \n",
       "2                                     0.022760                       0.848094   \n",
       "3                                     0.022046                       0.848005   \n",
       "4                                     0.022096                       0.848258   \n",
       "...                                        ...                            ...   \n",
       "6814                                  0.022374                       0.848205   \n",
       "6815                                  0.022159                       0.848245   \n",
       "6816                                  0.022068                       0.847978   \n",
       "6817                                  0.022350                       0.854064   \n",
       "6818                                  0.025316                       0.848053   \n",
       "\n",
       "       Regular Net Profit Growth Rate   Continuous Net Profit Growth Rate  \\\n",
       "0                            0.688979                            0.217535   \n",
       "1                            0.689702                            0.217620   \n",
       "2                            0.689470                            0.217601   \n",
       "3                            0.689110                            0.217568   \n",
       "4                            0.689697                            0.217626   \n",
       "...                               ...                                 ...   \n",
       "6814                         0.689778                            0.217635   \n",
       "6815                         0.689734                            0.217631   \n",
       "6816                         0.689202                            0.217547   \n",
       "6817                         0.696113                            0.218006   \n",
       "6818                         0.689527                            0.217605   \n",
       "\n",
       "       Total Asset Growth Rate   Total Asset Return Growth Rate Ratio  \\\n",
       "0                 4.980000e+09                               0.263100   \n",
       "1                 6.110000e+09                               0.264516   \n",
       "2                 7.280000e+09                               0.264184   \n",
       "3                 4.880000e+09                               0.263371   \n",
       "4                 5.510000e+09                               0.265218   \n",
       "...                        ...                                    ...   \n",
       "6814              7.070000e+09                               0.264517   \n",
       "6815              5.220000e+09                               0.264730   \n",
       "6816              5.990000e+09                               0.263858   \n",
       "6817              7.250000e+09                               0.264409   \n",
       "6818              9.350000e+09                               0.264186   \n",
       "\n",
       "       Cash Reinvestment %   Interest Expense Ratio   Debt ratio %  \\\n",
       "0                 0.363725                 0.629951       0.207576   \n",
       "1                 0.376709                 0.635172       0.171176   \n",
       "2                 0.368913                 0.629631       0.207516   \n",
       "3                 0.384077                 0.630228       0.151465   \n",
       "4                 0.379690                 0.636055       0.106509   \n",
       "...                    ...                      ...            ...   \n",
       "6814              0.380155                 0.631415       0.124618   \n",
       "6815              0.377389                 0.631489       0.099253   \n",
       "6816              0.379392                 0.630612       0.038939   \n",
       "6817              0.401028                 0.630731       0.086979   \n",
       "6818              0.360102                 0.630618       0.014149   \n",
       "\n",
       "       Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "0              0.792424                               0.005024   \n",
       "1              0.828824                               0.005059   \n",
       "2              0.792484                               0.005100   \n",
       "3              0.848535                               0.005047   \n",
       "4              0.893491                               0.005303   \n",
       "...                 ...                                    ...   \n",
       "6814           0.875382                               0.005150   \n",
       "6815           0.900747                               0.006772   \n",
       "6816           0.961061                               0.009149   \n",
       "6817           0.913021                               0.005529   \n",
       "6818           0.985851                               0.058476   \n",
       "\n",
       "       Borrowing dependency   Operating profit/Paid-in capital  \\\n",
       "0                  0.390284                           0.095885   \n",
       "1                  0.376760                           0.093743   \n",
       "2                  0.379093                           0.092318   \n",
       "3                  0.379743                           0.077727   \n",
       "4                  0.375025                           0.096927   \n",
       "...                     ...                                ...   \n",
       "6814               0.373823                           0.098222   \n",
       "6815               0.372505                           0.098572   \n",
       "6816               0.369637                           0.100103   \n",
       "6817               0.369649                           0.111722   \n",
       "6818               0.370049                           0.092465   \n",
       "\n",
       "       Net profit before tax/Paid-in capital  \\\n",
       "0                                   0.137757   \n",
       "1                                   0.168962   \n",
       "2                                   0.148036   \n",
       "3                                   0.147561   \n",
       "4                                   0.167461   \n",
       "...                                      ...   \n",
       "6814                                0.171111   \n",
       "6815                                0.171805   \n",
       "6816                                0.172287   \n",
       "6817                                0.182498   \n",
       "6818                                0.179911   \n",
       "\n",
       "       Inventory and accounts receivable/Net value  \\\n",
       "0                                         0.398036   \n",
       "1                                         0.397725   \n",
       "2                                         0.406580   \n",
       "3                                         0.397925   \n",
       "4                                         0.400079   \n",
       "...                                            ...   \n",
       "6814                                      0.404804   \n",
       "6815                                      0.399926   \n",
       "6816                                      0.395592   \n",
       "6817                                      0.401540   \n",
       "6818                                      0.393883   \n",
       "\n",
       "       Operating profit per person   Working Capital to Total Assets  \\\n",
       "0                         0.392913                          0.672775   \n",
       "1                         0.391590                          0.751111   \n",
       "2                         0.381968                          0.829502   \n",
       "3                         0.378497                          0.725754   \n",
       "4                         0.394371                          0.751822   \n",
       "...                            ...                               ...   \n",
       "6814                      0.392596                          0.817769   \n",
       "6815                      0.393625                          0.793387   \n",
       "6816                      0.393693                          0.866047   \n",
       "6817                      0.396735                          0.832340   \n",
       "6818                      0.385767                          0.873759   \n",
       "\n",
       "       Quick Assets/Total Assets   Cash/Total Assets  \\\n",
       "0                       0.166673            0.004094   \n",
       "1                       0.127236            0.014948   \n",
       "2                       0.340201            0.000991   \n",
       "3                       0.161575            0.018851   \n",
       "4                       0.260330            0.014161   \n",
       "...                          ...                 ...   \n",
       "6814                    0.312840            0.099481   \n",
       "6815                    0.335085            0.080337   \n",
       "6816                    0.476747            0.412885   \n",
       "6817                    0.353624            0.112238   \n",
       "6818                    0.527136            0.238147   \n",
       "\n",
       "       Current Liability to Assets   Operating Funds to Liability  \\\n",
       "0                         0.147308                       0.334015   \n",
       "1                         0.056963                       0.341106   \n",
       "2                         0.098162                       0.336731   \n",
       "3                         0.098715                       0.348716   \n",
       "4                         0.110195                       0.344639   \n",
       "...                            ...                            ...   \n",
       "6814                      0.103838                       0.346224   \n",
       "6815                      0.089901                       0.342166   \n",
       "6816                      0.024414                       0.358847   \n",
       "6817                      0.083199                       0.380251   \n",
       "6818                      0.018517                       0.239585   \n",
       "\n",
       "       Inventory/Working Capital   Current Liabilities/Liability  \\\n",
       "0                       0.276920                        0.676269   \n",
       "1                       0.289642                        0.308589   \n",
       "2                       0.277456                        0.446027   \n",
       "3                       0.276580                        0.615848   \n",
       "4                       0.287913                        0.975007   \n",
       "...                          ...                             ...   \n",
       "6814                    0.277543                        0.786888   \n",
       "6815                    0.277368                        0.849898   \n",
       "6816                    0.277022                        0.553964   \n",
       "6817                    0.277353                        0.893241   \n",
       "6818                    0.276975                        1.000000   \n",
       "\n",
       "       Working Capital/Equity   Current Liabilities/Equity  \\\n",
       "0                    0.721275                     0.339077   \n",
       "1                    0.731975                     0.329740   \n",
       "2                    0.742729                     0.334777   \n",
       "3                    0.729825                     0.331509   \n",
       "4                    0.732000                     0.330726   \n",
       "...                       ...                          ...   \n",
       "6814                 0.736716                     0.330914   \n",
       "6815                 0.734584                     0.329753   \n",
       "6816                 0.737432                     0.326921   \n",
       "6817                 0.736713                     0.329294   \n",
       "6818                 0.737286                     0.326690   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total income/Total expense  \\\n",
       "0                               0.903225                     0.002022   \n",
       "1                               0.931065                     0.002226   \n",
       "2                               0.909903                     0.002060   \n",
       "3                               0.906902                     0.001831   \n",
       "4                               0.913850                     0.002224   \n",
       "...                                  ...                          ...   \n",
       "6814                            0.925611                     0.002266   \n",
       "6815                            0.932629                     0.002288   \n",
       "6816                            0.932000                     0.002239   \n",
       "6817                            0.939613                     0.002395   \n",
       "6818                            0.938005                     0.002791   \n",
       "\n",
       "       Total expense/Assets   Working capitcal Turnover Rate  \\\n",
       "0                  0.064856                         0.593831   \n",
       "1                  0.025516                         0.593916   \n",
       "2                  0.021387                         0.594502   \n",
       "3                  0.024161                         0.593889   \n",
       "4                  0.026385                         0.593915   \n",
       "...                     ...                              ...   \n",
       "6814               0.019060                         0.593985   \n",
       "6815               0.011118                         0.593954   \n",
       "6816               0.035446                         0.594025   \n",
       "6817               0.016443                         0.593997   \n",
       "6818               0.006089                         0.598674   \n",
       "\n",
       "       Cash Flow to Sales   Current Liability to Equity  \\\n",
       "0                0.671568                      0.339077   \n",
       "1                0.671570                      0.329740   \n",
       "2                0.671571                      0.334777   \n",
       "3                0.671519                      0.331509   \n",
       "4                0.671563                      0.330726   \n",
       "...                   ...                           ...   \n",
       "6814             0.671570                      0.330914   \n",
       "6815             0.671572                      0.329753   \n",
       "6816             0.671564                      0.326921   \n",
       "6817             0.671606                      0.329294   \n",
       "6818             0.672096                      0.326690   \n",
       "\n",
       "       Equity to Long-term Liability   Cash Flow to Total Assets  \\\n",
       "0                           0.126549                    0.637555   \n",
       "1                           0.120916                    0.641100   \n",
       "2                           0.117922                    0.642765   \n",
       "3                           0.120760                    0.579039   \n",
       "4                           0.110933                    0.622374   \n",
       "...                              ...                         ...   \n",
       "6814                        0.112622                    0.639806   \n",
       "6815                        0.112329                    0.642072   \n",
       "6816                        0.110933                    0.631678   \n",
       "6817                        0.110957                    0.684857   \n",
       "6818                        0.110933                    0.659917   \n",
       "\n",
       "       Cash Flow to Liability   CFO to Assets   Cash Flow to Equity  \\\n",
       "0                    0.458609        0.520382              0.312905   \n",
       "1                    0.459001        0.567101              0.314163   \n",
       "2                    0.459254        0.538491              0.314515   \n",
       "3                    0.448518        0.604105              0.302382   \n",
       "4                    0.454411        0.578469              0.311567   \n",
       "...                       ...             ...                   ...   \n",
       "6814                 0.458639        0.587178              0.314063   \n",
       "6815                 0.459058        0.569498              0.314446   \n",
       "6816                 0.452465        0.589341              0.313353   \n",
       "6817                 0.471313        0.678338              0.320118   \n",
       "6818                 0.483285        0.505531              0.316238   \n",
       "\n",
       "       Current Liability to Current Assets   Net Income to Total Assets  \\\n",
       "0                                 0.118250                     0.716845   \n",
       "1                                 0.047775                     0.795297   \n",
       "2                                 0.025346                     0.774670   \n",
       "3                                 0.067250                     0.739555   \n",
       "4                                 0.047725                     0.795016   \n",
       "...                                    ...                          ...   \n",
       "6814                              0.027951                     0.799927   \n",
       "6815                              0.031470                     0.799748   \n",
       "6816                              0.007542                     0.797778   \n",
       "6817                              0.022916                     0.811808   \n",
       "6818                              0.005579                     0.815956   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "0                0.622879                0.601453   \n",
       "1                0.623652                0.610237   \n",
       "2                0.623841                0.601449   \n",
       "3                0.622929                0.583538   \n",
       "4                0.623521                0.598782   \n",
       "...                   ...                     ...   \n",
       "6814             0.623620                0.604455   \n",
       "6815             0.623931                0.598306   \n",
       "6816             0.624156                0.610441   \n",
       "6817             0.623957                0.607846   \n",
       "6818             0.626680                0.627408   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                                0.827890              0.290202   \n",
       "1                                0.839969              0.283846   \n",
       "2                                0.836774              0.290189   \n",
       "3                                0.834697              0.281721   \n",
       "4                                0.839973              0.278514   \n",
       "...                                   ...                   ...   \n",
       "6814                             0.840359              0.279606   \n",
       "6815                             0.840306              0.278132   \n",
       "6816                             0.840138              0.275789   \n",
       "6817                             0.841084              0.277547   \n",
       "6818                             0.841019              0.275114   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "0                                0.026601   \n",
       "1                                0.264577   \n",
       "2                                0.026555   \n",
       "3                                0.026697   \n",
       "4                                0.024752   \n",
       "...                                   ...   \n",
       "6814                             0.027064   \n",
       "6815                             0.027009   \n",
       "6816                             0.026791   \n",
       "6817                             0.026822   \n",
       "6818                             0.026793   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)  \\\n",
       "0                                              0.564050     \n",
       "1                                              0.570175     \n",
       "2                                              0.563706     \n",
       "3                                              0.564663     \n",
       "4                                              0.575617     \n",
       "...                                                 ...     \n",
       "6814                                           0.566193     \n",
       "6815                                           0.566018     \n",
       "6816                                           0.565158     \n",
       "6817                                           0.565302     \n",
       "6818                                           0.565167     \n",
       "\n",
       "       Equity to Liability  \n",
       "0                 0.016469  \n",
       "1                 0.020794  \n",
       "2                 0.016474  \n",
       "3                 0.023982  \n",
       "4                 0.035490  \n",
       "...                    ...  \n",
       "6814              0.029890  \n",
       "6815              0.038284  \n",
       "6816              0.097649  \n",
       "6817              0.044009  \n",
       "6818              0.233902  \n",
       "\n",
       "[6819 rows x 60 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFE_df = pd.DataFrame(X1, columns=sel.get_feature_names_out())\n",
    "RFE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb67523",
   "metadata": {},
   "source": [
    "# Balancing the dataset: upsampling and downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4b6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.concat([RFE_df, y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad462f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_1['Bankrupt?'].astype('int')\n",
    "X = df_1.drop(['Bankrupt?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e16158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4672631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5455, 61)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN = pd.concat([X_train, y_train], axis=1)\n",
    "TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57cbec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>Continuous interest rate (after tax)</th>\n",
       "      <th>Cash flow rate</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
       "      <th>Operating Profit Growth Rate</th>\n",
       "      <th>Regular Net Profit Growth Rate</th>\n",
       "      <th>Continuous Net Profit Growth Rate</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Total Asset Return Growth Rate Ratio</th>\n",
       "      <th>Cash Reinvestment %</th>\n",
       "      <th>Interest Expense Ratio</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Inventory and accounts receivable/Net value</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Inventory/Working Capital</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Working Capital/Equity</th>\n",
       "      <th>Current Liabilities/Equity</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total income/Total expense</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Working capitcal Turnover Rate</th>\n",
       "      <th>Cash Flow to Sales</th>\n",
       "      <th>Current Liability to Equity</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>Cash Flow to Total Assets</th>\n",
       "      <th>Cash Flow to Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Cash Flow to Equity</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Equity to Liability</th>\n",
       "      <th>Bankrupt?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>0.519768</td>\n",
       "      <td>0.538051</td>\n",
       "      <td>0.558060</td>\n",
       "      <td>0.622595</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>0.797430</td>\n",
       "      <td>0.809328</td>\n",
       "      <td>0.303392</td>\n",
       "      <td>0.781587</td>\n",
       "      <td>0.465974</td>\n",
       "      <td>0.174877</td>\n",
       "      <td>0.174877</td>\n",
       "      <td>0.174877</td>\n",
       "      <td>0.215940</td>\n",
       "      <td>0.172629</td>\n",
       "      <td>0.022061</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.689149</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>7.380000e+09</td>\n",
       "      <td>0.263509</td>\n",
       "      <td>0.386023</td>\n",
       "      <td>0.632316</td>\n",
       "      <td>0.097197</td>\n",
       "      <td>0.902803</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.373542</td>\n",
       "      <td>0.102562</td>\n",
       "      <td>0.171631</td>\n",
       "      <td>0.396874</td>\n",
       "      <td>0.398408</td>\n",
       "      <td>0.792863</td>\n",
       "      <td>0.474841</td>\n",
       "      <td>0.100335</td>\n",
       "      <td>0.095271</td>\n",
       "      <td>0.353342</td>\n",
       "      <td>0.276990</td>\n",
       "      <td>0.919886</td>\n",
       "      <td>0.734525</td>\n",
       "      <td>0.329928</td>\n",
       "      <td>0.937487</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.041187</td>\n",
       "      <td>0.593967</td>\n",
       "      <td>0.671564</td>\n",
       "      <td>0.329928</td>\n",
       "      <td>0.112116</td>\n",
       "      <td>0.634688</td>\n",
       "      <td>0.457146</td>\n",
       "      <td>0.605097</td>\n",
       "      <td>0.313420</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>0.623977</td>\n",
       "      <td>0.622592</td>\n",
       "      <td>0.840240</td>\n",
       "      <td>0.278029</td>\n",
       "      <td>0.027053</td>\n",
       "      <td>0.566160</td>\n",
       "      <td>0.039145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.781241</td>\n",
       "      <td>0.465705</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.148603</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.217568</td>\n",
       "      <td>4.880000e+09</td>\n",
       "      <td>0.263371</td>\n",
       "      <td>0.384077</td>\n",
       "      <td>0.630228</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.397925</td>\n",
       "      <td>0.378497</td>\n",
       "      <td>0.725754</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>0.098715</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.276580</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.729825</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>0.906902</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>0.593889</td>\n",
       "      <td>0.671519</td>\n",
       "      <td>0.331509</td>\n",
       "      <td>0.120760</td>\n",
       "      <td>0.579039</td>\n",
       "      <td>0.448518</td>\n",
       "      <td>0.604105</td>\n",
       "      <td>0.302382</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641</th>\n",
       "      <td>0.337640</td>\n",
       "      <td>0.254307</td>\n",
       "      <td>0.378446</td>\n",
       "      <td>0.590842</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.796943</td>\n",
       "      <td>0.808897</td>\n",
       "      <td>0.302953</td>\n",
       "      <td>0.781340</td>\n",
       "      <td>0.465222</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>0.164792</td>\n",
       "      <td>0.091738</td>\n",
       "      <td>0.018849</td>\n",
       "      <td>0.847953</td>\n",
       "      <td>0.687465</td>\n",
       "      <td>0.217496</td>\n",
       "      <td>2.570000e+08</td>\n",
       "      <td>0.261368</td>\n",
       "      <td>0.445615</td>\n",
       "      <td>0.630567</td>\n",
       "      <td>0.268706</td>\n",
       "      <td>0.731294</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.402534</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>0.090634</td>\n",
       "      <td>0.471817</td>\n",
       "      <td>0.378983</td>\n",
       "      <td>0.740426</td>\n",
       "      <td>0.465562</td>\n",
       "      <td>0.108758</td>\n",
       "      <td>0.239915</td>\n",
       "      <td>0.350564</td>\n",
       "      <td>0.272963</td>\n",
       "      <td>0.858173</td>\n",
       "      <td>0.727664</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.852516</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>0.136224</td>\n",
       "      <td>0.593912</td>\n",
       "      <td>0.671564</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.144985</td>\n",
       "      <td>0.609092</td>\n",
       "      <td>0.456023</td>\n",
       "      <td>0.649732</td>\n",
       "      <td>0.282216</td>\n",
       "      <td>0.050780</td>\n",
       "      <td>0.557733</td>\n",
       "      <td>0.623302</td>\n",
       "      <td>0.590838</td>\n",
       "      <td>0.726888</td>\n",
       "      <td>0.336515</td>\n",
       "      <td>0.026777</td>\n",
       "      <td>0.565092</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.469458</td>\n",
       "      <td>0.539250</td>\n",
       "      <td>0.526902</td>\n",
       "      <td>0.602091</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.797387</td>\n",
       "      <td>0.809365</td>\n",
       "      <td>0.303452</td>\n",
       "      <td>0.781610</td>\n",
       "      <td>0.464348</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.216224</td>\n",
       "      <td>0.170144</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.848401</td>\n",
       "      <td>0.689822</td>\n",
       "      <td>0.217640</td>\n",
       "      <td>7.460000e+09</td>\n",
       "      <td>0.264212</td>\n",
       "      <td>0.379271</td>\n",
       "      <td>0.636432</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>0.867944</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.377480</td>\n",
       "      <td>0.096894</td>\n",
       "      <td>0.169165</td>\n",
       "      <td>0.396366</td>\n",
       "      <td>0.393207</td>\n",
       "      <td>0.742227</td>\n",
       "      <td>0.077033</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>0.053358</td>\n",
       "      <td>0.344074</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.375513</td>\n",
       "      <td>0.731298</td>\n",
       "      <td>0.328724</td>\n",
       "      <td>0.931610</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.007297</td>\n",
       "      <td>0.593893</td>\n",
       "      <td>0.671516</td>\n",
       "      <td>0.328724</td>\n",
       "      <td>0.125439</td>\n",
       "      <td>0.616598</td>\n",
       "      <td>0.454211</td>\n",
       "      <td>0.579259</td>\n",
       "      <td>0.310132</td>\n",
       "      <td>0.058460</td>\n",
       "      <td>0.799720</td>\n",
       "      <td>0.622374</td>\n",
       "      <td>0.602089</td>\n",
       "      <td>0.840356</td>\n",
       "      <td>0.280124</td>\n",
       "      <td>0.032780</td>\n",
       "      <td>0.569441</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.475469</td>\n",
       "      <td>0.456074</td>\n",
       "      <td>0.593688</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>0.797057</td>\n",
       "      <td>0.809027</td>\n",
       "      <td>0.303147</td>\n",
       "      <td>0.781236</td>\n",
       "      <td>0.463382</td>\n",
       "      <td>0.146559</td>\n",
       "      <td>0.146559</td>\n",
       "      <td>0.146559</td>\n",
       "      <td>0.180108</td>\n",
       "      <td>0.142577</td>\n",
       "      <td>0.022112</td>\n",
       "      <td>0.848035</td>\n",
       "      <td>0.689368</td>\n",
       "      <td>0.217594</td>\n",
       "      <td>5.460000e+09</td>\n",
       "      <td>0.263864</td>\n",
       "      <td>0.394769</td>\n",
       "      <td>0.628938</td>\n",
       "      <td>0.231702</td>\n",
       "      <td>0.768298</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.404609</td>\n",
       "      <td>0.085463</td>\n",
       "      <td>0.141603</td>\n",
       "      <td>0.417332</td>\n",
       "      <td>0.370378</td>\n",
       "      <td>0.708529</td>\n",
       "      <td>0.311743</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.197270</td>\n",
       "      <td>0.345557</td>\n",
       "      <td>0.276369</td>\n",
       "      <td>0.815745</td>\n",
       "      <td>0.724078</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>0.910702</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.037014</td>\n",
       "      <td>0.593871</td>\n",
       "      <td>0.671575</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>0.125988</td>\n",
       "      <td>0.646103</td>\n",
       "      <td>0.459627</td>\n",
       "      <td>0.601032</td>\n",
       "      <td>0.315905</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>0.749555</td>\n",
       "      <td>0.622808</td>\n",
       "      <td>0.593690</td>\n",
       "      <td>0.830318</td>\n",
       "      <td>0.298268</td>\n",
       "      <td>0.026437</td>\n",
       "      <td>0.562615</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>0.360796</td>\n",
       "      <td>0.400403</td>\n",
       "      <td>0.398094</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.998804</td>\n",
       "      <td>0.796708</td>\n",
       "      <td>0.808651</td>\n",
       "      <td>0.302679</td>\n",
       "      <td>0.780851</td>\n",
       "      <td>0.458747</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.166210</td>\n",
       "      <td>0.133313</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.848012</td>\n",
       "      <td>0.689225</td>\n",
       "      <td>0.217572</td>\n",
       "      <td>4.670000e+09</td>\n",
       "      <td>0.263319</td>\n",
       "      <td>0.371987</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>0.158660</td>\n",
       "      <td>0.841340</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.381373</td>\n",
       "      <td>0.084135</td>\n",
       "      <td>0.132268</td>\n",
       "      <td>0.397559</td>\n",
       "      <td>0.380916</td>\n",
       "      <td>0.717092</td>\n",
       "      <td>0.106695</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.087106</td>\n",
       "      <td>0.336488</td>\n",
       "      <td>0.276752</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.728948</td>\n",
       "      <td>0.331127</td>\n",
       "      <td>0.880488</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>0.593875</td>\n",
       "      <td>0.671535</td>\n",
       "      <td>0.331127</td>\n",
       "      <td>0.126967</td>\n",
       "      <td>0.603664</td>\n",
       "      <td>0.452955</td>\n",
       "      <td>0.543242</td>\n",
       "      <td>0.306692</td>\n",
       "      <td>0.084685</td>\n",
       "      <td>0.701107</td>\n",
       "      <td>0.623389</td>\n",
       "      <td>0.597026</td>\n",
       "      <td>0.830591</td>\n",
       "      <td>0.282426</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.564509</td>\n",
       "      <td>0.022732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>0.781506</td>\n",
       "      <td>0.461867</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.182251</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.169918</td>\n",
       "      <td>0.022080</td>\n",
       "      <td>0.848088</td>\n",
       "      <td>0.689702</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>6.110000e+09</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.376709</td>\n",
       "      <td>0.635172</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.828824</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.093743</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.397725</td>\n",
       "      <td>0.391590</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.127236</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>0.341106</td>\n",
       "      <td>0.289642</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.731975</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.931065</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.593916</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.329740</td>\n",
       "      <td>0.120916</td>\n",
       "      <td>0.641100</td>\n",
       "      <td>0.459001</td>\n",
       "      <td>0.567101</td>\n",
       "      <td>0.314163</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>0.464047</td>\n",
       "      <td>0.527257</td>\n",
       "      <td>0.517533</td>\n",
       "      <td>0.610711</td>\n",
       "      <td>0.998977</td>\n",
       "      <td>0.797348</td>\n",
       "      <td>0.809280</td>\n",
       "      <td>0.303436</td>\n",
       "      <td>0.781531</td>\n",
       "      <td>0.461711</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.181703</td>\n",
       "      <td>0.209133</td>\n",
       "      <td>0.165775</td>\n",
       "      <td>0.022073</td>\n",
       "      <td>0.847979</td>\n",
       "      <td>0.688642</td>\n",
       "      <td>0.217472</td>\n",
       "      <td>7.290000e+09</td>\n",
       "      <td>0.263763</td>\n",
       "      <td>0.377948</td>\n",
       "      <td>0.624179</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.379319</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>0.164822</td>\n",
       "      <td>0.411580</td>\n",
       "      <td>0.392517</td>\n",
       "      <td>0.783541</td>\n",
       "      <td>0.463405</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>0.168455</td>\n",
       "      <td>0.341540</td>\n",
       "      <td>0.277887</td>\n",
       "      <td>0.909853</td>\n",
       "      <td>0.735291</td>\n",
       "      <td>0.337174</td>\n",
       "      <td>0.932147</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.593947</td>\n",
       "      <td>0.671574</td>\n",
       "      <td>0.337174</td>\n",
       "      <td>0.113409</td>\n",
       "      <td>0.644983</td>\n",
       "      <td>0.459572</td>\n",
       "      <td>0.569659</td>\n",
       "      <td>0.315040</td>\n",
       "      <td>0.039624</td>\n",
       "      <td>0.791303</td>\n",
       "      <td>0.623639</td>\n",
       "      <td>0.610706</td>\n",
       "      <td>0.839508</td>\n",
       "      <td>0.284577</td>\n",
       "      <td>0.026062</td>\n",
       "      <td>0.553919</td>\n",
       "      <td>0.020014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>0.446497</td>\n",
       "      <td>0.495475</td>\n",
       "      <td>0.493763</td>\n",
       "      <td>0.606473</td>\n",
       "      <td>0.998907</td>\n",
       "      <td>0.797282</td>\n",
       "      <td>0.809207</td>\n",
       "      <td>0.303468</td>\n",
       "      <td>0.781452</td>\n",
       "      <td>0.463288</td>\n",
       "      <td>0.147529</td>\n",
       "      <td>0.147529</td>\n",
       "      <td>0.147529</td>\n",
       "      <td>0.201191</td>\n",
       "      <td>0.160654</td>\n",
       "      <td>0.022098</td>\n",
       "      <td>0.848073</td>\n",
       "      <td>0.689501</td>\n",
       "      <td>0.217604</td>\n",
       "      <td>5.260000e+09</td>\n",
       "      <td>0.264179</td>\n",
       "      <td>0.382884</td>\n",
       "      <td>0.629674</td>\n",
       "      <td>0.197539</td>\n",
       "      <td>0.802461</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.382879</td>\n",
       "      <td>0.088565</td>\n",
       "      <td>0.159679</td>\n",
       "      <td>0.406262</td>\n",
       "      <td>0.389736</td>\n",
       "      <td>0.706854</td>\n",
       "      <td>0.292874</td>\n",
       "      <td>0.060502</td>\n",
       "      <td>0.177582</td>\n",
       "      <td>0.345573</td>\n",
       "      <td>0.276556</td>\n",
       "      <td>0.859651</td>\n",
       "      <td>0.726538</td>\n",
       "      <td>0.340166</td>\n",
       "      <td>0.898535</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.060988</td>\n",
       "      <td>0.593887</td>\n",
       "      <td>0.671579</td>\n",
       "      <td>0.340166</td>\n",
       "      <td>0.117993</td>\n",
       "      <td>0.655186</td>\n",
       "      <td>0.460867</td>\n",
       "      <td>0.595849</td>\n",
       "      <td>0.317977</td>\n",
       "      <td>0.066816</td>\n",
       "      <td>0.768935</td>\n",
       "      <td>0.623540</td>\n",
       "      <td>0.606475</td>\n",
       "      <td>0.836306</td>\n",
       "      <td>0.287988</td>\n",
       "      <td>0.026529</td>\n",
       "      <td>0.563500</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0.466728</td>\n",
       "      <td>0.490078</td>\n",
       "      <td>0.501097</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.998855</td>\n",
       "      <td>0.797196</td>\n",
       "      <td>0.809109</td>\n",
       "      <td>0.303425</td>\n",
       "      <td>0.781354</td>\n",
       "      <td>0.461146</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.194006</td>\n",
       "      <td>0.155457</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.847129</td>\n",
       "      <td>0.688374</td>\n",
       "      <td>0.217434</td>\n",
       "      <td>6.730000e+09</td>\n",
       "      <td>0.263410</td>\n",
       "      <td>0.375396</td>\n",
       "      <td>0.629541</td>\n",
       "      <td>0.218672</td>\n",
       "      <td>0.781328</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.393737</td>\n",
       "      <td>0.085650</td>\n",
       "      <td>0.156104</td>\n",
       "      <td>0.401894</td>\n",
       "      <td>0.384150</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.007454</td>\n",
       "      <td>0.158285</td>\n",
       "      <td>0.340148</td>\n",
       "      <td>0.276878</td>\n",
       "      <td>0.690745</td>\n",
       "      <td>0.720350</td>\n",
       "      <td>0.341977</td>\n",
       "      <td>0.910031</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.041586</td>\n",
       "      <td>0.593827</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.341977</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>0.639996</td>\n",
       "      <td>0.458929</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.313533</td>\n",
       "      <td>0.101788</td>\n",
       "      <td>0.767057</td>\n",
       "      <td>0.623321</td>\n",
       "      <td>0.607845</td>\n",
       "      <td>0.834982</td>\n",
       "      <td>0.293298</td>\n",
       "      <td>0.026485</td>\n",
       "      <td>0.563103</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "1868                                           0.519768          \n",
       "3                                              0.399844          \n",
       "6641                                           0.337640          \n",
       "233                                            0.469458          \n",
       "1445                                           0.411300          \n",
       "...                                                 ...          \n",
       "1179                                           0.360796          \n",
       "1                                              0.464291          \n",
       "948                                            0.464047          \n",
       "2908                                           0.446497          \n",
       "2022                                           0.466728          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "1868                                 0.538051   \n",
       "3                                    0.451265   \n",
       "6641                                 0.254307   \n",
       "233                                  0.539250   \n",
       "1445                                 0.475469   \n",
       "...                                       ...   \n",
       "1179                                 0.400403   \n",
       "1                                    0.538214   \n",
       "948                                  0.527257   \n",
       "2908                                 0.495475   \n",
       "2022                                 0.490078   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "1868                                           0.558060    \n",
       "3                                              0.457733    \n",
       "6641                                           0.378446    \n",
       "233                                            0.526902    \n",
       "1445                                           0.456074    \n",
       "...                                                 ...    \n",
       "1179                                           0.398094    \n",
       "1                                              0.516730    \n",
       "948                                            0.517533    \n",
       "2908                                           0.493763    \n",
       "2022                                           0.501097    \n",
       "\n",
       "       Operating Gross Margin   Operating Profit Rate  \\\n",
       "1868                 0.622595                0.999066   \n",
       "3                    0.583541                0.998700   \n",
       "6641                 0.590842                0.998869   \n",
       "233                  0.602091                0.999001   \n",
       "1445                 0.593688                0.998872   \n",
       "...                       ...                     ...   \n",
       "1179                 0.597025                0.998804   \n",
       "1                    0.610235                0.998946   \n",
       "948                  0.610711                0.998977   \n",
       "2908                 0.606473                0.998907   \n",
       "2022                 0.607850                0.998855   \n",
       "\n",
       "       Pre-tax net Interest Rate   After-tax net Interest Rate  \\\n",
       "1868                    0.797430                      0.809328   \n",
       "3                       0.796967                      0.808966   \n",
       "6641                    0.796943                      0.808897   \n",
       "233                     0.797387                      0.809365   \n",
       "1445                    0.797057                      0.809027   \n",
       "...                          ...                           ...   \n",
       "1179                    0.796708                      0.808651   \n",
       "1                       0.797380                      0.809301   \n",
       "948                     0.797348                      0.809280   \n",
       "2908                    0.797282                      0.809207   \n",
       "2022                    0.797196                      0.809109   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  \\\n",
       "1868                                      0.303392   \n",
       "3                                         0.303350   \n",
       "6641                                      0.302953   \n",
       "233                                       0.303452   \n",
       "1445                                      0.303147   \n",
       "...                                            ...   \n",
       "1179                                      0.302679   \n",
       "1                                         0.303556   \n",
       "948                                       0.303436   \n",
       "2908                                      0.303468   \n",
       "2022                                      0.303425   \n",
       "\n",
       "       Continuous interest rate (after tax)   Cash flow rate  \\\n",
       "1868                               0.781587         0.465974   \n",
       "3                                  0.781241         0.465705   \n",
       "6641                               0.781340         0.465222   \n",
       "233                                0.781610         0.464348   \n",
       "1445                               0.781236         0.463382   \n",
       "...                                     ...              ...   \n",
       "1179                               0.780851         0.458747   \n",
       "1                                  0.781506         0.461867   \n",
       "948                                0.781531         0.461711   \n",
       "2908                               0.781452         0.463288   \n",
       "2022                               0.781354         0.461146   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "1868                  0.174877                  0.174877   \n",
       "3                     0.154187                  0.154187   \n",
       "6641                  0.131010                  0.131010   \n",
       "233                   0.177953                  0.177953   \n",
       "1445                  0.146559                  0.146559   \n",
       "...                        ...                       ...   \n",
       "1179                  0.158400                  0.158400   \n",
       "1                     0.182251                  0.182251   \n",
       "948                   0.181703                  0.181703   \n",
       "2908                  0.147529                  0.147529   \n",
       "2022                  0.152206                  0.152206   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "1868                  0.174877                                  0.215940   \n",
       "3                     0.154187                                  0.193722   \n",
       "6641                  0.131010                                  0.164792   \n",
       "233                   0.177953                                  0.216224   \n",
       "1445                  0.146559                                  0.180108   \n",
       "...                        ...                                       ...   \n",
       "1179                  0.158400                                  0.166210   \n",
       "1                     0.182251                                  0.208944   \n",
       "948                   0.181703                                  0.209133   \n",
       "2908                  0.147529                                  0.201191   \n",
       "2022                  0.152206                                  0.194006   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)  \\\n",
       "1868                                   0.172629   \n",
       "3                                      0.148603   \n",
       "6641                                   0.091738   \n",
       "233                                    0.170144   \n",
       "1445                                   0.142577   \n",
       "...                                         ...   \n",
       "1179                                   0.133313   \n",
       "1                                      0.169918   \n",
       "948                                    0.165775   \n",
       "2908                                   0.160654   \n",
       "2022                                   0.155457   \n",
       "\n",
       "       Realized Sales Gross Profit Growth Rate   Operating Profit Growth Rate  \\\n",
       "1868                                  0.022061                       0.847966   \n",
       "3                                     0.022046                       0.848005   \n",
       "6641                                  0.018849                       0.847953   \n",
       "233                                   0.022217                       0.848401   \n",
       "1445                                  0.022112                       0.848035   \n",
       "...                                        ...                            ...   \n",
       "1179                                  0.022140                       0.848012   \n",
       "1                                     0.022080                       0.848088   \n",
       "948                                   0.022073                       0.847979   \n",
       "2908                                  0.022098                       0.848073   \n",
       "2022                                  0.022033                       0.847129   \n",
       "\n",
       "       Regular Net Profit Growth Rate   Continuous Net Profit Growth Rate  \\\n",
       "1868                         0.689149                            0.217564   \n",
       "3                            0.689110                            0.217568   \n",
       "6641                         0.687465                            0.217496   \n",
       "233                          0.689822                            0.217640   \n",
       "1445                         0.689368                            0.217594   \n",
       "...                               ...                                 ...   \n",
       "1179                         0.689225                            0.217572   \n",
       "1                            0.689702                            0.217620   \n",
       "948                          0.688642                            0.217472   \n",
       "2908                         0.689501                            0.217604   \n",
       "2022                         0.688374                            0.217434   \n",
       "\n",
       "       Total Asset Growth Rate   Total Asset Return Growth Rate Ratio  \\\n",
       "1868              7.380000e+09                               0.263509   \n",
       "3                 4.880000e+09                               0.263371   \n",
       "6641              2.570000e+08                               0.261368   \n",
       "233               7.460000e+09                               0.264212   \n",
       "1445              5.460000e+09                               0.263864   \n",
       "...                        ...                                    ...   \n",
       "1179              4.670000e+09                               0.263319   \n",
       "1                 6.110000e+09                               0.264516   \n",
       "948               7.290000e+09                               0.263763   \n",
       "2908              5.260000e+09                               0.264179   \n",
       "2022              6.730000e+09                               0.263410   \n",
       "\n",
       "       Cash Reinvestment %   Interest Expense Ratio   Debt ratio %  \\\n",
       "1868              0.386023                 0.632316       0.097197   \n",
       "3                 0.384077                 0.630228       0.151465   \n",
       "6641              0.445615                 0.630567       0.268706   \n",
       "233               0.379271                 0.636432       0.132056   \n",
       "1445              0.394769                 0.628938       0.231702   \n",
       "...                    ...                      ...            ...   \n",
       "1179              0.371987                 0.630163       0.158660   \n",
       "1                 0.376709                 0.635172       0.171176   \n",
       "948               0.377948                 0.624179       0.176800   \n",
       "2908              0.382884                 0.629674       0.197539   \n",
       "2022              0.375396                 0.629541       0.218672   \n",
       "\n",
       "       Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "1868           0.902803                               0.005325   \n",
       "3              0.848535                               0.005047   \n",
       "6641           0.731294                               0.005033   \n",
       "233            0.867944                               0.005046   \n",
       "1445           0.768298                               0.005720   \n",
       "...                 ...                                    ...   \n",
       "1179           0.841340                               0.005025   \n",
       "1              0.828824                               0.005059   \n",
       "948            0.823200                               0.005386   \n",
       "2908           0.802461                               0.005059   \n",
       "2022           0.781328                               0.004911   \n",
       "\n",
       "       Borrowing dependency   Operating profit/Paid-in capital  \\\n",
       "1868               0.373542                           0.102562   \n",
       "3                  0.379743                           0.077727   \n",
       "6641               0.402534                           0.074055   \n",
       "233                0.377480                           0.096894   \n",
       "1445               0.404609                           0.085463   \n",
       "...                     ...                                ...   \n",
       "1179               0.381373                           0.084135   \n",
       "1                  0.376760                           0.093743   \n",
       "948                0.379319                           0.097872   \n",
       "2908               0.382879                           0.088565   \n",
       "2022               0.393737                           0.085650   \n",
       "\n",
       "       Net profit before tax/Paid-in capital  \\\n",
       "1868                                0.171631   \n",
       "3                                   0.147561   \n",
       "6641                                0.090634   \n",
       "233                                 0.169165   \n",
       "1445                                0.141603   \n",
       "...                                      ...   \n",
       "1179                                0.132268   \n",
       "1                                   0.168962   \n",
       "948                                 0.164822   \n",
       "2908                                0.159679   \n",
       "2022                                0.156104   \n",
       "\n",
       "       Inventory and accounts receivable/Net value  \\\n",
       "1868                                      0.396874   \n",
       "3                                         0.397925   \n",
       "6641                                      0.471817   \n",
       "233                                       0.396366   \n",
       "1445                                      0.417332   \n",
       "...                                            ...   \n",
       "1179                                      0.397559   \n",
       "1                                         0.397725   \n",
       "948                                       0.411580   \n",
       "2908                                      0.406262   \n",
       "2022                                      0.401894   \n",
       "\n",
       "       Operating profit per person   Working Capital to Total Assets  \\\n",
       "1868                      0.398408                          0.792863   \n",
       "3                         0.378497                          0.725754   \n",
       "6641                      0.378983                          0.740426   \n",
       "233                       0.393207                          0.742227   \n",
       "1445                      0.370378                          0.708529   \n",
       "...                            ...                               ...   \n",
       "1179                      0.380916                          0.717092   \n",
       "1                         0.391590                          0.751111   \n",
       "948                       0.392517                          0.783541   \n",
       "2908                      0.389736                          0.706854   \n",
       "2022                      0.384150                          0.676385   \n",
       "\n",
       "       Quick Assets/Total Assets   Cash/Total Assets  \\\n",
       "1868                    0.474841            0.100335   \n",
       "3                       0.161575            0.018851   \n",
       "6641                    0.465562            0.108758   \n",
       "233                     0.077033            0.011382   \n",
       "1445                    0.311743            0.050565   \n",
       "...                          ...                 ...   \n",
       "1179                    0.106695            0.000409   \n",
       "1                       0.127236            0.014948   \n",
       "948                     0.463405            0.047491   \n",
       "2908                    0.292874            0.060502   \n",
       "2022                    0.195478            0.007454   \n",
       "\n",
       "       Current Liability to Assets   Operating Funds to Liability  \\\n",
       "1868                      0.095271                       0.353342   \n",
       "3                         0.098715                       0.348716   \n",
       "6641                      0.239915                       0.350564   \n",
       "233                       0.053358                       0.344074   \n",
       "1445                      0.197270                       0.345557   \n",
       "...                            ...                            ...   \n",
       "1179                      0.087106                       0.336488   \n",
       "1                         0.056963                       0.341106   \n",
       "948                       0.168455                       0.341540   \n",
       "2908                      0.177582                       0.345573   \n",
       "2022                      0.158285                       0.340148   \n",
       "\n",
       "       Inventory/Working Capital   Current Liabilities/Liability  \\\n",
       "1868                    0.276990                        0.919886   \n",
       "3                       0.276580                        0.615848   \n",
       "6641                    0.272963                        0.858173   \n",
       "233                     0.275900                        0.375513   \n",
       "1445                    0.276369                        0.815745   \n",
       "...                          ...                             ...   \n",
       "1179                    0.276752                        0.517257   \n",
       "1                       0.289642                        0.308589   \n",
       "948                     0.277887                        0.909853   \n",
       "2908                    0.276556                        0.859651   \n",
       "2022                    0.276878                        0.690745   \n",
       "\n",
       "       Working Capital/Equity   Current Liabilities/Equity  \\\n",
       "1868                 0.734525                     0.329928   \n",
       "3                    0.729825                     0.331509   \n",
       "6641                 0.727664                     0.391128   \n",
       "233                  0.731298                     0.328724   \n",
       "1445                 0.724078                     0.349741   \n",
       "...                       ...                          ...   \n",
       "1179                 0.728948                     0.331127   \n",
       "1                    0.731975                     0.329740   \n",
       "948                  0.735291                     0.337174   \n",
       "2908                 0.726538                     0.340166   \n",
       "2022                 0.720350                     0.341977   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total income/Total expense  \\\n",
       "1868                            0.937487                     0.002242   \n",
       "3                               0.906902                     0.001831   \n",
       "6641                            0.852516                     0.001931   \n",
       "233                             0.931610                     0.002318   \n",
       "1445                            0.910702                     0.002016   \n",
       "...                                  ...                          ...   \n",
       "1179                            0.880488                     0.001972   \n",
       "1                               0.931065                     0.002226   \n",
       "948                             0.932147                     0.002210   \n",
       "2908                            0.898535                     0.002154   \n",
       "2022                            0.910031                     0.002111   \n",
       "\n",
       "       Total expense/Assets   Working capitcal Turnover Rate  \\\n",
       "1868               0.041187                         0.593967   \n",
       "3                  0.024161                         0.593889   \n",
       "6641               0.136224                         0.593912   \n",
       "233                0.007297                         0.593893   \n",
       "1445               0.037014                         0.593871   \n",
       "...                     ...                              ...   \n",
       "1179               0.062863                         0.593875   \n",
       "1                  0.025516                         0.593916   \n",
       "948                0.039506                         0.593947   \n",
       "2908               0.060988                         0.593887   \n",
       "2022               0.041586                         0.593827   \n",
       "\n",
       "       Cash Flow to Sales   Current Liability to Equity  \\\n",
       "1868             0.671564                      0.329928   \n",
       "3                0.671519                      0.331509   \n",
       "6641             0.671564                      0.391128   \n",
       "233              0.671516                      0.328724   \n",
       "1445             0.671575                      0.349741   \n",
       "...                   ...                           ...   \n",
       "1179             0.671535                      0.331127   \n",
       "1                0.671570                      0.329740   \n",
       "948              0.671574                      0.337174   \n",
       "2908             0.671579                      0.340166   \n",
       "2022             0.671570                      0.341977   \n",
       "\n",
       "       Equity to Long-term Liability   Cash Flow to Total Assets  \\\n",
       "1868                        0.112116                    0.634688   \n",
       "3                           0.120760                    0.579039   \n",
       "6641                        0.144985                    0.609092   \n",
       "233                         0.125439                    0.616598   \n",
       "1445                        0.125988                    0.646103   \n",
       "...                              ...                         ...   \n",
       "1179                        0.126967                    0.603664   \n",
       "1                           0.120916                    0.641100   \n",
       "948                         0.113409                    0.644983   \n",
       "2908                        0.117993                    0.655186   \n",
       "2022                        0.134579                    0.639996   \n",
       "\n",
       "       Cash Flow to Liability   CFO to Assets   Cash Flow to Equity  \\\n",
       "1868                 0.457146        0.605097              0.313420   \n",
       "3                    0.448518        0.604105              0.302382   \n",
       "6641                 0.456023        0.649732              0.282216   \n",
       "233                  0.454211        0.579259              0.310132   \n",
       "1445                 0.459627        0.601032              0.315905   \n",
       "...                       ...             ...                   ...   \n",
       "1179                 0.452955        0.543242              0.306692   \n",
       "1                    0.459001        0.567101              0.314163   \n",
       "948                  0.459572        0.569659              0.315040   \n",
       "2908                 0.460867        0.595849              0.317977   \n",
       "2022                 0.458929        0.561082              0.313533   \n",
       "\n",
       "       Current Liability to Current Assets   Net Income to Total Assets  \\\n",
       "1868                              0.032234                     0.798835   \n",
       "3                                 0.067250                     0.739555   \n",
       "6641                              0.050780                     0.557733   \n",
       "233                               0.058460                     0.799720   \n",
       "1445                              0.063574                     0.749555   \n",
       "...                                    ...                          ...   \n",
       "1179                              0.084685                     0.701107   \n",
       "1                                 0.047775                     0.795297   \n",
       "948                               0.039624                     0.791303   \n",
       "2908                              0.066816                     0.768935   \n",
       "2022                              0.101788                     0.767057   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "1868             0.623977                0.622592   \n",
       "3                0.622929                0.583538   \n",
       "6641             0.623302                0.590838   \n",
       "233              0.622374                0.602089   \n",
       "1445             0.622808                0.593690   \n",
       "...                   ...                     ...   \n",
       "1179             0.623389                0.597026   \n",
       "1                0.623652                0.610237   \n",
       "948              0.623639                0.610706   \n",
       "2908             0.623540                0.606475   \n",
       "2022             0.623321                0.607845   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "1868                             0.840240              0.278029   \n",
       "3                                0.834697              0.281721   \n",
       "6641                             0.726888              0.336515   \n",
       "233                              0.840356              0.280124   \n",
       "1445                             0.830318              0.298268   \n",
       "...                                   ...                   ...   \n",
       "1179                             0.830591              0.282426   \n",
       "1                                0.839969              0.283846   \n",
       "948                              0.839508              0.284577   \n",
       "2908                             0.836306              0.287988   \n",
       "2022                             0.834982              0.293298   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "1868                             0.027053   \n",
       "3                                0.026697   \n",
       "6641                             0.026777   \n",
       "233                              0.032780   \n",
       "1445                             0.026437   \n",
       "...                                   ...   \n",
       "1179                             0.026670   \n",
       "1                                0.264577   \n",
       "948                              0.026062   \n",
       "2908                             0.026529   \n",
       "2022                             0.026485   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)  \\\n",
       "1868                                           0.566160     \n",
       "3                                              0.564663     \n",
       "6641                                           0.565092     \n",
       "233                                            0.569441     \n",
       "1445                                           0.562615     \n",
       "...                                                 ...     \n",
       "1179                                           0.564509     \n",
       "1                                              0.570175     \n",
       "948                                            0.553919     \n",
       "2908                                           0.563500     \n",
       "2022                                           0.563103     \n",
       "\n",
       "       Equity to Liability  Bankrupt?  \n",
       "1868              0.039145          1  \n",
       "3                 0.023982          1  \n",
       "6641              0.011797          1  \n",
       "233               0.028018          1  \n",
       "1445              0.014336          1  \n",
       "...                    ...        ...  \n",
       "1179              0.022732          1  \n",
       "1                 0.020794          1  \n",
       "948               0.020014          1  \n",
       "2908              0.017506          1  \n",
       "2022              0.015432          1  \n",
       "\n",
       "[176 rows x 61 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TRAIN[TRAIN[\"Bankrupt?\"]==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478991e",
   "metadata": {},
   "source": [
    "## Upsampling the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f0f997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_indexes = np.random.choice(TRAIN[TRAIN[\"Bankrupt?\"]==1].index, size = 300, replace=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb4bc5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>Continuous interest rate (after tax)</th>\n",
       "      <th>Cash flow rate</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
       "      <th>Operating Profit Growth Rate</th>\n",
       "      <th>Regular Net Profit Growth Rate</th>\n",
       "      <th>Continuous Net Profit Growth Rate</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Total Asset Return Growth Rate Ratio</th>\n",
       "      <th>Cash Reinvestment %</th>\n",
       "      <th>Interest Expense Ratio</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Inventory and accounts receivable/Net value</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Inventory/Working Capital</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Working Capital/Equity</th>\n",
       "      <th>Current Liabilities/Equity</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total income/Total expense</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Working capitcal Turnover Rate</th>\n",
       "      <th>Cash Flow to Sales</th>\n",
       "      <th>Current Liability to Equity</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>Cash Flow to Total Assets</th>\n",
       "      <th>Cash Flow to Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Cash Flow to Equity</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Equity to Liability</th>\n",
       "      <th>Bankrupt?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0.466728</td>\n",
       "      <td>0.490078</td>\n",
       "      <td>0.501097</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.998855</td>\n",
       "      <td>0.797196</td>\n",
       "      <td>0.809109</td>\n",
       "      <td>0.303425</td>\n",
       "      <td>0.781354</td>\n",
       "      <td>0.461146</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.194006</td>\n",
       "      <td>0.155457</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.847129</td>\n",
       "      <td>0.688374</td>\n",
       "      <td>0.217434</td>\n",
       "      <td>6.730000e+09</td>\n",
       "      <td>0.263410</td>\n",
       "      <td>0.375396</td>\n",
       "      <td>0.629541</td>\n",
       "      <td>0.218672</td>\n",
       "      <td>0.781328</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.393737</td>\n",
       "      <td>0.085650</td>\n",
       "      <td>0.156104</td>\n",
       "      <td>0.401894</td>\n",
       "      <td>0.384150</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.007454</td>\n",
       "      <td>0.158285</td>\n",
       "      <td>0.340148</td>\n",
       "      <td>0.276878</td>\n",
       "      <td>0.690745</td>\n",
       "      <td>0.720350</td>\n",
       "      <td>0.341977</td>\n",
       "      <td>0.910031</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.041586</td>\n",
       "      <td>0.593827</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.341977</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>0.639996</td>\n",
       "      <td>0.458929</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.313533</td>\n",
       "      <td>0.101788</td>\n",
       "      <td>0.767057</td>\n",
       "      <td>0.623321</td>\n",
       "      <td>0.607845</td>\n",
       "      <td>0.834982</td>\n",
       "      <td>0.293298</td>\n",
       "      <td>0.026485</td>\n",
       "      <td>0.563103</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>0.417540</td>\n",
       "      <td>0.470726</td>\n",
       "      <td>0.460892</td>\n",
       "      <td>0.600852</td>\n",
       "      <td>0.998935</td>\n",
       "      <td>0.797160</td>\n",
       "      <td>0.809086</td>\n",
       "      <td>0.303195</td>\n",
       "      <td>0.781327</td>\n",
       "      <td>0.457600</td>\n",
       "      <td>0.173655</td>\n",
       "      <td>0.173655</td>\n",
       "      <td>0.179428</td>\n",
       "      <td>0.189846</td>\n",
       "      <td>0.148528</td>\n",
       "      <td>0.022102</td>\n",
       "      <td>0.847878</td>\n",
       "      <td>0.685899</td>\n",
       "      <td>0.217175</td>\n",
       "      <td>6.670000e+09</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.368317</td>\n",
       "      <td>0.629741</td>\n",
       "      <td>0.126886</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.375776</td>\n",
       "      <td>0.092392</td>\n",
       "      <td>0.150509</td>\n",
       "      <td>0.405191</td>\n",
       "      <td>0.386920</td>\n",
       "      <td>0.872358</td>\n",
       "      <td>0.748745</td>\n",
       "      <td>0.146474</td>\n",
       "      <td>0.087214</td>\n",
       "      <td>0.333228</td>\n",
       "      <td>0.276982</td>\n",
       "      <td>0.647171</td>\n",
       "      <td>0.740728</td>\n",
       "      <td>0.330210</td>\n",
       "      <td>0.911033</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.041469</td>\n",
       "      <td>0.594027</td>\n",
       "      <td>0.671587</td>\n",
       "      <td>0.330210</td>\n",
       "      <td>0.117055</td>\n",
       "      <td>0.662536</td>\n",
       "      <td>0.463193</td>\n",
       "      <td>0.533933</td>\n",
       "      <td>0.317768</td>\n",
       "      <td>0.018729</td>\n",
       "      <td>0.754696</td>\n",
       "      <td>0.624491</td>\n",
       "      <td>0.600848</td>\n",
       "      <td>0.836693</td>\n",
       "      <td>0.279759</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.563668</td>\n",
       "      <td>0.029296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.416126</td>\n",
       "      <td>0.470235</td>\n",
       "      <td>0.463783</td>\n",
       "      <td>0.599115</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797241</td>\n",
       "      <td>0.809176</td>\n",
       "      <td>0.303258</td>\n",
       "      <td>0.781429</td>\n",
       "      <td>0.460247</td>\n",
       "      <td>0.156209</td>\n",
       "      <td>0.156209</td>\n",
       "      <td>0.156209</td>\n",
       "      <td>0.177839</td>\n",
       "      <td>0.139640</td>\n",
       "      <td>0.022149</td>\n",
       "      <td>0.848455</td>\n",
       "      <td>0.689099</td>\n",
       "      <td>0.217576</td>\n",
       "      <td>6.540000e+08</td>\n",
       "      <td>0.263455</td>\n",
       "      <td>0.369137</td>\n",
       "      <td>0.629275</td>\n",
       "      <td>0.219609</td>\n",
       "      <td>0.780391</td>\n",
       "      <td>0.005003</td>\n",
       "      <td>0.392590</td>\n",
       "      <td>0.098547</td>\n",
       "      <td>0.138625</td>\n",
       "      <td>0.407169</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.703730</td>\n",
       "      <td>0.316573</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.198285</td>\n",
       "      <td>0.337688</td>\n",
       "      <td>0.276486</td>\n",
       "      <td>0.865216</td>\n",
       "      <td>0.724531</td>\n",
       "      <td>0.346198</td>\n",
       "      <td>0.908948</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.053090</td>\n",
       "      <td>0.593891</td>\n",
       "      <td>0.671567</td>\n",
       "      <td>0.346198</td>\n",
       "      <td>0.113571</td>\n",
       "      <td>0.629329</td>\n",
       "      <td>0.457681</td>\n",
       "      <td>0.543740</td>\n",
       "      <td>0.309716</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>0.750431</td>\n",
       "      <td>0.623183</td>\n",
       "      <td>0.599110</td>\n",
       "      <td>0.831976</td>\n",
       "      <td>0.293599</td>\n",
       "      <td>0.026472</td>\n",
       "      <td>0.562977</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>0.391995</td>\n",
       "      <td>0.456444</td>\n",
       "      <td>0.454789</td>\n",
       "      <td>0.603670</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.797211</td>\n",
       "      <td>0.809178</td>\n",
       "      <td>0.303129</td>\n",
       "      <td>0.781424</td>\n",
       "      <td>0.460082</td>\n",
       "      <td>0.177363</td>\n",
       "      <td>0.177363</td>\n",
       "      <td>0.177363</td>\n",
       "      <td>0.163657</td>\n",
       "      <td>0.112902</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.688342</td>\n",
       "      <td>0.217473</td>\n",
       "      <td>9.500000e+09</td>\n",
       "      <td>0.262339</td>\n",
       "      <td>0.365290</td>\n",
       "      <td>0.630437</td>\n",
       "      <td>0.212958</td>\n",
       "      <td>0.787042</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.385563</td>\n",
       "      <td>0.113472</td>\n",
       "      <td>0.119297</td>\n",
       "      <td>0.414193</td>\n",
       "      <td>0.395909</td>\n",
       "      <td>0.792009</td>\n",
       "      <td>0.558547</td>\n",
       "      <td>0.151173</td>\n",
       "      <td>0.166879</td>\n",
       "      <td>0.337704</td>\n",
       "      <td>0.277485</td>\n",
       "      <td>0.748619</td>\n",
       "      <td>0.737961</td>\n",
       "      <td>0.341712</td>\n",
       "      <td>0.919759</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.064867</td>\n",
       "      <td>0.593938</td>\n",
       "      <td>0.671582</td>\n",
       "      <td>0.341712</td>\n",
       "      <td>0.129703</td>\n",
       "      <td>0.663280</td>\n",
       "      <td>0.461732</td>\n",
       "      <td>0.544469</td>\n",
       "      <td>0.321255</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.756953</td>\n",
       "      <td>0.623735</td>\n",
       "      <td>0.603669</td>\n",
       "      <td>0.833672</td>\n",
       "      <td>0.291601</td>\n",
       "      <td>0.026749</td>\n",
       "      <td>0.564948</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>0.459221</td>\n",
       "      <td>0.524041</td>\n",
       "      <td>0.512233</td>\n",
       "      <td>0.594654</td>\n",
       "      <td>0.998932</td>\n",
       "      <td>0.797336</td>\n",
       "      <td>0.809269</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>0.781528</td>\n",
       "      <td>0.462147</td>\n",
       "      <td>0.159285</td>\n",
       "      <td>0.159285</td>\n",
       "      <td>0.159285</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>0.164570</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.848125</td>\n",
       "      <td>0.689617</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>6.280000e+08</td>\n",
       "      <td>0.264440</td>\n",
       "      <td>0.383005</td>\n",
       "      <td>0.625989</td>\n",
       "      <td>0.206065</td>\n",
       "      <td>0.793935</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.380584</td>\n",
       "      <td>0.090568</td>\n",
       "      <td>0.163562</td>\n",
       "      <td>0.416289</td>\n",
       "      <td>0.389905</td>\n",
       "      <td>0.751315</td>\n",
       "      <td>0.484530</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.200153</td>\n",
       "      <td>0.342782</td>\n",
       "      <td>0.306562</td>\n",
       "      <td>0.930716</td>\n",
       "      <td>0.732027</td>\n",
       "      <td>0.343407</td>\n",
       "      <td>0.923606</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.011804</td>\n",
       "      <td>0.593915</td>\n",
       "      <td>0.671566</td>\n",
       "      <td>0.343407</td>\n",
       "      <td>0.113331</td>\n",
       "      <td>0.633907</td>\n",
       "      <td>0.458149</td>\n",
       "      <td>0.578704</td>\n",
       "      <td>0.311823</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.787988</td>\n",
       "      <td>0.622856</td>\n",
       "      <td>0.594649</td>\n",
       "      <td>0.838838</td>\n",
       "      <td>0.289838</td>\n",
       "      <td>0.026137</td>\n",
       "      <td>0.556960</td>\n",
       "      <td>0.016619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0.475796</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>0.526581</td>\n",
       "      <td>0.595050</td>\n",
       "      <td>0.998882</td>\n",
       "      <td>0.797105</td>\n",
       "      <td>0.809093</td>\n",
       "      <td>0.303209</td>\n",
       "      <td>0.781446</td>\n",
       "      <td>0.466838</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>0.199868</td>\n",
       "      <td>0.142653</td>\n",
       "      <td>0.021952</td>\n",
       "      <td>0.847852</td>\n",
       "      <td>0.681076</td>\n",
       "      <td>0.217305</td>\n",
       "      <td>5.570000e+08</td>\n",
       "      <td>0.263457</td>\n",
       "      <td>0.384244</td>\n",
       "      <td>0.629659</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.811773</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>0.387715</td>\n",
       "      <td>0.085080</td>\n",
       "      <td>0.141648</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>0.388153</td>\n",
       "      <td>0.767276</td>\n",
       "      <td>0.172758</td>\n",
       "      <td>0.039696</td>\n",
       "      <td>0.077183</td>\n",
       "      <td>0.347095</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>0.384163</td>\n",
       "      <td>0.733808</td>\n",
       "      <td>0.331751</td>\n",
       "      <td>0.923829</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.021639</td>\n",
       "      <td>0.593939</td>\n",
       "      <td>0.671582</td>\n",
       "      <td>0.331751</td>\n",
       "      <td>0.142463</td>\n",
       "      <td>0.651255</td>\n",
       "      <td>0.460409</td>\n",
       "      <td>0.603616</td>\n",
       "      <td>0.316686</td>\n",
       "      <td>0.038877</td>\n",
       "      <td>0.770699</td>\n",
       "      <td>0.624185</td>\n",
       "      <td>0.595047</td>\n",
       "      <td>0.836847</td>\n",
       "      <td>0.286295</td>\n",
       "      <td>0.026591</td>\n",
       "      <td>0.563980</td>\n",
       "      <td>0.018567</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.535543</td>\n",
       "      <td>0.521334</td>\n",
       "      <td>0.599310</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.797383</td>\n",
       "      <td>0.809303</td>\n",
       "      <td>0.303444</td>\n",
       "      <td>0.781560</td>\n",
       "      <td>0.457838</td>\n",
       "      <td>0.169399</td>\n",
       "      <td>0.169399</td>\n",
       "      <td>0.169399</td>\n",
       "      <td>0.213955</td>\n",
       "      <td>0.170144</td>\n",
       "      <td>0.022077</td>\n",
       "      <td>0.848070</td>\n",
       "      <td>0.688949</td>\n",
       "      <td>0.217544</td>\n",
       "      <td>5.800000e+09</td>\n",
       "      <td>0.263983</td>\n",
       "      <td>0.350620</td>\n",
       "      <td>0.635979</td>\n",
       "      <td>0.192188</td>\n",
       "      <td>0.807812</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.380906</td>\n",
       "      <td>0.100208</td>\n",
       "      <td>0.169195</td>\n",
       "      <td>0.419146</td>\n",
       "      <td>0.396463</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.265549</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>0.186461</td>\n",
       "      <td>0.330751</td>\n",
       "      <td>0.278373</td>\n",
       "      <td>0.928440</td>\n",
       "      <td>0.738127</td>\n",
       "      <td>0.340119</td>\n",
       "      <td>0.930257</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>0.593979</td>\n",
       "      <td>0.671570</td>\n",
       "      <td>0.340119</td>\n",
       "      <td>0.113949</td>\n",
       "      <td>0.640175</td>\n",
       "      <td>0.458907</td>\n",
       "      <td>0.503414</td>\n",
       "      <td>0.313839</td>\n",
       "      <td>0.036623</td>\n",
       "      <td>0.795422</td>\n",
       "      <td>0.621335</td>\n",
       "      <td>0.599307</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.286978</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.569831</td>\n",
       "      <td>0.018104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>0.404036</td>\n",
       "      <td>0.223615</td>\n",
       "      <td>0.430055</td>\n",
       "      <td>0.586611</td>\n",
       "      <td>0.998568</td>\n",
       "      <td>0.796179</td>\n",
       "      <td>0.808154</td>\n",
       "      <td>0.302249</td>\n",
       "      <td>0.781061</td>\n",
       "      <td>0.457979</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.173301</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>0.843895</td>\n",
       "      <td>0.684855</td>\n",
       "      <td>0.217416</td>\n",
       "      <td>4.120000e+09</td>\n",
       "      <td>0.260341</td>\n",
       "      <td>0.365169</td>\n",
       "      <td>0.630462</td>\n",
       "      <td>0.338483</td>\n",
       "      <td>0.661517</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.295894</td>\n",
       "      <td>0.061581</td>\n",
       "      <td>0.089465</td>\n",
       "      <td>0.390738</td>\n",
       "      <td>0.369225</td>\n",
       "      <td>0.643091</td>\n",
       "      <td>0.129527</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>0.160152</td>\n",
       "      <td>0.335961</td>\n",
       "      <td>0.276975</td>\n",
       "      <td>0.449042</td>\n",
       "      <td>0.764645</td>\n",
       "      <td>0.294906</td>\n",
       "      <td>0.843977</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.126337</td>\n",
       "      <td>0.593819</td>\n",
       "      <td>0.671554</td>\n",
       "      <td>0.294906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617235</td>\n",
       "      <td>0.457308</td>\n",
       "      <td>0.514097</td>\n",
       "      <td>0.332253</td>\n",
       "      <td>0.193950</td>\n",
       "      <td>0.572881</td>\n",
       "      <td>0.623173</td>\n",
       "      <td>0.586607</td>\n",
       "      <td>0.916329</td>\n",
       "      <td>0.218785</td>\n",
       "      <td>0.026745</td>\n",
       "      <td>0.564930</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>0.395213</td>\n",
       "      <td>0.415013</td>\n",
       "      <td>0.427485</td>\n",
       "      <td>0.592867</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.796672</td>\n",
       "      <td>0.808605</td>\n",
       "      <td>0.302797</td>\n",
       "      <td>0.780868</td>\n",
       "      <td>0.458712</td>\n",
       "      <td>0.166533</td>\n",
       "      <td>0.166533</td>\n",
       "      <td>0.166533</td>\n",
       "      <td>0.170748</td>\n",
       "      <td>0.133615</td>\n",
       "      <td>0.021932</td>\n",
       "      <td>0.847249</td>\n",
       "      <td>0.686332</td>\n",
       "      <td>0.217231</td>\n",
       "      <td>5.860000e+07</td>\n",
       "      <td>0.262281</td>\n",
       "      <td>0.362291</td>\n",
       "      <td>0.630453</td>\n",
       "      <td>0.163890</td>\n",
       "      <td>0.836110</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.380815</td>\n",
       "      <td>0.079225</td>\n",
       "      <td>0.133029</td>\n",
       "      <td>0.403258</td>\n",
       "      <td>0.379096</td>\n",
       "      <td>0.731176</td>\n",
       "      <td>0.347873</td>\n",
       "      <td>0.097388</td>\n",
       "      <td>0.166235</td>\n",
       "      <td>0.332881</td>\n",
       "      <td>0.276034</td>\n",
       "      <td>0.967777</td>\n",
       "      <td>0.730132</td>\n",
       "      <td>0.335982</td>\n",
       "      <td>0.907363</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.044064</td>\n",
       "      <td>0.593883</td>\n",
       "      <td>0.671521</td>\n",
       "      <td>0.335982</td>\n",
       "      <td>0.112116</td>\n",
       "      <td>0.604106</td>\n",
       "      <td>0.453221</td>\n",
       "      <td>0.523477</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>0.055644</td>\n",
       "      <td>0.723121</td>\n",
       "      <td>0.623017</td>\n",
       "      <td>0.592868</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.282987</td>\n",
       "      <td>0.026741</td>\n",
       "      <td>0.564908</td>\n",
       "      <td>0.021886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0.367084</td>\n",
       "      <td>0.398114</td>\n",
       "      <td>0.389528</td>\n",
       "      <td>0.598913</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>0.796949</td>\n",
       "      <td>0.808831</td>\n",
       "      <td>0.302721</td>\n",
       "      <td>0.781055</td>\n",
       "      <td>0.463390</td>\n",
       "      <td>0.148835</td>\n",
       "      <td>0.148835</td>\n",
       "      <td>0.148835</td>\n",
       "      <td>0.158173</td>\n",
       "      <td>0.131430</td>\n",
       "      <td>0.022073</td>\n",
       "      <td>0.848009</td>\n",
       "      <td>0.682685</td>\n",
       "      <td>0.216798</td>\n",
       "      <td>4.600000e+09</td>\n",
       "      <td>0.262126</td>\n",
       "      <td>0.391519</td>\n",
       "      <td>0.630337</td>\n",
       "      <td>0.214983</td>\n",
       "      <td>0.785017</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.389075</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.130390</td>\n",
       "      <td>0.401138</td>\n",
       "      <td>0.394699</td>\n",
       "      <td>0.761175</td>\n",
       "      <td>0.728138</td>\n",
       "      <td>0.018224</td>\n",
       "      <td>0.219430</td>\n",
       "      <td>0.346548</td>\n",
       "      <td>0.277624</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.733510</td>\n",
       "      <td>0.347094</td>\n",
       "      <td>0.902914</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.065140</td>\n",
       "      <td>0.593924</td>\n",
       "      <td>0.671572</td>\n",
       "      <td>0.347094</td>\n",
       "      <td>0.112007</td>\n",
       "      <td>0.643202</td>\n",
       "      <td>0.459307</td>\n",
       "      <td>0.605278</td>\n",
       "      <td>0.314658</td>\n",
       "      <td>0.045867</td>\n",
       "      <td>0.701446</td>\n",
       "      <td>0.623689</td>\n",
       "      <td>0.598911</td>\n",
       "      <td>0.824230</td>\n",
       "      <td>0.292176</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564666</td>\n",
       "      <td>0.015765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "2022                                           0.466728          \n",
       "2027                                           0.417540          \n",
       "29                                             0.416126          \n",
       "3059                                           0.391995          \n",
       "2294                                           0.459221          \n",
       "...                                                 ...          \n",
       "1025                                           0.475796          \n",
       "870                                            0.469800          \n",
       "2470                                           0.404036          \n",
       "5900                                           0.395213          \n",
       "2024                                           0.367084          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "2022                                 0.490078   \n",
       "2027                                 0.470726   \n",
       "29                                   0.470235   \n",
       "3059                                 0.456444   \n",
       "2294                                 0.524041   \n",
       "...                                       ...   \n",
       "1025                                 0.497056   \n",
       "870                                  0.535543   \n",
       "2470                                 0.223615   \n",
       "5900                                 0.415013   \n",
       "2024                                 0.398114   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "2022                                           0.501097    \n",
       "2027                                           0.460892    \n",
       "29                                             0.463783    \n",
       "3059                                           0.454789    \n",
       "2294                                           0.512233    \n",
       "...                                                 ...    \n",
       "1025                                           0.526581    \n",
       "870                                            0.521334    \n",
       "2470                                           0.430055    \n",
       "5900                                           0.427485    \n",
       "2024                                           0.389528    \n",
       "\n",
       "       Operating Gross Margin   Operating Profit Rate  \\\n",
       "2022                 0.607850                0.998855   \n",
       "2027                 0.600852                0.998935   \n",
       "29                   0.599115                0.998973   \n",
       "3059                 0.603670                0.999009   \n",
       "2294                 0.594654                0.998932   \n",
       "...                       ...                     ...   \n",
       "1025                 0.595050                0.998882   \n",
       "870                  0.599310                0.999002   \n",
       "2470                 0.586611                0.998568   \n",
       "5900                 0.592867                0.998717   \n",
       "2024                 0.598913                0.998985   \n",
       "\n",
       "       Pre-tax net Interest Rate   After-tax net Interest Rate  \\\n",
       "2022                    0.797196                      0.809109   \n",
       "2027                    0.797160                      0.809086   \n",
       "29                      0.797241                      0.809176   \n",
       "3059                    0.797211                      0.809178   \n",
       "2294                    0.797336                      0.809269   \n",
       "...                          ...                           ...   \n",
       "1025                    0.797105                      0.809093   \n",
       "870                     0.797383                      0.809303   \n",
       "2470                    0.796179                      0.808154   \n",
       "5900                    0.796672                      0.808605   \n",
       "2024                    0.796949                      0.808831   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  \\\n",
       "2022                                      0.303425   \n",
       "2027                                      0.303195   \n",
       "29                                        0.303258   \n",
       "3059                                      0.303129   \n",
       "2294                                      0.303510   \n",
       "...                                            ...   \n",
       "1025                                      0.303209   \n",
       "870                                       0.303444   \n",
       "2470                                      0.302249   \n",
       "5900                                      0.302797   \n",
       "2024                                      0.302721   \n",
       "\n",
       "       Continuous interest rate (after tax)   Cash flow rate  \\\n",
       "2022                               0.781354         0.461146   \n",
       "2027                               0.781327         0.457600   \n",
       "29                                 0.781429         0.460247   \n",
       "3059                               0.781424         0.460082   \n",
       "2294                               0.781528         0.462147   \n",
       "...                                     ...              ...   \n",
       "1025                               0.781446         0.466838   \n",
       "870                                0.781560         0.457838   \n",
       "2470                               0.781061         0.457979   \n",
       "5900                               0.780868         0.458712   \n",
       "2024                               0.781055         0.463390   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "2022                  0.152206                  0.152206   \n",
       "2027                  0.173655                  0.173655   \n",
       "29                    0.156209                  0.156209   \n",
       "3059                  0.177363                  0.177363   \n",
       "2294                  0.159285                  0.159285   \n",
       "...                        ...                       ...   \n",
       "1025                  0.183136                  0.183136   \n",
       "870                   0.169399                  0.169399   \n",
       "2470                  0.116599                  0.116599   \n",
       "5900                  0.166533                  0.166533   \n",
       "2024                  0.148835                  0.148835   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "2022                  0.152206                                  0.194006   \n",
       "2027                  0.179428                                  0.189846   \n",
       "29                    0.156209                                  0.177839   \n",
       "3059                  0.177363                                  0.163657   \n",
       "2294                  0.159285                                  0.208849   \n",
       "...                        ...                                       ...   \n",
       "1025                  0.183136                                  0.199868   \n",
       "870                   0.169399                                  0.213955   \n",
       "2470                  0.116599                                  0.173301   \n",
       "5900                  0.166533                                  0.170748   \n",
       "2024                  0.148835                                  0.158173   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)  \\\n",
       "2022                                   0.155457   \n",
       "2027                                   0.148528   \n",
       "29                                     0.139640   \n",
       "3059                                   0.112902   \n",
       "2294                                   0.164570   \n",
       "...                                         ...   \n",
       "1025                                   0.142653   \n",
       "870                                    0.170144   \n",
       "2470                                   0.085561   \n",
       "5900                                   0.133615   \n",
       "2024                                   0.131430   \n",
       "\n",
       "       Realized Sales Gross Profit Growth Rate   Operating Profit Growth Rate  \\\n",
       "2022                                  0.022033                       0.847129   \n",
       "2027                                  0.022102                       0.847878   \n",
       "29                                    0.022149                       0.848455   \n",
       "3059                                  0.022161                       0.848005   \n",
       "2294                                  0.022346                       0.848125   \n",
       "...                                        ...                            ...   \n",
       "1025                                  0.021952                       0.847852   \n",
       "870                                   0.022077                       0.848070   \n",
       "2470                                  0.021732                       0.843895   \n",
       "5900                                  0.021932                       0.847249   \n",
       "2024                                  0.022073                       0.848009   \n",
       "\n",
       "       Regular Net Profit Growth Rate   Continuous Net Profit Growth Rate  \\\n",
       "2022                         0.688374                            0.217434   \n",
       "2027                         0.685899                            0.217175   \n",
       "29                           0.689099                            0.217576   \n",
       "3059                         0.688342                            0.217473   \n",
       "2294                         0.689617                            0.217620   \n",
       "...                               ...                                 ...   \n",
       "1025                         0.681076                            0.217305   \n",
       "870                          0.688949                            0.217544   \n",
       "2470                         0.684855                            0.217416   \n",
       "5900                         0.686332                            0.217231   \n",
       "2024                         0.682685                            0.216798   \n",
       "\n",
       "       Total Asset Growth Rate   Total Asset Return Growth Rate Ratio  \\\n",
       "2022              6.730000e+09                               0.263410   \n",
       "2027              6.670000e+09                               0.262900   \n",
       "29                6.540000e+08                               0.263455   \n",
       "3059              9.500000e+09                               0.262339   \n",
       "2294              6.280000e+08                               0.264440   \n",
       "...                        ...                                    ...   \n",
       "1025              5.570000e+08                               0.263457   \n",
       "870               5.800000e+09                               0.263983   \n",
       "2470              4.120000e+09                               0.260341   \n",
       "5900              5.860000e+07                               0.262281   \n",
       "2024              4.600000e+09                               0.262126   \n",
       "\n",
       "       Cash Reinvestment %   Interest Expense Ratio   Debt ratio %  \\\n",
       "2022              0.375396                 0.629541       0.218672   \n",
       "2027              0.368317                 0.629741       0.126886   \n",
       "29                0.369137                 0.629275       0.219609   \n",
       "3059              0.365290                 0.630437       0.212958   \n",
       "2294              0.383005                 0.625989       0.206065   \n",
       "...                    ...                      ...            ...   \n",
       "1025              0.384244                 0.629659       0.188227   \n",
       "870               0.350620                 0.635979       0.192188   \n",
       "2470              0.365169                 0.630462       0.338483   \n",
       "5900              0.362291                 0.630453       0.163890   \n",
       "2024              0.391519                 0.630337       0.214983   \n",
       "\n",
       "       Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "2022           0.781328                               0.004911   \n",
       "2027           0.873114                               0.007088   \n",
       "29             0.780391                               0.005003   \n",
       "3059           0.787042                               0.005283   \n",
       "2294           0.793935                               0.005072   \n",
       "...                 ...                                    ...   \n",
       "1025           0.811773                               0.005037   \n",
       "870            0.807812                               0.005672   \n",
       "2470           0.661517                               0.004877   \n",
       "5900           0.836110                               0.005125   \n",
       "2024           0.785017                               0.005291   \n",
       "\n",
       "       Borrowing dependency   Operating profit/Paid-in capital  \\\n",
       "2022               0.393737                           0.085650   \n",
       "2027               0.375776                           0.092392   \n",
       "29                 0.392590                           0.098547   \n",
       "3059               0.385563                           0.113472   \n",
       "2294               0.380584                           0.090568   \n",
       "...                     ...                                ...   \n",
       "1025               0.387715                           0.085080   \n",
       "870                0.380906                           0.100208   \n",
       "2470               0.295894                           0.061581   \n",
       "5900               0.380815                           0.079225   \n",
       "2024               0.389075                           0.098026   \n",
       "\n",
       "       Net profit before tax/Paid-in capital  \\\n",
       "2022                                0.156104   \n",
       "2027                                0.150509   \n",
       "29                                  0.138625   \n",
       "3059                                0.119297   \n",
       "2294                                0.163562   \n",
       "...                                      ...   \n",
       "1025                                0.141648   \n",
       "870                                 0.169195   \n",
       "2470                                0.089465   \n",
       "5900                                0.133029   \n",
       "2024                                0.130390   \n",
       "\n",
       "       Inventory and accounts receivable/Net value  \\\n",
       "2022                                      0.401894   \n",
       "2027                                      0.405191   \n",
       "29                                        0.407169   \n",
       "3059                                      0.414193   \n",
       "2294                                      0.416289   \n",
       "...                                            ...   \n",
       "1025                                      0.400901   \n",
       "870                                       0.419146   \n",
       "2470                                      0.390738   \n",
       "5900                                      0.403258   \n",
       "2024                                      0.401138   \n",
       "\n",
       "       Operating profit per person   Working Capital to Total Assets  \\\n",
       "2022                      0.384150                          0.676385   \n",
       "2027                      0.386920                          0.872358   \n",
       "29                        0.394100                          0.703730   \n",
       "3059                      0.395909                          0.792009   \n",
       "2294                      0.389905                          0.751315   \n",
       "...                            ...                               ...   \n",
       "1025                      0.388153                          0.767276   \n",
       "870                       0.396463                          0.803591   \n",
       "2470                      0.369225                          0.643091   \n",
       "5900                      0.379096                          0.731176   \n",
       "2024                      0.394699                          0.761175   \n",
       "\n",
       "       Quick Assets/Total Assets   Cash/Total Assets  \\\n",
       "2022                    0.195478            0.007454   \n",
       "2027                    0.748745            0.146474   \n",
       "29                      0.316573            0.008266   \n",
       "3059                    0.558547            0.151173   \n",
       "2294                    0.484530            0.043945   \n",
       "...                          ...                 ...   \n",
       "1025                    0.172758            0.039696   \n",
       "870                     0.265549            0.004748   \n",
       "2470                    0.129527            0.015737   \n",
       "5900                    0.347873            0.097388   \n",
       "2024                    0.728138            0.018224   \n",
       "\n",
       "       Current Liability to Assets   Operating Funds to Liability  \\\n",
       "2022                      0.158285                       0.340148   \n",
       "2027                      0.087214                       0.333228   \n",
       "29                        0.198285                       0.337688   \n",
       "3059                      0.166879                       0.337704   \n",
       "2294                      0.200153                       0.342782   \n",
       "...                            ...                            ...   \n",
       "1025                      0.077183                       0.347095   \n",
       "870                       0.186461                       0.330751   \n",
       "2470                      0.160152                       0.335961   \n",
       "5900                      0.166235                       0.332881   \n",
       "2024                      0.219430                       0.346548   \n",
       "\n",
       "       Inventory/Working Capital   Current Liabilities/Liability  \\\n",
       "2022                    0.276878                        0.690745   \n",
       "2027                    0.276982                        0.647171   \n",
       "29                      0.276486                        0.865216   \n",
       "3059                    0.277485                        0.748619   \n",
       "2294                    0.306562                        0.930716   \n",
       "...                          ...                             ...   \n",
       "1025                    0.278132                        0.384163   \n",
       "870                     0.278373                        0.928440   \n",
       "2470                    0.276975                        0.449042   \n",
       "5900                    0.276034                        0.967777   \n",
       "2024                    0.277624                        0.979563   \n",
       "\n",
       "       Working Capital/Equity   Current Liabilities/Equity  \\\n",
       "2022                 0.720350                     0.341977   \n",
       "2027                 0.740728                     0.330210   \n",
       "29                   0.724531                     0.346198   \n",
       "3059                 0.737961                     0.341712   \n",
       "2294                 0.732027                     0.343407   \n",
       "...                       ...                          ...   \n",
       "1025                 0.733808                     0.331751   \n",
       "870                  0.738127                     0.340119   \n",
       "2470                 0.764645                     0.294906   \n",
       "5900                 0.730132                     0.335982   \n",
       "2024                 0.733510                     0.347094   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total income/Total expense  \\\n",
       "2022                            0.910031                     0.002111   \n",
       "2027                            0.911033                     0.002060   \n",
       "29                              0.908948                     0.002084   \n",
       "3059                            0.919759                     0.002127   \n",
       "2294                            0.923606                     0.002115   \n",
       "...                                  ...                          ...   \n",
       "1025                            0.923829                     0.002031   \n",
       "870                             0.930257                     0.002221   \n",
       "2470                            0.843977                     0.001928   \n",
       "5900                            0.907363                     0.001948   \n",
       "2024                            0.902914                     0.001982   \n",
       "\n",
       "       Total expense/Assets   Working capitcal Turnover Rate  \\\n",
       "2022               0.041586                         0.593827   \n",
       "2027               0.041469                         0.594027   \n",
       "29                 0.053090                         0.593891   \n",
       "3059               0.064867                         0.593938   \n",
       "2294               0.011804                         0.593915   \n",
       "...                     ...                              ...   \n",
       "1025               0.021639                         0.593939   \n",
       "870                0.010456                         0.593979   \n",
       "2470               0.126337                         0.593819   \n",
       "5900               0.044064                         0.593883   \n",
       "2024               0.065140                         0.593924   \n",
       "\n",
       "       Cash Flow to Sales   Current Liability to Equity  \\\n",
       "2022             0.671570                      0.341977   \n",
       "2027             0.671587                      0.330210   \n",
       "29               0.671567                      0.346198   \n",
       "3059             0.671582                      0.341712   \n",
       "2294             0.671566                      0.343407   \n",
       "...                   ...                           ...   \n",
       "1025             0.671582                      0.331751   \n",
       "870              0.671570                      0.340119   \n",
       "2470             0.671554                      0.294906   \n",
       "5900             0.671521                      0.335982   \n",
       "2024             0.671572                      0.347094   \n",
       "\n",
       "       Equity to Long-term Liability   Cash Flow to Total Assets  \\\n",
       "2022                        0.134579                    0.639996   \n",
       "2027                        0.117055                    0.662536   \n",
       "29                          0.113571                    0.629329   \n",
       "3059                        0.129703                    0.663280   \n",
       "2294                        0.113331                    0.633907   \n",
       "...                              ...                         ...   \n",
       "1025                        0.142463                    0.651255   \n",
       "870                         0.113949                    0.640175   \n",
       "2470                        0.000000                    0.617235   \n",
       "5900                        0.112116                    0.604106   \n",
       "2024                        0.112007                    0.643202   \n",
       "\n",
       "       Cash Flow to Liability   CFO to Assets   Cash Flow to Equity  \\\n",
       "2022                 0.458929        0.561082              0.313533   \n",
       "2027                 0.463193        0.533933              0.317768   \n",
       "29                   0.457681        0.543740              0.309716   \n",
       "3059                 0.461732        0.544469              0.321255   \n",
       "2294                 0.458149        0.578704              0.311823   \n",
       "...                       ...             ...                   ...   \n",
       "1025                 0.460409        0.603616              0.316686   \n",
       "870                  0.458907        0.503414              0.313839   \n",
       "2470                 0.457308        0.514097              0.332253   \n",
       "5900                 0.453221        0.523477              0.306476   \n",
       "2024                 0.459307        0.605278              0.314658   \n",
       "\n",
       "       Current Liability to Current Assets   Net Income to Total Assets  \\\n",
       "2022                              0.101788                     0.767057   \n",
       "2027                              0.018729                     0.754696   \n",
       "29                                0.065816                     0.750431   \n",
       "3059                              0.037787                     0.756953   \n",
       "2294                              0.048158                     0.787988   \n",
       "...                                    ...                          ...   \n",
       "1025                              0.038877                     0.770699   \n",
       "870                               0.036623                     0.795422   \n",
       "2470                              0.193950                     0.572881   \n",
       "5900                              0.055644                     0.723121   \n",
       "2024                              0.045867                     0.701446   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "2022             0.623321                0.607845   \n",
       "2027             0.624491                0.600848   \n",
       "29               0.623183                0.599110   \n",
       "3059             0.623735                0.603669   \n",
       "2294             0.622856                0.594649   \n",
       "...                   ...                     ...   \n",
       "1025             0.624185                0.595047   \n",
       "870              0.621335                0.599307   \n",
       "2470             0.623173                0.586607   \n",
       "5900             0.623017                0.592868   \n",
       "2024             0.623689                0.598911   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "2022                             0.834982              0.293298   \n",
       "2027                             0.836693              0.279759   \n",
       "29                               0.831976              0.293599   \n",
       "3059                             0.833672              0.291601   \n",
       "2294                             0.838838              0.289838   \n",
       "...                                   ...                   ...   \n",
       "1025                             0.836847              0.286295   \n",
       "870                              0.839972              0.286978   \n",
       "2470                             0.916329              0.218785   \n",
       "5900                             0.832500              0.282987   \n",
       "2024                             0.824230              0.292176   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "2022                             0.026485   \n",
       "2027                             0.026550   \n",
       "29                               0.026472   \n",
       "3059                             0.026749   \n",
       "2294                             0.026137   \n",
       "...                                   ...   \n",
       "1025                             0.026591   \n",
       "870                              0.040254   \n",
       "2470                             0.026745   \n",
       "5900                             0.026741   \n",
       "2024                             0.026697   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)  \\\n",
       "2022                                           0.563103     \n",
       "2027                                           0.563668     \n",
       "29                                             0.562977     \n",
       "3059                                           0.564948     \n",
       "2294                                           0.556960     \n",
       "...                                                 ...     \n",
       "1025                                           0.563980     \n",
       "870                                            0.569831     \n",
       "2470                                           0.564930     \n",
       "5900                                           0.564908     \n",
       "2024                                           0.564666     \n",
       "\n",
       "       Equity to Liability  Bankrupt?  \n",
       "2022              0.015432          1  \n",
       "2027              0.029296          1  \n",
       "29                0.015349          1  \n",
       "3059              0.015953          1  \n",
       "2294              0.016619          1  \n",
       "...                    ...        ...  \n",
       "1025              0.018567          1  \n",
       "870               0.018104          1  \n",
       "2470              0.008500          1  \n",
       "5900              0.021886          1  \n",
       "2024              0.015765          1  \n",
       "\n",
       "[300 rows x 61 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample = TRAIN.loc[upsample_indexes,:]\n",
    "upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c01eb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5279\n",
       "1     176\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f9e72",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2f7cc",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm  =  SMOTE()\n",
    "X_resample, y_resample = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7cfe3",
   "metadata": {},
   "source": [
    "y_resample.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d104f7",
   "metadata": {},
   "source": [
    "## Clustering to downsize the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96fd3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_bankrupcies =  TRAIN[(TRAIN['Bankrupt?'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4f4c1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 2 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.872800348849784e+21\n",
      "Iteration 1, inertia 6.75252133108383e+21\n",
      "Iteration 2, inertia 6.751527901122703e+21\n",
      "Converged at iteration 2: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.229915352576016e+22\n",
      "Iteration 1, inertia 1.095938150892216e+22\n",
      "Iteration 2, inertia 7.097438506987432e+21\n",
      "Iteration 3, inertia 6.76108192754398e+21\n",
      "Iteration 4, inertia 6.751578225276941e+21\n",
      "Iteration 5, inertia 6.751527901122705e+21\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.847142431551956e+21\n",
      "Iteration 1, inertia 6.793170717658019e+21\n",
      "Iteration 2, inertia 6.751731369056185e+21\n",
      "Iteration 3, inertia 6.751527901122703e+21\n",
      "Converged at iteration 3: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.6438969348849505e+22\n",
      "Iteration 1, inertia 6.901739767912263e+21\n",
      "Iteration 2, inertia 6.75274606574255e+21\n",
      "Iteration 3, inertia 6.75154043177054e+21\n",
      "Converged at iteration 3: center shift 9789680082840.494 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.8555504787302268e+22\n",
      "Iteration 1, inertia 7.010682227091242e+21\n",
      "Iteration 2, inertia 6.755923155807816e+21\n",
      "Iteration 3, inertia 6.75154043177054e+21\n",
      "Converged at iteration 3: center shift 9789680082840.123 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.143302848849771e+21\n",
      "Iteration 1, inertia 6.75154024313579e+21\n",
      "Converged at iteration 1: center shift 9564279715422.057 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.169813048849764e+21\n",
      "Iteration 1, inertia 6.752328868157249e+21\n",
      "Iteration 2, inertia 6.75154043177054e+21\n",
      "Converged at iteration 2: center shift 9789680082840.867 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.0431777773337696e+22\n",
      "Iteration 1, inertia 7.669407310122304e+21\n",
      "Iteration 2, inertia 6.785021144345801e+21\n",
      "Iteration 3, inertia 6.751578225276941e+21\n",
      "Iteration 4, inertia 6.751527901122703e+21\n",
      "Converged at iteration 4: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.5473256468936317e+22\n",
      "Iteration 1, inertia 7.669407310122304e+21\n",
      "Iteration 2, inertia 6.785021144345801e+21\n",
      "Iteration 3, inertia 6.751578225276941e+21\n",
      "Iteration 4, inertia 6.751527901122703e+21\n",
      "Converged at iteration 4: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.22029525488498e+22\n",
      "Iteration 1, inertia 6.757567507020062e+21\n",
      "Iteration 2, inertia 6.75154024313579e+21\n",
      "Converged at iteration 2: center shift 9564279715416.354 within tolerance 13652364605003.256.\n",
      "Training a K-Means model with 3 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.5699998488497923e+21\n",
      "Iteration 1, inertia 3.3411726510906965e+21\n",
      "Iteration 2, inertia 2.96241187967338e+21\n",
      "Iteration 3, inertia 2.788759659177202e+21\n",
      "Iteration 4, inertia 2.720563833411057e+21\n",
      "Iteration 5, inertia 2.696296262041306e+21\n",
      "Iteration 6, inertia 2.681614579615278e+21\n",
      "Iteration 7, inertia 2.6762118044988924e+21\n",
      "Iteration 8, inertia 2.674072544568951e+21\n",
      "Iteration 9, inertia 2.673419883960502e+21\n",
      "Iteration 10, inertia 2.673177230484169e+21\n",
      "Iteration 11, inertia 2.672950571455087e+21\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.099304948849791e+21\n",
      "Iteration 1, inertia 2.6894878108861737e+21\n",
      "Iteration 2, inertia 2.6782882323012167e+21\n",
      "Iteration 3, inertia 2.675072487857673e+21\n",
      "Iteration 4, inertia 2.674191183097964e+21\n",
      "Iteration 5, inertia 2.6737213680174907e+21\n",
      "Iteration 6, inertia 2.673395763863916e+21\n",
      "Iteration 7, inertia 2.673108555719193e+21\n",
      "Converged at iteration 7: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.9778773488497915e+21\n",
      "Iteration 1, inertia 2.676211804498893e+21\n",
      "Iteration 2, inertia 2.674072544568951e+21\n",
      "Iteration 3, inertia 2.673419883960502e+21\n",
      "Iteration 4, inertia 2.673177230484169e+21\n",
      "Iteration 5, inertia 2.6729505714550866e+21\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.659330748849789e+21\n",
      "Iteration 1, inertia 3.0114925048179086e+21\n",
      "Iteration 2, inertia 2.8450822148962863e+21\n",
      "Iteration 3, inertia 2.7594551695321344e+21\n",
      "Iteration 4, inertia 2.7132706544807843e+21\n",
      "Iteration 5, inertia 2.688188310992907e+21\n",
      "Iteration 6, inertia 2.678170406718888e+21\n",
      "Iteration 7, inertia 2.675072487857673e+21\n",
      "Iteration 8, inertia 2.674191183097964e+21\n",
      "Iteration 9, inertia 2.6737213680174907e+21\n",
      "Iteration 10, inertia 2.6733957638639164e+21\n",
      "Iteration 11, inertia 2.673108555719193e+21\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.23957083832661e+21\n",
      "Iteration 1, inertia 2.6738374321540164e+21\n",
      "Iteration 2, inertia 2.673419883960502e+21\n",
      "Iteration 3, inertia 2.6731772304841696e+21\n",
      "Iteration 4, inertia 2.672950571455087e+21\n",
      "Converged at iteration 4: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5.760357538490499e+21\n",
      "Iteration 1, inertia 2.7642054825090833e+21\n",
      "Iteration 2, inertia 2.713994893476985e+21\n",
      "Iteration 3, inertia 2.6929851708552564e+21\n",
      "Iteration 4, inertia 2.6816145796152783e+21\n",
      "Iteration 5, inertia 2.6762118044988924e+21\n",
      "Iteration 6, inertia 2.674072544568951e+21\n",
      "Iteration 7, inertia 2.673419883960502e+21\n",
      "Iteration 8, inertia 2.6731772304841696e+21\n",
      "Iteration 9, inertia 2.672950571455087e+21\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.000002448849677e+21\n",
      "Iteration 1, inertia 2.729684708988385e+21\n",
      "Iteration 2, inertia 2.6980826653987255e+21\n",
      "Iteration 3, inertia 2.683636678580929e+21\n",
      "Iteration 4, inertia 2.676211804498893e+21\n",
      "Iteration 5, inertia 2.674072544568951e+21\n",
      "Iteration 6, inertia 2.6734198839605015e+21\n",
      "Iteration 7, inertia 2.673177230484169e+21\n",
      "Iteration 8, inertia 2.672950571455087e+21\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.2189528350923875e+21\n",
      "Iteration 1, inertia 2.7584721086370567e+21\n",
      "Iteration 2, inertia 2.712945759961348e+21\n",
      "Iteration 3, inertia 2.6881883109929066e+21\n",
      "Iteration 4, inertia 2.678170406718888e+21\n",
      "Iteration 5, inertia 2.6750724878576727e+21\n",
      "Iteration 6, inertia 2.674191183097963e+21\n",
      "Iteration 7, inertia 2.6737213680174907e+21\n",
      "Iteration 8, inertia 2.673395763863916e+21\n",
      "Iteration 9, inertia 2.673108555719193e+21\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.943426748849804e+21\n",
      "Iteration 1, inertia 2.7372178718512083e+21\n",
      "Iteration 2, inertia 2.7029670521357656e+21\n",
      "Iteration 3, inertia 2.6851378224330834e+21\n",
      "Iteration 4, inertia 2.677099486701332e+21\n",
      "Iteration 5, inertia 2.674072544568951e+21\n",
      "Iteration 6, inertia 2.673419883960502e+21\n",
      "Iteration 7, inertia 2.6731772304841696e+21\n",
      "Iteration 8, inertia 2.672950571455087e+21\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.3383946880386817e+21\n",
      "Iteration 1, inertia 2.941980551459038e+21\n",
      "Iteration 2, inertia 2.803454846044123e+21\n",
      "Iteration 3, inertia 2.7356873266708884e+21\n",
      "Iteration 4, inertia 2.7001678562922185e+21\n",
      "Iteration 5, inertia 2.682443742048128e+21\n",
      "Iteration 6, inertia 2.6763571566782233e+21\n",
      "Iteration 7, inertia 2.674445669537193e+21\n",
      "Iteration 8, inertia 2.6737213680174907e+21\n",
      "Iteration 9, inertia 2.673395763863916e+21\n",
      "Iteration 10, inertia 2.673108555719193e+21\n",
      "Converged at iteration 10: strict convergence.\n",
      "Training a K-Means model with 4 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.2209816488498168e+21\n",
      "Iteration 1, inertia 1.5478342985615333e+21\n",
      "Iteration 2, inertia 1.4797559057212852e+21\n",
      "Iteration 3, inertia 1.4690235070369494e+21\n",
      "Iteration 4, inertia 1.4644099156502787e+21\n",
      "Iteration 5, inertia 1.46333256856406e+21\n",
      "Iteration 6, inertia 1.4624030687304936e+21\n",
      "Iteration 7, inertia 1.4620935208077044e+21\n",
      "Iteration 8, inertia 1.4620205003883846e+21\n",
      "Iteration 9, inertia 1.4619179385326786e+21\n",
      "Iteration 10, inertia 1.4617805727878026e+21\n",
      "Converged at iteration 10: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.1674700488497475e+21\n",
      "Iteration 1, inertia 1.6435983740782418e+21\n",
      "Iteration 2, inertia 1.534589534526981e+21\n",
      "Iteration 3, inertia 1.5057417058376976e+21\n",
      "Iteration 4, inertia 1.494005614074209e+21\n",
      "Iteration 5, inertia 1.4864129822569744e+21\n",
      "Iteration 6, inertia 1.4814410188034508e+21\n",
      "Iteration 7, inertia 1.4782704724777763e+21\n",
      "Iteration 8, inertia 1.4754779084749322e+21\n",
      "Iteration 9, inertia 1.4719216031248805e+21\n",
      "Iteration 10, inertia 1.4694690493260088e+21\n",
      "Iteration 11, inertia 1.4675775075397135e+21\n",
      "Iteration 12, inertia 1.4658970949730102e+21\n",
      "Iteration 13, inertia 1.4643732846325373e+21\n",
      "Iteration 14, inertia 1.4636047110211625e+21\n",
      "Iteration 15, inertia 1.4628565282007779e+21\n",
      "Iteration 16, inertia 1.4625301350217118e+21\n",
      "Iteration 17, inertia 1.462292320111556e+21\n",
      "Converged at iteration 17: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.2351785488497996e+21\n",
      "Iteration 1, inertia 1.7907322847190235e+21\n",
      "Iteration 2, inertia 1.6530300944930628e+21\n",
      "Iteration 3, inertia 1.5808785649098477e+21\n",
      "Iteration 4, inertia 1.5424368325891478e+21\n",
      "Iteration 5, inertia 1.520826247784217e+21\n",
      "Iteration 6, inertia 1.5069170265995522e+21\n",
      "Iteration 7, inertia 1.4984262811063167e+21\n",
      "Iteration 8, inertia 1.4911376203069352e+21\n",
      "Iteration 9, inertia 1.4854457899182994e+21\n",
      "Iteration 10, inertia 1.4811551311180485e+21\n",
      "Iteration 11, inertia 1.4784503036603114e+21\n",
      "Iteration 12, inertia 1.475476883966184e+21\n",
      "Iteration 13, inertia 1.4719267868532922e+21\n",
      "Iteration 14, inertia 1.4694435733334464e+21\n",
      "Iteration 15, inertia 1.4675775075397133e+21\n",
      "Iteration 16, inertia 1.4658970949730105e+21\n",
      "Iteration 17, inertia 1.4643732846325376e+21\n",
      "Iteration 18, inertia 1.4636047110211625e+21\n",
      "Iteration 19, inertia 1.4628565282007779e+21\n",
      "Iteration 20, inertia 1.4625301350217115e+21\n",
      "Iteration 21, inertia 1.4622923201115558e+21\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.8067749488498133e+21\n",
      "Iteration 1, inertia 1.6147785055692288e+21\n",
      "Iteration 2, inertia 1.5215198272224276e+21\n",
      "Iteration 3, inertia 1.4975976390319166e+21\n",
      "Iteration 4, inertia 1.488682522052212e+21\n",
      "Iteration 5, inertia 1.4831382640052485e+21\n",
      "Iteration 6, inertia 1.4795271871323164e+21\n",
      "Iteration 7, inertia 1.4770771404317825e+21\n",
      "Iteration 8, inertia 1.4735953848326517e+21\n",
      "Iteration 9, inertia 1.4707779663623314e+21\n",
      "Iteration 10, inertia 1.4687694331963255e+21\n",
      "Iteration 11, inertia 1.4666101826367164e+21\n",
      "Iteration 12, inertia 1.464605315797157e+21\n",
      "Iteration 13, inertia 1.4638774449375092e+21\n",
      "Iteration 14, inertia 1.4628565282007779e+21\n",
      "Iteration 15, inertia 1.4625301350217115e+21\n",
      "Iteration 16, inertia 1.4622923201115558e+21\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.7671136488497965e+21\n",
      "Iteration 1, inertia 1.5385341751501204e+21\n",
      "Iteration 2, inertia 1.4991037512342792e+21\n",
      "Iteration 3, inertia 1.484071433597925e+21\n",
      "Iteration 4, inertia 1.4769767781685996e+21\n",
      "Iteration 5, inertia 1.471481112593419e+21\n",
      "Iteration 6, inertia 1.4660884774307813e+21\n",
      "Iteration 7, inertia 1.4638061804078553e+21\n",
      "Iteration 8, inertia 1.4626684836141854e+21\n",
      "Iteration 9, inertia 1.4621668338354498e+21\n",
      "Iteration 10, inertia 1.4620205003883846e+21\n",
      "Iteration 11, inertia 1.4619179385326786e+21\n",
      "Iteration 12, inertia 1.461780572787802e+21\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.058952448849792e+21\n",
      "Iteration 1, inertia 1.68429442169641e+21\n",
      "Iteration 2, inertia 1.6174733336350643e+21\n",
      "Iteration 3, inertia 1.5720969660770676e+21\n",
      "Iteration 4, inertia 1.5385348276683512e+21\n",
      "Iteration 5, inertia 1.518414047350775e+21\n",
      "Iteration 6, inertia 1.505706107118086e+21\n",
      "Iteration 7, inertia 1.496466484039381e+21\n",
      "Iteration 8, inertia 1.4894701701107726e+21\n",
      "Iteration 9, inertia 1.4846787438111383e+21\n",
      "Iteration 10, inertia 1.4800980513705052e+21\n",
      "Iteration 11, inertia 1.4777912267342448e+21\n",
      "Iteration 12, inertia 1.4744666601811151e+21\n",
      "Iteration 13, inertia 1.4712533356010086e+21\n",
      "Iteration 14, inertia 1.469061177389437e+21\n",
      "Iteration 15, inertia 1.466610182636717e+21\n",
      "Iteration 16, inertia 1.464605315797157e+21\n",
      "Iteration 17, inertia 1.4638774449375098e+21\n",
      "Iteration 18, inertia 1.4628565282007779e+21\n",
      "Iteration 19, inertia 1.4625301350217115e+21\n",
      "Iteration 20, inertia 1.4622923201115558e+21\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.9747898488498217e+21\n",
      "Iteration 1, inertia 1.669682174387907e+21\n",
      "Iteration 2, inertia 1.5975992139661637e+21\n",
      "Iteration 3, inertia 1.5572479825652736e+21\n",
      "Iteration 4, inertia 1.528009261012474e+21\n",
      "Iteration 5, inertia 1.512052352724839e+21\n",
      "Iteration 6, inertia 1.5018078774603484e+21\n",
      "Iteration 7, inertia 1.4949614008477e+21\n",
      "Iteration 8, inertia 1.4887548965630978e+21\n",
      "Iteration 9, inertia 1.483676462285739e+21\n",
      "Iteration 10, inertia 1.480098051370505e+21\n",
      "Iteration 11, inertia 1.4777912267342448e+21\n",
      "Iteration 12, inertia 1.4744666601811157e+21\n",
      "Iteration 13, inertia 1.471253335601009e+21\n",
      "Iteration 14, inertia 1.4690611773894366e+21\n",
      "Iteration 15, inertia 1.4666101826367167e+21\n",
      "Iteration 16, inertia 1.464605315797157e+21\n",
      "Iteration 17, inertia 1.4638774449375098e+21\n",
      "Iteration 18, inertia 1.4628565282007779e+21\n",
      "Iteration 19, inertia 1.4625301350217115e+21\n",
      "Iteration 20, inertia 1.4622923201115558e+21\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.36454202888254e+21\n",
      "Iteration 1, inertia 1.8079742227274085e+21\n",
      "Iteration 2, inertia 1.6729344419197593e+21\n",
      "Iteration 3, inertia 1.6016637701848598e+21\n",
      "Iteration 4, inertia 1.5582727974950124e+21\n",
      "Iteration 5, inertia 1.5293599914817455e+21\n",
      "Iteration 6, inertia 1.51277848531512e+21\n",
      "Iteration 7, inertia 1.5018078774603484e+21\n",
      "Iteration 8, inertia 1.4949614008477e+21\n",
      "Iteration 9, inertia 1.4887548965630976e+21\n",
      "Iteration 10, inertia 1.4836764622857387e+21\n",
      "Iteration 11, inertia 1.4800980513705052e+21\n",
      "Iteration 12, inertia 1.4777912267342448e+21\n",
      "Iteration 13, inertia 1.4744666601811154e+21\n",
      "Iteration 14, inertia 1.4712533356010086e+21\n",
      "Iteration 15, inertia 1.4690611773894364e+21\n",
      "Iteration 16, inertia 1.4666101826367167e+21\n",
      "Iteration 17, inertia 1.464605315797157e+21\n",
      "Iteration 18, inertia 1.46387744493751e+21\n",
      "Iteration 19, inertia 1.4628565282007779e+21\n",
      "Iteration 20, inertia 1.4625301350217115e+21\n",
      "Iteration 21, inertia 1.462292320111556e+21\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.6025560130386946e+21\n",
      "Iteration 1, inertia 1.471434184874916e+21\n",
      "Iteration 2, inertia 1.4633388535483906e+21\n",
      "Iteration 3, inertia 1.4619812439089267e+21\n",
      "Iteration 4, inertia 1.461625227869163e+21\n",
      "Iteration 5, inertia 1.461582449850418e+21\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.8453467488498127e+21\n",
      "Iteration 1, inertia 1.536243958445656e+21\n",
      "Iteration 2, inertia 1.4863518207955082e+21\n",
      "Iteration 3, inertia 1.474487355580234e+21\n",
      "Iteration 4, inertia 1.4697117696117574e+21\n",
      "Iteration 5, inertia 1.466871956078819e+21\n",
      "Iteration 6, inertia 1.4646292974550445e+21\n",
      "Iteration 7, inertia 1.4637595001916655e+21\n",
      "Iteration 8, inertia 1.4634296612427373e+21\n",
      "Iteration 9, inertia 1.4628565282007779e+21\n",
      "Iteration 10, inertia 1.4625301350217115e+21\n",
      "Iteration 11, inertia 1.4622923201115558e+21\n",
      "Converged at iteration 11: strict convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 5 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.3458528488498258e+21\n",
      "Iteration 1, inertia 1.1652164511586765e+21\n",
      "Iteration 2, inertia 1.0954193972233083e+21\n",
      "Iteration 3, inertia 1.0492316769190678e+21\n",
      "Iteration 4, inertia 1.0192519761183244e+21\n",
      "Iteration 5, inertia 9.96022626324601e+20\n",
      "Iteration 6, inertia 9.776119876824919e+20\n",
      "Iteration 7, inertia 9.622835962348477e+20\n",
      "Iteration 8, inertia 9.491845694867015e+20\n",
      "Iteration 9, inertia 9.400793394281147e+20\n",
      "Iteration 10, inertia 9.333262892866563e+20\n",
      "Iteration 11, inertia 9.283131343095683e+20\n",
      "Iteration 12, inertia 9.253942588143058e+20\n",
      "Iteration 13, inertia 9.235087498758763e+20\n",
      "Iteration 14, inertia 9.220948316636808e+20\n",
      "Iteration 15, inertia 9.210744673543297e+20\n",
      "Iteration 16, inertia 9.203418115469554e+20\n",
      "Iteration 17, inertia 9.1983601083899e+20\n",
      "Iteration 18, inertia 9.193127990815052e+20\n",
      "Iteration 19, inertia 9.189280635565184e+20\n",
      "Iteration 20, inertia 9.18653886217704e+20\n",
      "Iteration 21, inertia 9.183510966124395e+20\n",
      "Iteration 22, inertia 9.181754820554945e+20\n",
      "Iteration 23, inertia 9.181356364218267e+20\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.2826370513022626e+21\n",
      "Iteration 1, inertia 1.0140942276340199e+21\n",
      "Iteration 2, inertia 9.871005235870976e+20\n",
      "Iteration 3, inertia 9.676835921321675e+20\n",
      "Iteration 4, inertia 9.53574570737864e+20\n",
      "Iteration 5, inertia 9.433892733140595e+20\n",
      "Iteration 6, inertia 9.346607506012817e+20\n",
      "Iteration 7, inertia 9.290072140639733e+20\n",
      "Iteration 8, inertia 9.259965495946914e+20\n",
      "Iteration 9, inertia 9.242308512786147e+20\n",
      "Iteration 10, inertia 9.228693787765188e+20\n",
      "Iteration 11, inertia 9.221026734924649e+20\n",
      "Iteration 12, inertia 9.214319892077747e+20\n",
      "Iteration 13, inertia 9.208150325511207e+20\n",
      "Iteration 14, inertia 9.203460330176812e+20\n",
      "Iteration 15, inertia 9.199857702098707e+20\n",
      "Iteration 16, inertia 9.196255945826241e+20\n",
      "Iteration 17, inertia 9.190875433048468e+20\n",
      "Iteration 18, inertia 9.185355681972224e+20\n",
      "Iteration 19, inertia 9.18173224662982e+20\n",
      "Iteration 20, inertia 9.178325870818461e+20\n",
      "Iteration 21, inertia 9.177111870106222e+20\n",
      "Iteration 22, inertia 9.175477856790362e+20\n",
      "Iteration 23, inertia 9.174003682322291e+20\n",
      "Iteration 24, inertia 9.173688519135533e+20\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.214481448849773e+21\n",
      "Iteration 1, inertia 9.89966829080196e+20\n",
      "Iteration 2, inertia 9.606209183655466e+20\n",
      "Iteration 3, inertia 9.480009789958953e+20\n",
      "Iteration 4, inertia 9.389391667124425e+20\n",
      "Iteration 5, inertia 9.321042860446315e+20\n",
      "Iteration 6, inertia 9.273501148076111e+20\n",
      "Iteration 7, inertia 9.250350658277646e+20\n",
      "Iteration 8, inertia 9.234344653135336e+20\n",
      "Iteration 9, inertia 9.227351332352396e+20\n",
      "Iteration 10, inertia 9.221026734924652e+20\n",
      "Iteration 11, inertia 9.214319892077747e+20\n",
      "Iteration 12, inertia 9.208150325511208e+20\n",
      "Iteration 13, inertia 9.203460330176811e+20\n",
      "Iteration 14, inertia 9.199857702098707e+20\n",
      "Iteration 15, inertia 9.196255945826241e+20\n",
      "Iteration 16, inertia 9.190875433048468e+20\n",
      "Iteration 17, inertia 9.185355681972223e+20\n",
      "Iteration 18, inertia 9.181732246629821e+20\n",
      "Iteration 19, inertia 9.17832587081846e+20\n",
      "Iteration 20, inertia 9.177111870106223e+20\n",
      "Iteration 21, inertia 9.17547785679036e+20\n",
      "Iteration 22, inertia 9.174003682322291e+20\n",
      "Iteration 23, inertia 9.173688519135533e+20\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.0547548488498141e+21\n",
      "Iteration 1, inertia 9.237105795929017e+20\n",
      "Iteration 2, inertia 9.204322059882284e+20\n",
      "Iteration 3, inertia 9.196149699294867e+20\n",
      "Iteration 4, inertia 9.193127990815051e+20\n",
      "Iteration 5, inertia 9.189280635565184e+20\n",
      "Iteration 6, inertia 9.18653886217704e+20\n",
      "Iteration 7, inertia 9.183510966124396e+20\n",
      "Iteration 8, inertia 9.181754820554946e+20\n",
      "Iteration 9, inertia 9.181356364218266e+20\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.3679024488498022e+21\n",
      "Iteration 1, inertia 1.1547539732797286e+21\n",
      "Iteration 2, inertia 1.0881706496510262e+21\n",
      "Iteration 3, inertia 1.0501268933007354e+21\n",
      "Iteration 4, inertia 1.0184627883393859e+21\n",
      "Iteration 5, inertia 9.936248532605635e+20\n",
      "Iteration 6, inertia 9.719233822576867e+20\n",
      "Iteration 7, inertia 9.590368148257549e+20\n",
      "Iteration 8, inertia 9.459573863942901e+20\n",
      "Iteration 9, inertia 9.374921595829829e+20\n",
      "Iteration 10, inertia 9.309552371642317e+20\n",
      "Iteration 11, inertia 9.27350114807611e+20\n",
      "Iteration 12, inertia 9.250350658277645e+20\n",
      "Iteration 13, inertia 9.234344653135333e+20\n",
      "Iteration 14, inertia 9.227351332352393e+20\n",
      "Iteration 15, inertia 9.221026734924648e+20\n",
      "Iteration 16, inertia 9.214319892077747e+20\n",
      "Iteration 17, inertia 9.208150325511207e+20\n",
      "Iteration 18, inertia 9.203460330176812e+20\n",
      "Iteration 19, inertia 9.199857702098706e+20\n",
      "Iteration 20, inertia 9.196255945826241e+20\n",
      "Iteration 21, inertia 9.190875433048468e+20\n",
      "Iteration 22, inertia 9.185355681972224e+20\n",
      "Iteration 23, inertia 9.181732246629821e+20\n",
      "Iteration 24, inertia 9.178325870818461e+20\n",
      "Iteration 25, inertia 9.177111870106222e+20\n",
      "Iteration 26, inertia 9.17547785679036e+20\n",
      "Iteration 27, inertia 9.174003682322291e+20\n",
      "Iteration 28, inertia 9.173688519135533e+20\n",
      "Converged at iteration 28: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.3231135243725848e+21\n",
      "Iteration 1, inertia 9.664781580891044e+20\n",
      "Iteration 2, inertia 9.360856519091185e+20\n",
      "Iteration 3, inertia 9.235103962180228e+20\n",
      "Iteration 4, inertia 9.196285302406813e+20\n",
      "Iteration 5, inertia 9.181997437989096e+20\n",
      "Iteration 6, inertia 9.177545535538395e+20\n",
      "Iteration 7, inertia 9.176933400440414e+20\n",
      "Iteration 8, inertia 9.174050953672794e+20\n",
      "Iteration 9, inertia 9.172175929839478e+20\n",
      "Iteration 10, inertia 9.17083938187883e+20\n",
      "Iteration 11, inertia 9.170016429660289e+20\n",
      "Iteration 12, inertia 9.168508216566937e+20\n",
      "Iteration 13, inertia 9.16769633098795e+20\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.503372014118429e+21\n",
      "Iteration 1, inertia 1.1725629968808188e+21\n",
      "Iteration 2, inertia 1.07442323124864e+21\n",
      "Iteration 3, inertia 1.0288028506073998e+21\n",
      "Iteration 4, inertia 1.0015125307799722e+21\n",
      "Iteration 5, inertia 9.812056776768325e+20\n",
      "Iteration 6, inertia 9.650227289119933e+20\n",
      "Iteration 7, inertia 9.523605112391429e+20\n",
      "Iteration 8, inertia 9.425254329597107e+20\n",
      "Iteration 9, inertia 9.355742232203254e+20\n",
      "Iteration 10, inertia 9.289338869966538e+20\n",
      "Iteration 11, inertia 9.256737172720123e+20\n",
      "Iteration 12, inertia 9.238568341473492e+20\n",
      "Iteration 13, inertia 9.226069951265259e+20\n",
      "Iteration 14, inertia 9.210744673543297e+20\n",
      "Iteration 15, inertia 9.203418115469554e+20\n",
      "Iteration 16, inertia 9.198360108389899e+20\n",
      "Iteration 17, inertia 9.193127990815052e+20\n",
      "Iteration 18, inertia 9.189280635565184e+20\n",
      "Iteration 19, inertia 9.18653886217704e+20\n",
      "Iteration 20, inertia 9.183510966124396e+20\n",
      "Iteration 21, inertia 9.181754820554947e+20\n",
      "Iteration 22, inertia 9.181356364218266e+20\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1845382488497898e+21\n",
      "Iteration 1, inertia 9.432974831680715e+20\n",
      "Iteration 2, inertia 9.28358175802863e+20\n",
      "Iteration 3, inertia 9.22448389428233e+20\n",
      "Iteration 4, inertia 9.196366840598642e+20\n",
      "Iteration 5, inertia 9.186316242578862e+20\n",
      "Iteration 6, inertia 9.18184797706265e+20\n",
      "Iteration 7, inertia 9.175056083371373e+20\n",
      "Iteration 8, inertia 9.1718355684313e+20\n",
      "Iteration 9, inertia 9.171274847664365e+20\n",
      "Iteration 10, inertia 9.169716631788624e+20\n",
      "Iteration 11, inertia 9.168295541990742e+20\n",
      "Iteration 12, inertia 9.167696330987949e+20\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1545230895746911e+21\n",
      "Iteration 1, inertia 1.0046232257751381e+21\n",
      "Iteration 2, inertia 9.684504721376663e+20\n",
      "Iteration 3, inertia 9.518102918505698e+20\n",
      "Iteration 4, inertia 9.411924091568513e+20\n",
      "Iteration 5, inertia 9.323357023277068e+20\n",
      "Iteration 6, inertia 9.277572186528421e+20\n",
      "Iteration 7, inertia 9.253690177659615e+20\n",
      "Iteration 8, inertia 9.234344653135336e+20\n",
      "Iteration 9, inertia 9.227351332352392e+20\n",
      "Iteration 10, inertia 9.22102673492465e+20\n",
      "Iteration 11, inertia 9.214319892077747e+20\n",
      "Iteration 12, inertia 9.208150325511208e+20\n",
      "Iteration 13, inertia 9.203460330176812e+20\n",
      "Iteration 14, inertia 9.199857702098709e+20\n",
      "Iteration 15, inertia 9.196255945826241e+20\n",
      "Iteration 16, inertia 9.19087543304847e+20\n",
      "Iteration 17, inertia 9.185355681972224e+20\n",
      "Iteration 18, inertia 9.18173224662982e+20\n",
      "Iteration 19, inertia 9.178325870818458e+20\n",
      "Iteration 20, inertia 9.177111870106223e+20\n",
      "Iteration 21, inertia 9.17547785679036e+20\n",
      "Iteration 22, inertia 9.174003682322291e+20\n",
      "Iteration 23, inertia 9.173688519135533e+20\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.4555561488498085e+21\n",
      "Iteration 1, inertia 9.64859484084415e+20\n",
      "Iteration 2, inertia 9.353905370302587e+20\n",
      "Iteration 3, inertia 9.235227696248297e+20\n",
      "Iteration 4, inertia 9.190997049649191e+20\n",
      "Iteration 5, inertia 9.177985843383891e+20\n",
      "Iteration 6, inertia 9.173713657817828e+20\n",
      "Iteration 7, inertia 9.171835951870584e+20\n",
      "Iteration 8, inertia 9.170499403909933e+20\n",
      "Iteration 9, inertia 9.169131569005161e+20\n",
      "Iteration 10, inertia 9.167909005564146e+20\n",
      "Iteration 11, inertia 9.16769633098795e+20\n",
      "Converged at iteration 11: strict convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 6 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.097517848849828e+21\n",
      "Iteration 1, inertia 9.200000452569655e+20\n",
      "Iteration 2, inertia 8.740076451943112e+20\n",
      "Iteration 3, inertia 8.301106284247914e+20\n",
      "Iteration 4, inertia 7.95613338424029e+20\n",
      "Iteration 5, inertia 7.684667903957676e+20\n",
      "Iteration 6, inertia 7.502717868660169e+20\n",
      "Iteration 7, inertia 7.364227006594903e+20\n",
      "Iteration 8, inertia 7.231284831283467e+20\n",
      "Iteration 9, inertia 7.123095097275968e+20\n",
      "Iteration 10, inertia 7.01414937291618e+20\n",
      "Iteration 11, inertia 6.928918692885944e+20\n",
      "Iteration 12, inertia 6.858285516117461e+20\n",
      "Iteration 13, inertia 6.800438431107955e+20\n",
      "Iteration 14, inertia 6.753194166785399e+20\n",
      "Iteration 15, inertia 6.706362869518677e+20\n",
      "Iteration 16, inertia 6.656554199338678e+20\n",
      "Iteration 17, inertia 6.616284033203774e+20\n",
      "Iteration 18, inertia 6.583683804704712e+20\n",
      "Iteration 19, inertia 6.554054471245934e+20\n",
      "Iteration 20, inertia 6.535246207430264e+20\n",
      "Iteration 21, inertia 6.517510807193193e+20\n",
      "Iteration 22, inertia 6.508933063410142e+20\n",
      "Iteration 23, inertia 6.501752905727009e+20\n",
      "Iteration 24, inertia 6.49203523747927e+20\n",
      "Iteration 25, inertia 6.483179544071555e+20\n",
      "Iteration 26, inertia 6.474442592595685e+20\n",
      "Iteration 27, inertia 6.465681028533237e+20\n",
      "Iteration 28, inertia 6.45988844082787e+20\n",
      "Iteration 29, inertia 6.457413564302182e+20\n",
      "Iteration 30, inertia 6.454512102728034e+20\n",
      "Iteration 31, inertia 6.451170019462359e+20\n",
      "Iteration 32, inertia 6.44870483816345e+20\n",
      "Iteration 33, inertia 6.446916244920811e+20\n",
      "Iteration 34, inertia 6.446257947940422e+20\n",
      "Iteration 35, inertia 6.445499812561829e+20\n",
      "Iteration 36, inertia 6.445229248309271e+20\n",
      "Converged at iteration 36: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.033234488498073e+20\n",
      "Iteration 1, inertia 6.710287812464002e+20\n",
      "Iteration 2, inertia 6.5738188238303e+20\n",
      "Iteration 3, inertia 6.508252385461904e+20\n",
      "Iteration 4, inertia 6.467611425282595e+20\n",
      "Iteration 5, inertia 6.450576677708878e+20\n",
      "Iteration 6, inertia 6.44106193621355e+20\n",
      "Iteration 7, inertia 6.436910290202018e+20\n",
      "Iteration 8, inertia 6.436491839306644e+20\n",
      "Iteration 9, inertia 6.435874312588318e+20\n",
      "Iteration 10, inertia 6.434815054397025e+20\n",
      "Iteration 11, inertia 6.434736992843954e+20\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.860374488498238e+20\n",
      "Iteration 1, inertia 8.278661985771419e+20\n",
      "Iteration 2, inertia 7.859317408627162e+20\n",
      "Iteration 3, inertia 7.5992865128149e+20\n",
      "Iteration 4, inertia 7.433099102640874e+20\n",
      "Iteration 5, inertia 7.292926033373756e+20\n",
      "Iteration 6, inertia 7.169230138522786e+20\n",
      "Iteration 7, inertia 7.069824293549982e+20\n",
      "Iteration 8, inertia 6.964713335468838e+20\n",
      "Iteration 9, inertia 6.894483667929541e+20\n",
      "Iteration 10, inertia 6.829841015140669e+20\n",
      "Iteration 11, inertia 6.78167557809174e+20\n",
      "Iteration 12, inertia 6.731582398465586e+20\n",
      "Iteration 13, inertia 6.680658445172414e+20\n",
      "Iteration 14, inertia 6.631818449350064e+20\n",
      "Iteration 15, inertia 6.593114560251497e+20\n",
      "Iteration 16, inertia 6.56566588168634e+20\n",
      "Iteration 17, inertia 6.545619070407866e+20\n",
      "Iteration 18, inertia 6.526576527838235e+20\n",
      "Iteration 19, inertia 6.513113304222176e+20\n",
      "Iteration 20, inertia 6.505789783685802e+20\n",
      "Iteration 21, inertia 6.49513057692492e+20\n",
      "Iteration 22, inertia 6.487181114345789e+20\n",
      "Iteration 23, inertia 6.479807382806054e+20\n",
      "Iteration 24, inertia 6.468298695426183e+20\n",
      "Iteration 25, inertia 6.46037189694639e+20\n",
      "Iteration 26, inertia 6.457413564302182e+20\n",
      "Iteration 27, inertia 6.454512102728035e+20\n",
      "Iteration 28, inertia 6.451170019462362e+20\n",
      "Iteration 29, inertia 6.44870483816345e+20\n",
      "Iteration 30, inertia 6.44691624492081e+20\n",
      "Iteration 31, inertia 6.446257947940422e+20\n",
      "Iteration 32, inertia 6.445499812561829e+20\n",
      "Iteration 33, inertia 6.445229248309271e+20\n",
      "Converged at iteration 33: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.598298248498878e+20\n",
      "Iteration 1, inertia 8.360696632451885e+20\n",
      "Iteration 2, inertia 8.216999161585205e+20\n",
      "Iteration 3, inertia 8.132536396818649e+20\n",
      "Iteration 4, inertia 8.073437980376418e+20\n",
      "Iteration 5, inertia 8.020927343208421e+20\n",
      "Iteration 6, inertia 7.994703477270976e+20\n",
      "Iteration 7, inertia 7.979949791427069e+20\n",
      "Iteration 8, inertia 7.971651428253893e+20\n",
      "Iteration 9, inertia 7.965282779550047e+20\n",
      "Iteration 10, inertia 7.960070815096655e+20\n",
      "Iteration 11, inertia 7.95479622586875e+20\n",
      "Iteration 12, inertia 7.948872565704629e+20\n",
      "Iteration 13, inertia 7.943654053933723e+20\n",
      "Iteration 14, inertia 7.941489710775129e+20\n",
      "Iteration 15, inertia 7.941326542957927e+20\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.741371488497162e+20\n",
      "Iteration 1, inertia 6.681592766211499e+20\n",
      "Iteration 2, inertia 6.583374888796718e+20\n",
      "Iteration 3, inertia 6.532034235996389e+20\n",
      "Iteration 4, inertia 6.503806267775493e+20\n",
      "Iteration 5, inertia 6.487328146035397e+20\n",
      "Iteration 6, inertia 6.478878956338113e+20\n",
      "Iteration 7, inertia 6.472131992886966e+20\n",
      "Iteration 8, inertia 6.464966658944488e+20\n",
      "Iteration 9, inertia 6.459678855199714e+20\n",
      "Iteration 10, inertia 6.454190329611992e+20\n",
      "Iteration 11, inertia 6.451476513468361e+20\n",
      "Iteration 12, inertia 6.447967151665174e+20\n",
      "Iteration 13, inertia 6.44553847800343e+20\n",
      "Iteration 14, inertia 6.443781976658898e+20\n",
      "Iteration 15, inertia 6.442081226257224e+20\n",
      "Iteration 16, inertia 6.439152990223775e+20\n",
      "Iteration 17, inertia 6.438446002742956e+20\n",
      "Iteration 18, inertia 6.438216655061968e+20\n",
      "Iteration 19, inertia 6.436437813000651e+20\n",
      "Iteration 20, inertia 6.43594513206565e+20\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.207839488498131e+20\n",
      "Iteration 1, inertia 7.083310760450906e+20\n",
      "Iteration 2, inertia 6.74623574192611e+20\n",
      "Iteration 3, inertia 6.618346184260914e+20\n",
      "Iteration 4, inertia 6.560776519885054e+20\n",
      "Iteration 5, inertia 6.534209470663779e+20\n",
      "Iteration 6, inertia 6.518247318785688e+20\n",
      "Iteration 7, inertia 6.50300007010434e+20\n",
      "Iteration 8, inertia 6.496988469214035e+20\n",
      "Iteration 9, inertia 6.49110890904387e+20\n",
      "Iteration 10, inertia 6.484482373563542e+20\n",
      "Iteration 11, inertia 6.47757217221379e+20\n",
      "Iteration 12, inertia 6.46960350460772e+20\n",
      "Iteration 13, inertia 6.46328754154488e+20\n",
      "Iteration 14, inertia 6.46014990454753e+20\n",
      "Iteration 15, inertia 6.455131755918768e+20\n",
      "Iteration 16, inertia 6.449724033322779e+20\n",
      "Iteration 17, inertia 6.447667688819073e+20\n",
      "Iteration 18, inertia 6.445538478003433e+20\n",
      "Iteration 19, inertia 6.443781976658898e+20\n",
      "Iteration 20, inertia 6.442081226257224e+20\n",
      "Iteration 21, inertia 6.439152990223773e+20\n",
      "Iteration 22, inertia 6.438446002742957e+20\n",
      "Iteration 23, inertia 6.438216655061968e+20\n",
      "Iteration 24, inertia 6.436437813000651e+20\n",
      "Iteration 25, inertia 6.43594513206565e+20\n",
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.832468625027089e+20\n",
      "Iteration 1, inertia 6.615141426418031e+20\n",
      "Iteration 2, inertia 6.50750788362879e+20\n",
      "Iteration 3, inertia 6.465606236178421e+20\n",
      "Iteration 4, inertia 6.444297649465038e+20\n",
      "Iteration 5, inertia 6.43836806525499e+20\n",
      "Iteration 6, inertia 6.4355121100409e+20\n",
      "Iteration 7, inertia 6.434815054397026e+20\n",
      "Iteration 8, inertia 6.434736992843951e+20\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1394120088498872e+21\n",
      "Iteration 1, inertia 8.66859358039598e+20\n",
      "Iteration 2, inertia 8.164145429003769e+20\n",
      "Iteration 3, inertia 8.030436767036973e+20\n",
      "Iteration 4, inertia 7.98919898110332e+20\n",
      "Iteration 5, inertia 7.965201420710393e+20\n",
      "Iteration 6, inertia 7.953916784843758e+20\n",
      "Iteration 7, inertia 7.947576672543825e+20\n",
      "Iteration 8, inertia 7.942568750366736e+20\n",
      "Iteration 9, inertia 7.938989844442776e+20\n",
      "Iteration 10, inertia 7.938165211281544e+20\n",
      "Iteration 11, inertia 7.937240019451766e+20\n",
      "Iteration 12, inertia 7.93666508722931e+20\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.116825640265177e+20\n",
      "Iteration 1, inertia 7.132649309009761e+20\n",
      "Iteration 2, inertia 6.956546180274705e+20\n",
      "Iteration 3, inertia 6.828479095531024e+20\n",
      "Iteration 4, inertia 6.7490936703677e+20\n",
      "Iteration 5, inertia 6.698752465747282e+20\n",
      "Iteration 6, inertia 6.653978791555629e+20\n",
      "Iteration 7, inertia 6.614631414071972e+20\n",
      "Iteration 8, inertia 6.577495177359857e+20\n",
      "Iteration 9, inertia 6.553788643042675e+20\n",
      "Iteration 10, inertia 6.53147061957189e+20\n",
      "Iteration 11, inertia 6.517699849188457e+20\n",
      "Iteration 12, inertia 6.510112294667815e+20\n",
      "Iteration 13, inertia 6.50219079468662e+20\n",
      "Iteration 14, inertia 6.496809728633969e+20\n",
      "Iteration 15, inertia 6.490302791129487e+20\n",
      "Iteration 16, inertia 6.482403598639717e+20\n",
      "Iteration 17, inertia 6.47199972168048e+20\n",
      "Iteration 18, inertia 6.465850016882764e+20\n",
      "Iteration 19, inertia 6.461107204588811e+20\n",
      "Iteration 20, inertia 6.455131755918768e+20\n",
      "Iteration 21, inertia 6.449724033322779e+20\n",
      "Iteration 22, inertia 6.447667688819074e+20\n",
      "Iteration 23, inertia 6.445538478003433e+20\n",
      "Iteration 24, inertia 6.443781976658896e+20\n",
      "Iteration 25, inertia 6.442081226257224e+20\n",
      "Iteration 26, inertia 6.439152990223775e+20\n",
      "Iteration 27, inertia 6.438446002742957e+20\n",
      "Iteration 28, inertia 6.438216655061966e+20\n",
      "Iteration 29, inertia 6.436437813000651e+20\n",
      "Iteration 30, inertia 6.435945132065653e+20\n",
      "Converged at iteration 30: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.38165263849887e+20\n",
      "Iteration 1, inertia 8.656325043982558e+20\n",
      "Iteration 2, inertia 8.382362849722382e+20\n",
      "Iteration 3, inertia 8.23468567878831e+20\n",
      "Iteration 4, inertia 8.127189957177136e+20\n",
      "Iteration 5, inertia 8.069254562864079e+20\n",
      "Iteration 6, inertia 8.037694151398757e+20\n",
      "Iteration 7, inertia 8.019934949699319e+20\n",
      "Iteration 8, inertia 8.005156507598805e+20\n",
      "Iteration 9, inertia 7.996918323075407e+20\n",
      "Iteration 10, inertia 7.989717417483825e+20\n",
      "Iteration 11, inertia 7.983008931115986e+20\n",
      "Iteration 12, inertia 7.976162870829124e+20\n",
      "Iteration 13, inertia 7.972186448022653e+20\n",
      "Iteration 14, inertia 7.96399092975191e+20\n",
      "Iteration 15, inertia 7.95824918425151e+20\n",
      "Iteration 16, inertia 7.954100233089721e+20\n",
      "Iteration 17, inertia 7.950615117780606e+20\n",
      "Iteration 18, inertia 7.949321988054043e+20\n",
      "Iteration 19, inertia 7.947738500119631e+20\n",
      "Iteration 20, inertia 7.94626432565156e+20\n",
      "Iteration 21, inertia 7.9459491624648e+20\n",
      "Converged at iteration 21: strict convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 7 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.870778488498285e+20\n",
      "Iteration 1, inertia 6.17409879753863e+20\n",
      "Iteration 2, inertia 5.8212805795140534e+20\n",
      "Iteration 3, inertia 5.6513319440521914e+20\n",
      "Iteration 4, inertia 5.5294170397896743e+20\n",
      "Iteration 5, inertia 5.437351341513716e+20\n",
      "Iteration 6, inertia 5.360258110780798e+20\n",
      "Iteration 7, inertia 5.297661077571672e+20\n",
      "Iteration 8, inertia 5.2477680588165605e+20\n",
      "Iteration 9, inertia 5.2064195475204394e+20\n",
      "Iteration 10, inertia 5.1718503897767235e+20\n",
      "Iteration 11, inertia 5.1341963645965815e+20\n",
      "Iteration 12, inertia 5.1004890020138045e+20\n",
      "Iteration 13, inertia 5.0691499233545316e+20\n",
      "Iteration 14, inertia 5.0465015694163555e+20\n",
      "Iteration 15, inertia 5.035328562513909e+20\n",
      "Iteration 16, inertia 5.025156077693998e+20\n",
      "Iteration 17, inertia 5.017670990387999e+20\n",
      "Iteration 18, inertia 5.011850866787833e+20\n",
      "Iteration 19, inertia 5.006377236155682e+20\n",
      "Iteration 20, inertia 5.0030572042937945e+20\n",
      "Iteration 21, inertia 4.9998080970423547e+20\n",
      "Iteration 22, inertia 4.9980272788111386e+20\n",
      "Iteration 23, inertia 4.995191200033968e+20\n",
      "Iteration 24, inertia 4.991675063358445e+20\n",
      "Iteration 25, inertia 4.9903282483034574e+20\n",
      "Iteration 26, inertia 4.989108689139902e+20\n",
      "Iteration 27, inertia 4.988737241940995e+20\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.813450488498306e+20\n",
      "Iteration 1, inertia 5.280135616434979e+20\n",
      "Iteration 2, inertia 5.10825085760981e+20\n",
      "Iteration 3, inertia 5.028466465665438e+20\n",
      "Iteration 4, inertia 4.997539694643168e+20\n",
      "Iteration 5, inertia 4.980145321127087e+20\n",
      "Iteration 6, inertia 4.97176505629919e+20\n",
      "Iteration 7, inertia 4.967121943562897e+20\n",
      "Iteration 8, inertia 4.966064723216197e+20\n",
      "Iteration 9, inertia 4.964171124252994e+20\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.358453488498172e+20\n",
      "Iteration 1, inertia 5.262602740363341e+20\n",
      "Iteration 2, inertia 5.121054615797668e+20\n",
      "Iteration 3, inertia 5.0667489682429346e+20\n",
      "Iteration 4, inertia 5.033490948838949e+20\n",
      "Iteration 5, inertia 5.018029668182827e+20\n",
      "Iteration 6, inertia 5.007866572253011e+20\n",
      "Iteration 7, inertia 5.003330282566959e+20\n",
      "Iteration 8, inertia 4.9984489873908885e+20\n",
      "Iteration 9, inertia 4.9956020496614464e+20\n",
      "Iteration 10, inertia 4.991261644344658e+20\n",
      "Iteration 11, inertia 4.9891086891399034e+20\n",
      "Iteration 12, inertia 4.988737241940995e+20\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.2516750738498855e+21\n",
      "Iteration 1, inertia 6.015727992538878e+20\n",
      "Iteration 2, inertia 5.501682412871902e+20\n",
      "Iteration 3, inertia 5.3569526958431044e+20\n",
      "Iteration 4, inertia 5.296014061832745e+20\n",
      "Iteration 5, inertia 5.265361768406552e+20\n",
      "Iteration 6, inertia 5.2495571133948586e+20\n",
      "Iteration 7, inertia 5.239548477841055e+20\n",
      "Iteration 8, inertia 5.2327639687307683e+20\n",
      "Iteration 9, inertia 5.22948299471705e+20\n",
      "Iteration 10, inertia 5.227633001392911e+20\n",
      "Iteration 11, inertia 5.226509234735961e+20\n",
      "Converged at iteration 11: center shift 10234705649738.555 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.57488448849562e+20\n",
      "Iteration 1, inertia 5.9027245714486015e+20\n",
      "Iteration 2, inertia 5.6013777868848464e+20\n",
      "Iteration 3, inertia 5.391220682834565e+20\n",
      "Iteration 4, inertia 5.245784307488522e+20\n",
      "Iteration 5, inertia 5.144811361351028e+20\n",
      "Iteration 6, inertia 5.066679781013181e+20\n",
      "Iteration 7, inertia 5.017358510916412e+20\n",
      "Iteration 8, inertia 4.9884031647996864e+20\n",
      "Iteration 9, inertia 4.9808437714079783e+20\n",
      "Iteration 10, inertia 4.977048744684801e+20\n",
      "Iteration 11, inertia 4.975018576395925e+20\n",
      "Iteration 12, inertia 4.974685529637174e+20\n",
      "Iteration 13, inertia 4.974226780372574e+20\n",
      "Iteration 14, inertia 4.973623580013009e+20\n",
      "Iteration 15, inertia 4.972777099089999e+20\n",
      "Iteration 16, inertia 4.971274951483903e+20\n",
      "Converged at iteration 16: center shift 10912929008839.898 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.67538362502709e+20\n",
      "Iteration 1, inertia 5.6098048094808965e+20\n",
      "Iteration 2, inertia 5.497616425587721e+20\n",
      "Iteration 3, inertia 5.399456333376419e+20\n",
      "Iteration 4, inertia 5.300102660614697e+20\n",
      "Iteration 5, inertia 5.21375062331951e+20\n",
      "Iteration 6, inertia 5.1502688254648674e+20\n",
      "Iteration 7, inertia 5.115226417012621e+20\n",
      "Iteration 8, inertia 5.092683960042064e+20\n",
      "Iteration 9, inertia 5.067497200875071e+20\n",
      "Iteration 10, inertia 5.048562046903796e+20\n",
      "Iteration 11, inertia 5.0327706453063434e+20\n",
      "Iteration 12, inertia 5.0242889897816044e+20\n",
      "Iteration 13, inertia 5.0156253189652454e+20\n",
      "Iteration 14, inertia 5.010193986939683e+20\n",
      "Iteration 15, inertia 5.0050355698886e+20\n",
      "Iteration 16, inertia 5.0007475325596697e+20\n",
      "Iteration 17, inertia 4.9982234558741394e+20\n",
      "Iteration 18, inertia 4.995191200033968e+20\n",
      "Iteration 19, inertia 4.9916750633584466e+20\n",
      "Iteration 20, inertia 4.9903282483034574e+20\n",
      "Iteration 21, inertia 4.9891086891399034e+20\n",
      "Iteration 22, inertia 4.9887372419409936e+20\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.865305488498335e+20\n",
      "Iteration 1, inertia 5.449125969378834e+20\n",
      "Iteration 2, inertia 5.205737948854464e+20\n",
      "Iteration 3, inertia 5.0750089181278005e+20\n",
      "Iteration 4, inertia 5.0146194130191444e+20\n",
      "Iteration 5, inertia 4.994327374512465e+20\n",
      "Iteration 6, inertia 4.979647699726829e+20\n",
      "Iteration 7, inertia 4.971108990626359e+20\n",
      "Iteration 8, inertia 4.96866991582365e+20\n",
      "Iteration 9, inertia 4.9675309095147825e+20\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.333367978995338e+20\n",
      "Iteration 1, inertia 5.49378052438833e+20\n",
      "Iteration 2, inertia 5.366290689186042e+20\n",
      "Iteration 3, inertia 5.2842624089208036e+20\n",
      "Iteration 4, inertia 5.211510899119706e+20\n",
      "Iteration 5, inertia 5.160683447098638e+20\n",
      "Iteration 6, inertia 5.1217095844580753e+20\n",
      "Iteration 7, inertia 5.088651677450058e+20\n",
      "Iteration 8, inertia 5.062061583812488e+20\n",
      "Iteration 9, inertia 5.0414180952388724e+20\n",
      "Iteration 10, inertia 5.024233152024723e+20\n",
      "Iteration 11, inertia 5.0162159172103406e+20\n",
      "Iteration 12, inertia 5.011554983542179e+20\n",
      "Iteration 13, inertia 5.007079552794195e+20\n",
      "Iteration 14, inertia 5.003429716655341e+20\n",
      "Iteration 15, inertia 4.997982985375959e+20\n",
      "Iteration 16, inertia 4.989912852180014e+20\n",
      "Iteration 17, inertia 4.981900323235354e+20\n",
      "Iteration 18, inertia 4.976886699093241e+20\n",
      "Iteration 19, inertia 4.9744567423089266e+20\n",
      "Iteration 20, inertia 4.9725801120841164e+20\n",
      "Iteration 21, inertia 4.9721443472315836e+20\n",
      "Iteration 22, inertia 4.9712953977181785e+20\n",
      "Iteration 23, inertia 4.9707550860481515e+20\n",
      "Iteration 24, inertia 4.9699971445320594e+20\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.77338848849811e+20\n",
      "Iteration 1, inertia 5.48749971030607e+20\n",
      "Iteration 2, inertia 5.434321675933347e+20\n",
      "Iteration 3, inertia 5.385568196742295e+20\n",
      "Iteration 4, inertia 5.337455901997949e+20\n",
      "Iteration 5, inertia 5.290782119653476e+20\n",
      "Iteration 6, inertia 5.251848473457227e+20\n",
      "Iteration 7, inertia 5.2264829876686796e+20\n",
      "Iteration 8, inertia 5.200914990675921e+20\n",
      "Iteration 9, inertia 5.17207446507271e+20\n",
      "Iteration 10, inertia 5.1343501965097855e+20\n",
      "Iteration 11, inertia 5.108975719982128e+20\n",
      "Iteration 12, inertia 5.083283420445409e+20\n",
      "Iteration 13, inertia 5.057326274626378e+20\n",
      "Iteration 14, inertia 5.035754847584879e+20\n",
      "Iteration 15, inertia 5.021533554463867e+20\n",
      "Iteration 16, inertia 5.012279637432971e+20\n",
      "Iteration 17, inertia 5.007079552794193e+20\n",
      "Iteration 18, inertia 5.003429716655341e+20\n",
      "Iteration 19, inertia 4.997982985375958e+20\n",
      "Iteration 20, inertia 4.989912852180015e+20\n",
      "Iteration 21, inertia 4.9819003232353563e+20\n",
      "Iteration 22, inertia 4.97688669909324e+20\n",
      "Iteration 23, inertia 4.974456742308925e+20\n",
      "Iteration 24, inertia 4.9725801120841164e+20\n",
      "Iteration 25, inertia 4.972144347231584e+20\n",
      "Iteration 26, inertia 4.9712953977181785e+20\n",
      "Iteration 27, inertia 4.970755086048152e+20\n",
      "Iteration 28, inertia 4.9699971445320594e+20\n",
      "Converged at iteration 28: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.74794875849888e+20\n",
      "Iteration 1, inertia 5.358963913439328e+20\n",
      "Iteration 2, inertia 5.2442244045094866e+20\n",
      "Iteration 3, inertia 5.227939413635096e+20\n",
      "Iteration 4, inertia 5.2262448998615594e+20\n",
      "Iteration 5, inertia 5.224869575432419e+20\n",
      "Converged at iteration 5: strict convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 8 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5.077936488498316e+20\n",
      "Iteration 1, inertia 4.087914902179222e+20\n",
      "Iteration 2, inertia 4.0426342261944726e+20\n",
      "Iteration 3, inertia 4.0278857174942024e+20\n",
      "Iteration 4, inertia 4.0187887515523796e+20\n",
      "Iteration 5, inertia 4.014285447401095e+20\n",
      "Iteration 6, inertia 4.0109970460853384e+20\n",
      "Iteration 7, inertia 4.007897091266193e+20\n",
      "Iteration 8, inertia 4.0054466914767634e+20\n",
      "Iteration 9, inertia 4.003477561620386e+20\n",
      "Iteration 10, inertia 4.0020154540836245e+20\n",
      "Iteration 11, inertia 4.0008752119656625e+20\n",
      "Iteration 12, inertia 3.999475371726883e+20\n",
      "Iteration 13, inertia 3.998610117117113e+20\n",
      "Iteration 14, inertia 3.996958403920285e+20\n",
      "Iteration 15, inertia 3.9960873435870154e+20\n",
      "Iteration 16, inertia 3.9958654670365334e+20\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.4261869084988794e+20\n",
      "Iteration 1, inertia 4.0884177635571145e+20\n",
      "Iteration 2, inertia 3.963660948228859e+20\n",
      "Iteration 3, inertia 3.890348781847738e+20\n",
      "Iteration 4, inertia 3.851004411517717e+20\n",
      "Iteration 5, inertia 3.827664467508674e+20\n",
      "Iteration 6, inertia 3.813127304436969e+20\n",
      "Iteration 7, inertia 3.805894934703236e+20\n",
      "Iteration 8, inertia 3.8008967277000006e+20\n",
      "Iteration 9, inertia 3.796799776472786e+20\n",
      "Iteration 10, inertia 3.79653657306408e+20\n",
      "Converged at iteration 10: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.4505509630593486e+20\n",
      "Iteration 1, inertia 3.900781779448329e+20\n",
      "Iteration 2, inertia 3.8223474499531165e+20\n",
      "Iteration 3, inertia 3.7990708775945994e+20\n",
      "Iteration 4, inertia 3.7934072260108065e+20\n",
      "Iteration 5, inertia 3.791665832769905e+20\n",
      "Iteration 6, inertia 3.790818449434212e+20\n",
      "Iteration 7, inertia 3.789767337696227e+20\n",
      "Converged at iteration 7: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.78575608849888e+20\n",
      "Iteration 1, inertia 4.1327158494348686e+20\n",
      "Iteration 2, inertia 3.9851761263905695e+20\n",
      "Iteration 3, inertia 3.923809066166736e+20\n",
      "Iteration 4, inertia 3.881263223414936e+20\n",
      "Iteration 5, inertia 3.853619216218217e+20\n",
      "Iteration 6, inertia 3.8379719929594066e+20\n",
      "Iteration 7, inertia 3.8278682474541246e+20\n",
      "Iteration 8, inertia 3.8226369180044696e+20\n",
      "Iteration 9, inertia 3.816739777985139e+20\n",
      "Iteration 10, inertia 3.812080444165836e+20\n",
      "Iteration 11, inertia 3.809421589135376e+20\n",
      "Iteration 12, inertia 3.8078284262776334e+20\n",
      "Iteration 13, inertia 3.8065804088133177e+20\n",
      "Iteration 14, inertia 3.804957496955939e+20\n",
      "Iteration 15, inertia 3.8028869231911096e+20\n",
      "Iteration 16, inertia 3.800969392465748e+20\n",
      "Iteration 17, inertia 3.8008895637334544e+20\n",
      "Converged at iteration 17: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5.3172343428355405e+20\n",
      "Iteration 1, inertia 4.52896311162145e+20\n",
      "Iteration 2, inertia 4.220820921038768e+20\n",
      "Iteration 3, inertia 4.016099876867257e+20\n",
      "Iteration 4, inertia 3.9059423415323905e+20\n",
      "Iteration 5, inertia 3.8593401193763904e+20\n",
      "Iteration 6, inertia 3.82357555154877e+20\n",
      "Iteration 7, inertia 3.806664483774482e+20\n",
      "Iteration 8, inertia 3.7974451982976936e+20\n",
      "Iteration 9, inertia 3.791502337913008e+20\n",
      "Iteration 10, inertia 3.789762361663327e+20\n",
      "Iteration 11, inertia 3.788785886017858e+20\n",
      "Iteration 12, inertia 3.788569228004186e+20\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.355042438314315e+20\n",
      "Iteration 1, inertia 3.805081354187869e+20\n",
      "Iteration 2, inertia 3.7945682309638986e+20\n",
      "Iteration 3, inertia 3.7912454934960636e+20\n",
      "Iteration 4, inertia 3.7908303069849295e+20\n",
      "Converged at iteration 4: center shift 9157541781668.496 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5.2315844484988646e+20\n",
      "Iteration 1, inertia 4.063755160699205e+20\n",
      "Iteration 2, inertia 3.858337648608056e+20\n",
      "Iteration 3, inertia 3.808388802719487e+20\n",
      "Iteration 4, inertia 3.794612497778665e+20\n",
      "Iteration 5, inertia 3.7900432040260495e+20\n",
      "Iteration 6, inertia 3.7889924508819076e+20\n",
      "Converged at iteration 6: center shift 8245063263042.406 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 5.322659488498192e+20\n",
      "Iteration 1, inertia 4.147515677059666e+20\n",
      "Iteration 2, inertia 4.0999322448012535e+20\n",
      "Iteration 3, inertia 4.076761554544463e+20\n",
      "Iteration 4, inertia 4.059363212981642e+20\n",
      "Iteration 5, inertia 4.047766594346277e+20\n",
      "Iteration 6, inertia 4.040535068678015e+20\n",
      "Iteration 7, inertia 4.0358847891655564e+20\n",
      "Iteration 8, inertia 4.031948291921514e+20\n",
      "Iteration 9, inertia 4.0265773344371684e+20\n",
      "Iteration 10, inertia 4.023075558473706e+20\n",
      "Iteration 11, inertia 4.0203563180827404e+20\n",
      "Iteration 12, inertia 4.018056070747775e+20\n",
      "Iteration 13, inertia 4.016911719745002e+20\n",
      "Iteration 14, inertia 4.014755055399904e+20\n",
      "Iteration 15, inertia 4.0129072314906857e+20\n",
      "Iteration 16, inertia 4.011068181722702e+20\n",
      "Iteration 17, inertia 4.008177890609537e+20\n",
      "Iteration 18, inertia 4.006222175706924e+20\n",
      "Iteration 19, inertia 4.003536690914608e+20\n",
      "Iteration 20, inertia 4.002160549414954e+20\n",
      "Iteration 21, inertia 4.001216178411828e+20\n",
      "Iteration 22, inertia 4.000216200150356e+20\n",
      "Iteration 23, inertia 4.0000251959470824e+20\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.888296308498867e+20\n",
      "Iteration 1, inertia 4.1989578305495866e+20\n",
      "Iteration 2, inertia 4.078764292613722e+20\n",
      "Iteration 3, inertia 3.974936643629663e+20\n",
      "Iteration 4, inertia 3.919033561120423e+20\n",
      "Iteration 5, inertia 3.888407282874156e+20\n",
      "Iteration 6, inertia 3.869732873095074e+20\n",
      "Iteration 7, inertia 3.844872822769376e+20\n",
      "Iteration 8, inertia 3.829320797197995e+20\n",
      "Iteration 9, inertia 3.821271515819503e+20\n",
      "Iteration 10, inertia 3.8153217329319694e+20\n",
      "Iteration 11, inertia 3.81213571555397e+20\n",
      "Iteration 12, inertia 3.809408636165659e+20\n",
      "Iteration 13, inertia 3.808860343202628e+20\n",
      "Iteration 14, inertia 3.807828426277634e+20\n",
      "Iteration 15, inertia 3.806580408813319e+20\n",
      "Iteration 16, inertia 3.80495749695594e+20\n",
      "Iteration 17, inertia 3.802886923191108e+20\n",
      "Iteration 18, inertia 3.800969392465748e+20\n",
      "Iteration 19, inertia 3.800889563733455e+20\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.429585808498874e+20\n",
      "Iteration 1, inertia 3.9815074711748896e+20\n",
      "Iteration 2, inertia 3.890388269239509e+20\n",
      "Iteration 3, inertia 3.855750641165621e+20\n",
      "Iteration 4, inertia 3.82761038315378e+20\n",
      "Iteration 5, inertia 3.815223005493804e+20\n",
      "Iteration 6, inertia 3.809501832103825e+20\n",
      "Iteration 7, inertia 3.804062267224482e+20\n",
      "Iteration 8, inertia 3.8000059575363613e+20\n",
      "Iteration 9, inertia 3.7966291155128916e+20\n",
      "Iteration 10, inertia 3.794968210290515e+20\n",
      "Iteration 11, inertia 3.794028407491905e+20\n",
      "Iteration 12, inertia 3.79338501054587e+20\n",
      "Iteration 13, inertia 3.793170677827471e+20\n",
      "Converged at iteration 13: strict convergence.\n",
      "Training a K-Means model with 9 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.234909368498881e+20\n",
      "Iteration 1, inertia 2.9240577003900653e+20\n",
      "Iteration 2, inertia 2.8796855064106133e+20\n",
      "Iteration 3, inertia 2.8645312430328267e+20\n",
      "Iteration 4, inertia 2.855072213719102e+20\n",
      "Iteration 5, inertia 2.850520365690884e+20\n",
      "Iteration 6, inertia 2.8472772404860954e+20\n",
      "Iteration 7, inertia 2.844177285666949e+20\n",
      "Iteration 8, inertia 2.8417268858775208e+20\n",
      "Iteration 9, inertia 2.839860463397322e+20\n",
      "Iteration 10, inertia 2.8384701362950514e+20\n",
      "Iteration 11, inertia 2.837532024278264e+20\n",
      "Iteration 12, inertia 2.8359370741016143e+20\n",
      "Iteration 13, inertia 2.8350009525530893e+20\n",
      "Iteration 14, inertia 2.833391149832546e+20\n",
      "Iteration 15, inertia 2.8329140768881597e+20\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.8513622484988756e+20\n",
      "Iteration 1, inertia 2.938445803703999e+20\n",
      "Iteration 2, inertia 2.8666678411641723e+20\n",
      "Iteration 3, inertia 2.8444710568075903e+20\n",
      "Iteration 4, inertia 2.8382753642958304e+20\n",
      "Iteration 5, inertia 2.8353007512744536e+20\n",
      "Iteration 6, inertia 2.832788098050481e+20\n",
      "Iteration 7, inertia 2.831355456056832e+20\n",
      "Iteration 8, inertia 2.831246622340829e+20\n",
      "Iteration 9, inertia 2.830744049286617e+20\n",
      "Iteration 10, inertia 2.8298204886502316e+20\n",
      "Converged at iteration 10: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.2335365484988806e+20\n",
      "Iteration 1, inertia 2.9885046642621415e+20\n",
      "Iteration 2, inertia 2.925933708977663e+20\n",
      "Iteration 3, inertia 2.894912984342604e+20\n",
      "Iteration 4, inertia 2.8822003155762463e+20\n",
      "Iteration 5, inertia 2.8690321405639567e+20\n",
      "Iteration 6, inertia 2.8646635326906594e+20\n",
      "Iteration 7, inertia 2.8624277677251802e+20\n",
      "Iteration 8, inertia 2.860395255134746e+20\n",
      "Iteration 9, inertia 2.8577749876504042e+20\n",
      "Iteration 10, inertia 2.8553181985546866e+20\n",
      "Iteration 11, inertia 2.852064252413298e+20\n",
      "Iteration 12, inertia 2.849429285845045e+20\n",
      "Iteration 13, inertia 2.846565466620496e+20\n",
      "Iteration 14, inertia 2.844086759707513e+20\n",
      "Iteration 15, inertia 2.8420187692062396e+20\n",
      "Iteration 16, inertia 2.840585483560826e+20\n",
      "Iteration 17, inertia 2.839124068780592e+20\n",
      "Iteration 18, inertia 2.8380797143455308e+20\n",
      "Iteration 19, inertia 2.8364847641688814e+20\n",
      "Iteration 20, inertia 2.835548642620356e+20\n",
      "Iteration 21, inertia 2.8339388398998136e+20\n",
      "Iteration 22, inertia 2.8334617669554268e+20\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.0751573884988824e+20\n",
      "Iteration 1, inertia 3.4793827861760836e+20\n",
      "Iteration 2, inertia 3.339856389813923e+20\n",
      "Iteration 3, inertia 3.2384891200226774e+20\n",
      "Iteration 4, inertia 3.1750098251884364e+20\n",
      "Iteration 5, inertia 3.1383959166383437e+20\n",
      "Iteration 6, inertia 3.105875843276697e+20\n",
      "Iteration 7, inertia 3.077958344151533e+20\n",
      "Iteration 8, inertia 3.0560086234764155e+20\n",
      "Iteration 9, inertia 3.038568383405076e+20\n",
      "Iteration 10, inertia 3.017658383326095e+20\n",
      "Iteration 11, inertia 3.002124116237554e+20\n",
      "Iteration 12, inertia 2.9866296807430835e+20\n",
      "Iteration 13, inertia 2.971892323224526e+20\n",
      "Iteration 14, inertia 2.953810008676812e+20\n",
      "Iteration 15, inertia 2.9320140263248134e+20\n",
      "Iteration 16, inertia 2.9144156781012107e+20\n",
      "Iteration 17, inertia 2.8995403648514523e+20\n",
      "Iteration 18, inertia 2.8904007657814586e+20\n",
      "Iteration 19, inertia 2.884094696979374e+20\n",
      "Iteration 20, inertia 2.8769803799415652e+20\n",
      "Iteration 21, inertia 2.870602643278705e+20\n",
      "Iteration 22, inertia 2.8655511781673e+20\n",
      "Iteration 23, inertia 2.86396663781384e+20\n",
      "Iteration 24, inertia 2.86121822352263e+20\n",
      "Iteration 25, inertia 2.857774987650405e+20\n",
      "Iteration 26, inertia 2.8553181985546866e+20\n",
      "Iteration 27, inertia 2.852064252413298e+20\n",
      "Iteration 28, inertia 2.849429285845046e+20\n",
      "Iteration 29, inertia 2.8465654666204958e+20\n",
      "Iteration 30, inertia 2.8440867597075133e+20\n",
      "Iteration 31, inertia 2.8420187692062392e+20\n",
      "Iteration 32, inertia 2.840585483560827e+20\n",
      "Iteration 33, inertia 2.839124068780594e+20\n",
      "Iteration 34, inertia 2.838079714345531e+20\n",
      "Iteration 35, inertia 2.8364847641688814e+20\n",
      "Iteration 36, inertia 2.8355486426203564e+20\n",
      "Iteration 37, inertia 2.8339388398998136e+20\n",
      "Iteration 38, inertia 2.8334617669554264e+20\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.2107711884988763e+20\n",
      "Iteration 1, inertia 3.552918280012561e+20\n",
      "Iteration 2, inertia 3.2936751553881276e+20\n",
      "Iteration 3, inertia 3.200724774506034e+20\n",
      "Iteration 4, inertia 3.1545766244747883e+20\n",
      "Iteration 5, inertia 3.1190530739829906e+20\n",
      "Iteration 6, inertia 3.093979572503357e+20\n",
      "Iteration 7, inertia 3.074689354586041e+20\n",
      "Iteration 8, inertia 3.0510007669161656e+20\n",
      "Iteration 9, inertia 3.032934731276912e+20\n",
      "Iteration 10, inertia 3.015042798242554e+20\n",
      "Iteration 11, inertia 3.001390522193791e+20\n",
      "Iteration 12, inertia 2.9853391791235924e+20\n",
      "Iteration 13, inertia 2.970205156776e+20\n",
      "Iteration 14, inertia 2.9495948377942635e+20\n",
      "Iteration 15, inertia 2.9287884890189247e+20\n",
      "Iteration 16, inertia 2.9118421799476095e+20\n",
      "Iteration 17, inertia 2.8985315331158668e+20\n",
      "Iteration 18, inertia 2.8897358702466807e+20\n",
      "Iteration 19, inertia 2.881098630056836e+20\n",
      "Iteration 20, inertia 2.874947790202766e+20\n",
      "Iteration 21, inertia 2.869971478628272e+20\n",
      "Iteration 22, inertia 2.8653012774669864e+20\n",
      "Iteration 23, inertia 2.8639666378138403e+20\n",
      "Iteration 24, inertia 2.861218223522629e+20\n",
      "Iteration 25, inertia 2.8577749876504045e+20\n",
      "Iteration 26, inertia 2.8553181985546866e+20\n",
      "Iteration 27, inertia 2.8520642524132976e+20\n",
      "Iteration 28, inertia 2.8494292858450454e+20\n",
      "Iteration 29, inertia 2.8465654666204964e+20\n",
      "Iteration 30, inertia 2.8440867597075127e+20\n",
      "Iteration 31, inertia 2.8420187692062392e+20\n",
      "Iteration 32, inertia 2.8405854835608263e+20\n",
      "Iteration 33, inertia 2.839124068780594e+20\n",
      "Iteration 34, inertia 2.8380797143455315e+20\n",
      "Iteration 35, inertia 2.8364847641688814e+20\n",
      "Iteration 36, inertia 2.8355486426203554e+20\n",
      "Iteration 37, inertia 2.8339388398998133e+20\n",
      "Iteration 38, inertia 2.8334617669554264e+20\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.2150088384988735e+20\n",
      "Iteration 1, inertia 2.968237512337529e+20\n",
      "Iteration 2, inertia 2.932089996400295e+20\n",
      "Iteration 3, inertia 2.908651026744834e+20\n",
      "Iteration 4, inertia 2.8835836926730397e+20\n",
      "Iteration 5, inertia 2.87170455146312e+20\n",
      "Iteration 6, inertia 2.8672435452723924e+20\n",
      "Iteration 7, inertia 2.862880401069678e+20\n",
      "Iteration 8, inertia 2.860395255134746e+20\n",
      "Iteration 9, inertia 2.8577749876504045e+20\n",
      "Iteration 10, inertia 2.8553181985546866e+20\n",
      "Iteration 11, inertia 2.852064252413298e+20\n",
      "Iteration 12, inertia 2.849429285845046e+20\n",
      "Iteration 13, inertia 2.8465654666204958e+20\n",
      "Iteration 14, inertia 2.8440867597075127e+20\n",
      "Iteration 15, inertia 2.8420187692062392e+20\n",
      "Iteration 16, inertia 2.8405854835608263e+20\n",
      "Iteration 17, inertia 2.8391240687805943e+20\n",
      "Iteration 18, inertia 2.838079714345531e+20\n",
      "Iteration 19, inertia 2.8364847641688814e+20\n",
      "Iteration 20, inertia 2.8355486426203567e+20\n",
      "Iteration 21, inertia 2.833938839899814e+20\n",
      "Iteration 22, inertia 2.833461766955427e+20\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.828853988498884e+20\n",
      "Iteration 1, inertia 3.341945835379484e+20\n",
      "Iteration 2, inertia 3.2458653196898147e+20\n",
      "Iteration 3, inertia 3.182414658103348e+20\n",
      "Iteration 4, inertia 3.142473041525034e+20\n",
      "Iteration 5, inertia 3.115076894271497e+20\n",
      "Iteration 6, inertia 3.0916726676007826e+20\n",
      "Iteration 7, inertia 3.073051256065881e+20\n",
      "Iteration 8, inertia 3.0505930023212764e+20\n",
      "Iteration 9, inertia 3.022581096166508e+20\n",
      "Iteration 10, inertia 2.9925491166863353e+20\n",
      "Iteration 11, inertia 2.9638739341292326e+20\n",
      "Iteration 12, inertia 2.9448138530336313e+20\n",
      "Iteration 13, inertia 2.9248925178030696e+20\n",
      "Iteration 14, inertia 2.9109731913130924e+20\n",
      "Iteration 15, inertia 2.9020551035397223e+20\n",
      "Iteration 16, inertia 2.896042896366046e+20\n",
      "Iteration 17, inertia 2.8897781755086556e+20\n",
      "Iteration 18, inertia 2.8825763428607486e+20\n",
      "Iteration 19, inertia 2.877582772285226e+20\n",
      "Iteration 20, inertia 2.8730675600416342e+20\n",
      "Iteration 21, inertia 2.8691117600164833e+20\n",
      "Iteration 22, inertia 2.8665914240549913e+20\n",
      "Iteration 23, inertia 2.8629454103733233e+20\n",
      "Iteration 24, inertia 2.8605692279217848e+20\n",
      "Iteration 25, inertia 2.8571483903134188e+20\n",
      "Iteration 26, inertia 2.855318198554687e+20\n",
      "Iteration 27, inertia 2.8520642524132976e+20\n",
      "Iteration 28, inertia 2.849429285845045e+20\n",
      "Iteration 29, inertia 2.8465654666204958e+20\n",
      "Iteration 30, inertia 2.844086759707513e+20\n",
      "Iteration 31, inertia 2.8420187692062392e+20\n",
      "Iteration 32, inertia 2.8405854835608263e+20\n",
      "Iteration 33, inertia 2.839124068780594e+20\n",
      "Iteration 34, inertia 2.838079714345531e+20\n",
      "Iteration 35, inertia 2.836484764168882e+20\n",
      "Iteration 36, inertia 2.835548642620356e+20\n",
      "Iteration 37, inertia 2.833938839899814e+20\n",
      "Iteration 38, inertia 2.833461766955427e+20\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.6115844684988626e+20\n",
      "Iteration 1, inertia 3.2334014352403346e+20\n",
      "Iteration 2, inertia 3.144118092198571e+20\n",
      "Iteration 3, inertia 3.096054067025578e+20\n",
      "Iteration 4, inertia 3.064875455552543e+20\n",
      "Iteration 5, inertia 3.036062299000049e+20\n",
      "Iteration 6, inertia 3.021396731338816e+20\n",
      "Iteration 7, inertia 3.003578290740085e+20\n",
      "Iteration 8, inertia 2.9915676499117356e+20\n",
      "Iteration 9, inertia 2.979886320512252e+20\n",
      "Iteration 10, inertia 2.960768962250871e+20\n",
      "Iteration 11, inertia 2.94185547856845e+20\n",
      "Iteration 12, inertia 2.9186556616951195e+20\n",
      "Iteration 13, inertia 2.9015276476079433e+20\n",
      "Iteration 14, inertia 2.8912910848952215e+20\n",
      "Iteration 15, inertia 2.8840946969793754e+20\n",
      "Iteration 16, inertia 2.8769803799415652e+20\n",
      "Iteration 17, inertia 2.8706026432787048e+20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, inertia 2.8655511781673003e+20\n",
      "Iteration 19, inertia 2.8639666378138403e+20\n",
      "Iteration 20, inertia 2.86121822352263e+20\n",
      "Iteration 21, inertia 2.857774987650405e+20\n",
      "Iteration 22, inertia 2.8553181985546863e+20\n",
      "Iteration 23, inertia 2.852064252413298e+20\n",
      "Iteration 24, inertia 2.8494292858450457e+20\n",
      "Iteration 25, inertia 2.846565466620496e+20\n",
      "Iteration 26, inertia 2.844086759707513e+20\n",
      "Iteration 27, inertia 2.8420187692062392e+20\n",
      "Iteration 28, inertia 2.8405854835608243e+20\n",
      "Iteration 29, inertia 2.8391240687805936e+20\n",
      "Iteration 30, inertia 2.8380797143455315e+20\n",
      "Iteration 31, inertia 2.8364847641688814e+20\n",
      "Iteration 32, inertia 2.835548642620357e+20\n",
      "Iteration 33, inertia 2.8339388398998146e+20\n",
      "Iteration 34, inertia 2.8334617669554268e+20\n",
      "Converged at iteration 34: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.812628688498881e+20\n",
      "Iteration 1, inertia 3.320873429869296e+20\n",
      "Iteration 2, inertia 3.226811897813899e+20\n",
      "Iteration 3, inertia 3.164120711893594e+20\n",
      "Iteration 4, inertia 3.1321579146618346e+20\n",
      "Iteration 5, inertia 3.108958977735685e+20\n",
      "Iteration 6, inertia 3.0878278748509064e+20\n",
      "Iteration 7, inertia 3.0707167363206624e+20\n",
      "Iteration 8, inertia 3.048532786339283e+20\n",
      "Iteration 9, inertia 3.0241565688994116e+20\n",
      "Iteration 10, inertia 3.0055277967020936e+20\n",
      "Iteration 11, inertia 2.979843495828147e+20\n",
      "Iteration 12, inertia 2.953711565152738e+20\n",
      "Iteration 13, inertia 2.9322832951010378e+20\n",
      "Iteration 14, inertia 2.9193557500104006e+20\n",
      "Iteration 15, inertia 2.9094689458774295e+20\n",
      "Iteration 16, inertia 2.900728453540175e+20\n",
      "Iteration 17, inertia 2.8926942463693324e+20\n",
      "Iteration 18, inertia 2.8864438213260627e+20\n",
      "Iteration 19, inertia 2.8805745396337636e+20\n",
      "Iteration 20, inertia 2.8751829692598918e+20\n",
      "Iteration 21, inertia 2.8715350726610315e+20\n",
      "Iteration 22, inertia 2.867197031348108e+20\n",
      "Iteration 23, inertia 2.8647647586517035e+20\n",
      "Iteration 24, inertia 2.8613349125949574e+20\n",
      "Iteration 25, inertia 2.857774987650405e+20\n",
      "Iteration 26, inertia 2.8553181985546866e+20\n",
      "Iteration 27, inertia 2.8520642524132976e+20\n",
      "Iteration 28, inertia 2.8494292858450457e+20\n",
      "Iteration 29, inertia 2.846565466620496e+20\n",
      "Iteration 30, inertia 2.8440867597075133e+20\n",
      "Iteration 31, inertia 2.8420187692062392e+20\n",
      "Iteration 32, inertia 2.8405854835608243e+20\n",
      "Iteration 33, inertia 2.839124068780594e+20\n",
      "Iteration 34, inertia 2.838079714345531e+20\n",
      "Iteration 35, inertia 2.8364847641688814e+20\n",
      "Iteration 36, inertia 2.8355486426203554e+20\n",
      "Iteration 37, inertia 2.8339388398998146e+20\n",
      "Iteration 38, inertia 2.8334617669554268e+20\n",
      "Converged at iteration 38: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 4.030057188498883e+20\n",
      "Iteration 1, inertia 2.986037123734009e+20\n",
      "Iteration 2, inertia 2.8935172771001152e+20\n",
      "Iteration 3, inertia 2.8698157020788592e+20\n",
      "Iteration 4, inertia 2.8592507826599104e+20\n",
      "Iteration 5, inertia 2.850032164915942e+20\n",
      "Iteration 6, inertia 2.84525404592776e+20\n",
      "Iteration 7, inertia 2.8401555979391068e+20\n",
      "Iteration 8, inertia 2.8367046400990233e+20\n",
      "Iteration 9, inertia 2.8327057343572237e+20\n",
      "Iteration 10, inertia 2.8296710227578248e+20\n",
      "Iteration 11, inertia 2.8277245030564695e+20\n",
      "Iteration 12, inertia 2.8272425307292123e+20\n",
      "Iteration 13, inertia 2.8263077976439692e+20\n",
      "Iteration 14, inertia 2.825941224368151e+20\n",
      "Converged at iteration 14: strict convergence.\n",
      "Training a K-Means model with 10 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.811380368498881e+20\n",
      "Iteration 1, inertia 2.5173788543720482e+20\n",
      "Iteration 2, inertia 2.4569161110332378e+20\n",
      "Iteration 3, inertia 2.426101861303949e+20\n",
      "Iteration 4, inertia 2.4038465329669243e+20\n",
      "Iteration 5, inertia 2.393457593573739e+20\n",
      "Iteration 6, inertia 2.3883755114416253e+20\n",
      "Iteration 7, inertia 2.385418347123471e+20\n",
      "Iteration 8, inertia 2.3837420461996936e+20\n",
      "Iteration 9, inertia 2.3814349407565093e+20\n",
      "Iteration 10, inertia 2.380563449809273e+20\n",
      "Iteration 11, inertia 2.378509534134066e+20\n",
      "Iteration 12, inertia 2.3777705451801946e+20\n",
      "Iteration 13, inertia 2.3769423013646074e+20\n",
      "Iteration 14, inertia 2.376785164864538e+20\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.803776488498876e+20\n",
      "Iteration 1, inertia 2.39116005802418e+20\n",
      "Iteration 2, inertia 2.325548233820025e+20\n",
      "Iteration 3, inertia 2.3088163696450257e+20\n",
      "Iteration 4, inertia 2.29403610849734e+20\n",
      "Iteration 5, inertia 2.2826862710674014e+20\n",
      "Iteration 6, inertia 2.2731091617953428e+20\n",
      "Iteration 7, inertia 2.266630495932399e+20\n",
      "Iteration 8, inertia 2.2633246997424275e+20\n",
      "Iteration 9, inertia 2.2598677797611667e+20\n",
      "Iteration 10, inertia 2.2584272826074402e+20\n",
      "Iteration 11, inertia 2.2574626746935614e+20\n",
      "Iteration 12, inertia 2.2571418668135727e+20\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.6889990884988744e+20\n",
      "Iteration 1, inertia 2.4540029012661813e+20\n",
      "Iteration 2, inertia 2.4058896467630857e+20\n",
      "Iteration 3, inertia 2.386323558421755e+20\n",
      "Iteration 4, inertia 2.3746999258362505e+20\n",
      "Iteration 5, inertia 2.365313916926755e+20\n",
      "Iteration 6, inertia 2.358041585950729e+20\n",
      "Iteration 7, inertia 2.3538850652082405e+20\n",
      "Iteration 8, inertia 2.3502300460387258e+20\n",
      "Iteration 9, inertia 2.3484273162537684e+20\n",
      "Iteration 10, inertia 2.3468474892475066e+20\n",
      "Iteration 11, inertia 2.345211920114623e+20\n",
      "Iteration 12, inertia 2.343199644332553e+20\n",
      "Iteration 13, inertia 2.338389978303647e+20\n",
      "Iteration 14, inertia 2.3349291989645833e+20\n",
      "Iteration 15, inertia 2.3289060901759887e+20\n",
      "Iteration 16, inertia 2.323828179351531e+20\n",
      "Iteration 17, inertia 2.320926591409935e+20\n",
      "Iteration 18, inertia 2.3177614816465197e+20\n",
      "Iteration 19, inertia 2.3150658641190263e+20\n",
      "Iteration 20, inertia 2.311308099636607e+20\n",
      "Iteration 21, inertia 2.3072737197346066e+20\n",
      "Iteration 22, inertia 2.3053221685507863e+20\n",
      "Iteration 23, inertia 2.3025435636237474e+20\n",
      "Iteration 24, inertia 2.299848072667796e+20\n",
      "Iteration 25, inertia 2.2986046877841496e+20\n",
      "Iteration 26, inertia 2.296864401398556e+20\n",
      "Iteration 27, inertia 2.2951708550765655e+20\n",
      "Iteration 28, inertia 2.2947585823459508e+20\n",
      "Iteration 29, inertia 2.2942898419132862e+20\n",
      "Iteration 30, inertia 2.2940403953745976e+20\n",
      "Converged at iteration 30: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.9734140884988625e+20\n",
      "Iteration 1, inertia 2.3755832883443293e+20\n",
      "Iteration 2, inertia 2.3361889225865288e+20\n",
      "Iteration 3, inertia 2.3097372944936233e+20\n",
      "Iteration 4, inertia 2.289288666994185e+20\n",
      "Iteration 5, inertia 2.2708034583262128e+20\n",
      "Iteration 6, inertia 2.2603127121345475e+20\n",
      "Iteration 7, inertia 2.255612052401461e+20\n",
      "Iteration 8, inertia 2.2505741614275658e+20\n",
      "Iteration 9, inertia 2.2477711261708347e+20\n",
      "Iteration 10, inertia 2.2468206446978377e+20\n",
      "Iteration 11, inertia 2.2450235818388967e+20\n",
      "Iteration 12, inertia 2.2444579720425547e+20\n",
      "Iteration 13, inertia 2.244250341544536e+20\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.0397223384988805e+20\n",
      "Iteration 1, inertia 2.4781062643998776e+20\n",
      "Iteration 2, inertia 2.3845762072697894e+20\n",
      "Iteration 3, inertia 2.3492958276224998e+20\n",
      "Iteration 4, inertia 2.3317230324145265e+20\n",
      "Iteration 5, inertia 2.3164744934400007e+20\n",
      "Iteration 6, inertia 2.3071475361668222e+20\n",
      "Iteration 7, inertia 2.299964453598235e+20\n",
      "Iteration 8, inertia 2.2925243400494973e+20\n",
      "Iteration 9, inertia 2.280937608298136e+20\n",
      "Iteration 10, inertia 2.2672453370795255e+20\n",
      "Iteration 11, inertia 2.256951245766862e+20\n",
      "Iteration 12, inertia 2.2524688865314623e+20\n",
      "Iteration 13, inertia 2.2496356933315147e+20\n",
      "Iteration 14, inertia 2.2480900526955168e+20\n",
      "Iteration 15, inertia 2.246331428288393e+20\n",
      "Iteration 16, inertia 2.2447480060764992e+20\n",
      "Iteration 17, inertia 2.2441771564276577e+20\n",
      "Iteration 18, inertia 2.244020224411433e+20\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.028343588498853e+20\n",
      "Iteration 1, inertia 2.7404771433894307e+20\n",
      "Iteration 2, inertia 2.6339582563252935e+20\n",
      "Iteration 3, inertia 2.5643184058017346e+20\n",
      "Iteration 4, inertia 2.51833881803678e+20\n",
      "Iteration 5, inertia 2.4750344014774993e+20\n",
      "Iteration 6, inertia 2.4379689324774575e+20\n",
      "Iteration 7, inertia 2.4202559354833607e+20\n",
      "Iteration 8, inertia 2.4085354945545906e+20\n",
      "Iteration 9, inertia 2.3994106676490174e+20\n",
      "Iteration 10, inertia 2.394675996213762e+20\n",
      "Iteration 11, inertia 2.391104166920704e+20\n",
      "Iteration 12, inertia 2.3894489475054764e+20\n",
      "Iteration 13, inertia 2.385813306364529e+20\n",
      "Iteration 14, inertia 2.3836285273448376e+20\n",
      "Iteration 15, inertia 2.3829139000991934e+20\n",
      "Iteration 16, inertia 2.3813449386558253e+20\n",
      "Iteration 17, inertia 2.3798672430239623e+20\n",
      "Iteration 18, inertia 2.3793586165843362e+20\n",
      "Iteration 19, inertia 2.3782290410698025e+20\n",
      "Iteration 20, inertia 2.3780710415704785e+20\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.9266254984988774e+20\n",
      "Iteration 1, inertia 2.3571850711819092e+20\n",
      "Iteration 2, inertia 2.2810874095872963e+20\n",
      "Iteration 3, inertia 2.2580076453721824e+20\n",
      "Iteration 4, inertia 2.2527118779171176e+20\n",
      "Iteration 5, inertia 2.250069532508927e+20\n",
      "Iteration 6, inertia 2.2494736325566002e+20\n",
      "Iteration 7, inertia 2.2490146045194043e+20\n",
      "Iteration 8, inertia 2.2488227395014227e+20\n",
      "Iteration 9, inertia 2.248747902957895e+20\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.6879900884988812e+20\n",
      "Iteration 1, inertia 2.427430380293122e+20\n",
      "Iteration 2, inertia 2.3795375884737597e+20\n",
      "Iteration 3, inertia 2.3518280658102033e+20\n",
      "Iteration 4, inertia 2.3364731254397115e+20\n",
      "Iteration 5, inertia 2.3240684562267937e+20\n",
      "Iteration 6, inertia 2.314184127700761e+20\n",
      "Iteration 7, inertia 2.3070559165295998e+20\n",
      "Iteration 8, inertia 2.3032179906420733e+20\n",
      "Iteration 9, inertia 2.300369840716691e+20\n",
      "Iteration 10, inertia 2.2978144960805936e+20\n",
      "Iteration 11, inertia 2.294325480770669e+20\n",
      "Iteration 12, inertia 2.2887231189761933e+20\n",
      "Iteration 13, inertia 2.282858524416527e+20\n",
      "Iteration 14, inertia 2.275491829107852e+20\n",
      "Iteration 15, inertia 2.2704384279718556e+20\n",
      "Iteration 16, inertia 2.265853143256133e+20\n",
      "Iteration 17, inertia 2.2611489749092532e+20\n",
      "Iteration 18, inertia 2.2547424051946088e+20\n",
      "Iteration 19, inertia 2.251965975482739e+20\n",
      "Iteration 20, inertia 2.250975317163169e+20\n",
      "Iteration 21, inertia 2.2495315816203272e+20\n",
      "Iteration 22, inertia 2.2491188813578545e+20\n",
      "Iteration 23, inertia 2.248999803969538e+20\n",
      "Converged at iteration 23: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.6616624484988813e+20\n",
      "Iteration 1, inertia 2.3613461738850478e+20\n",
      "Iteration 2, inertia 2.3187250669735264e+20\n",
      "Iteration 3, inertia 2.2974796261924697e+20\n",
      "Iteration 4, inertia 2.2788960857575637e+20\n",
      "Iteration 5, inertia 2.2621815756603392e+20\n",
      "Iteration 6, inertia 2.2536809922659765e+20\n",
      "Iteration 7, inertia 2.2496316727489708e+20\n",
      "Iteration 8, inertia 2.247960964008628e+20\n",
      "Iteration 9, inertia 2.246104072057096e+20\n",
      "Iteration 10, inertia 2.2456264333933694e+20\n",
      "Iteration 11, inertia 2.2448468868196146e+20\n",
      "Iteration 12, inertia 2.2443394185199976e+20\n",
      "Iteration 13, inertia 2.244250341544536e+20\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.9116122784988712e+20\n",
      "Iteration 1, inertia 2.5085733012240767e+20\n",
      "Iteration 2, inertia 2.4211384951908264e+20\n",
      "Iteration 3, inertia 2.3772775195857496e+20\n",
      "Iteration 4, inertia 2.3532396985770243e+20\n",
      "Iteration 5, inertia 2.340086816586637e+20\n",
      "Iteration 6, inertia 2.331256026501997e+20\n",
      "Iteration 7, inertia 2.3273315637884607e+20\n",
      "Iteration 8, inertia 2.3234176993194433e+20\n",
      "Iteration 9, inertia 2.3193129088967918e+20\n",
      "Iteration 10, inertia 2.3147420788969855e+20\n",
      "Iteration 11, inertia 2.310464385218864e+20\n",
      "Iteration 12, inertia 2.305852557289368e+20\n",
      "Iteration 13, inertia 2.3012565131440875e+20\n",
      "Iteration 14, inertia 2.296154793233486e+20\n",
      "Iteration 15, inertia 2.291663445250439e+20\n",
      "Iteration 16, inertia 2.287936571727463e+20\n",
      "Iteration 17, inertia 2.2861174615686676e+20\n",
      "Iteration 18, inertia 2.2852138749002187e+20\n",
      "Iteration 19, inertia 2.2847329848146166e+20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, inertia 2.283700478159395e+20\n",
      "Iteration 21, inertia 2.2797660012365444e+20\n",
      "Iteration 22, inertia 2.2764388115406022e+20\n",
      "Iteration 23, inertia 2.2733779851858638e+20\n",
      "Iteration 24, inertia 2.2691701181560396e+20\n",
      "Iteration 25, inertia 2.2643360680061233e+20\n",
      "Iteration 26, inertia 2.2616487830386085e+20\n",
      "Iteration 27, inertia 2.2605924752950713e+20\n",
      "Iteration 28, inertia 2.259961895260441e+20\n",
      "Iteration 29, inertia 2.2599314198030424e+20\n",
      "Converged at iteration 29: strict convergence.\n",
      "Training a K-Means model with 11 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.475180368498881e+20\n",
      "Iteration 1, inertia 2.3222809222714157e+20\n",
      "Iteration 2, inertia 2.2810886079290324e+20\n",
      "Iteration 3, inertia 2.255306018709214e+20\n",
      "Iteration 4, inertia 2.2312569767284834e+20\n",
      "Iteration 5, inertia 2.213217156228784e+20\n",
      "Iteration 6, inertia 2.200442403775023e+20\n",
      "Iteration 7, inertia 2.1827339292153636e+20\n",
      "Iteration 8, inertia 2.1653082388118025e+20\n",
      "Iteration 9, inertia 2.1521106557999858e+20\n",
      "Iteration 10, inertia 2.1329800994056605e+20\n",
      "Iteration 11, inertia 2.116319985274369e+20\n",
      "Iteration 12, inertia 2.098035411159219e+20\n",
      "Iteration 13, inertia 2.076590883773421e+20\n",
      "Iteration 14, inertia 2.0526492826896528e+20\n",
      "Iteration 15, inertia 2.0324132490811094e+20\n",
      "Iteration 16, inertia 2.0163689490206278e+20\n",
      "Iteration 17, inertia 2.0042318770179693e+20\n",
      "Iteration 18, inertia 1.9969472919680418e+20\n",
      "Iteration 19, inertia 1.9902440889841956e+20\n",
      "Iteration 20, inertia 1.9815214655335255e+20\n",
      "Iteration 21, inertia 1.9729837139077608e+20\n",
      "Iteration 22, inertia 1.9628963959878245e+20\n",
      "Iteration 23, inertia 1.957068879600033e+20\n",
      "Iteration 24, inertia 1.9505855834660563e+20\n",
      "Iteration 25, inertia 1.9432228357565022e+20\n",
      "Iteration 26, inertia 1.9361203468433575e+20\n",
      "Iteration 27, inertia 1.9305036699863656e+20\n",
      "Iteration 28, inertia 1.9267825282885396e+20\n",
      "Iteration 29, inertia 1.923714528772485e+20\n",
      "Iteration 30, inertia 1.919997972959031e+20\n",
      "Iteration 31, inertia 1.915681633665274e+20\n",
      "Iteration 32, inertia 1.9085944564038274e+20\n",
      "Iteration 33, inertia 1.9029232448846735e+20\n",
      "Iteration 34, inertia 1.8988114512419347e+20\n",
      "Iteration 35, inertia 1.8958606490111495e+20\n",
      "Iteration 36, inertia 1.891279478281429e+20\n",
      "Iteration 37, inertia 1.8883125865011075e+20\n",
      "Iteration 38, inertia 1.8853127572275836e+20\n",
      "Iteration 39, inertia 1.883066308480642e+20\n",
      "Iteration 40, inertia 1.8816551000804308e+20\n",
      "Iteration 41, inertia 1.878422384667679e+20\n",
      "Iteration 42, inertia 1.877307467335673e+20\n",
      "Converged at iteration 42: center shift 9204537051336.164 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.3682594584988808e+20\n",
      "Iteration 1, inertia 2.0898440765609496e+20\n",
      "Iteration 2, inertia 2.028182784971246e+20\n",
      "Iteration 3, inertia 1.9748549809743603e+20\n",
      "Iteration 4, inertia 1.929410438683775e+20\n",
      "Iteration 5, inertia 1.8956566593740397e+20\n",
      "Iteration 6, inertia 1.8829607992369745e+20\n",
      "Iteration 7, inertia 1.8765512782997416e+20\n",
      "Iteration 8, inertia 1.873091410048676e+20\n",
      "Iteration 9, inertia 1.8694780187844238e+20\n",
      "Iteration 10, inertia 1.8660855871176212e+20\n",
      "Iteration 11, inertia 1.864996043314856e+20\n",
      "Iteration 12, inertia 1.863025194059903e+20\n",
      "Iteration 13, inertia 1.8626811576510064e+20\n",
      "Iteration 14, inertia 1.8621562967566228e+20\n",
      "Iteration 15, inertia 1.861880891541767e+20\n",
      "Iteration 16, inertia 1.8614963793883513e+20\n",
      "Iteration 17, inertia 1.861448824258579e+20\n",
      "Converged at iteration 17: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.298413658498884e+20\n",
      "Iteration 1, inertia 1.9604899702617686e+20\n",
      "Iteration 2, inertia 1.9059560791980707e+20\n",
      "Iteration 3, inertia 1.8925114832171336e+20\n",
      "Iteration 4, inertia 1.8860223124828065e+20\n",
      "Iteration 5, inertia 1.8838860901657284e+20\n",
      "Iteration 6, inertia 1.881935199864341e+20\n",
      "Iteration 7, inertia 1.8810127078990325e+20\n",
      "Iteration 8, inertia 1.8778405633518112e+20\n",
      "Iteration 9, inertia 1.8736137168548368e+20\n",
      "Iteration 10, inertia 1.872141419255982e+20\n",
      "Iteration 11, inertia 1.868892914212399e+20\n",
      "Iteration 12, inertia 1.8648517402461903e+20\n",
      "Iteration 13, inertia 1.863528881411752e+20\n",
      "Iteration 14, inertia 1.862832453914736e+20\n",
      "Iteration 15, inertia 1.8622270572146334e+20\n",
      "Iteration 16, inertia 1.8611368408212714e+20\n",
      "Iteration 17, inertia 1.8606247203363176e+20\n",
      "Iteration 18, inertia 1.8599318741096684e+20\n",
      "Iteration 19, inertia 1.8581325001343428e+20\n",
      "Iteration 20, inertia 1.8565785859822358e+20\n",
      "Iteration 21, inertia 1.8560058088772017e+20\n",
      "Iteration 22, inertia 1.8558702619508277e+20\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.164428058498882e+20\n",
      "Iteration 1, inertia 1.8917116139312795e+20\n",
      "Iteration 2, inertia 1.8813310497572644e+20\n",
      "Iteration 3, inertia 1.8727838225304676e+20\n",
      "Iteration 4, inertia 1.8663193439368357e+20\n",
      "Iteration 5, inertia 1.8608970482237263e+20\n",
      "Iteration 6, inertia 1.859312855741293e+20\n",
      "Iteration 7, inertia 1.8574744966590634e+20\n",
      "Iteration 8, inertia 1.8558479656240297e+20\n",
      "Iteration 9, inertia 1.853925069947266e+20\n",
      "Iteration 10, inertia 1.8535125151557337e+20\n",
      "Iteration 11, inertia 1.853376845399362e+20\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.180665348498879e+20\n",
      "Iteration 1, inertia 1.9489144033540375e+20\n",
      "Iteration 2, inertia 1.9114779278958464e+20\n",
      "Iteration 3, inertia 1.897377907673998e+20\n",
      "Iteration 4, inertia 1.8884781236660073e+20\n",
      "Iteration 5, inertia 1.8822533776828567e+20\n",
      "Iteration 6, inertia 1.8780570833348926e+20\n",
      "Iteration 7, inertia 1.875745084176617e+20\n",
      "Iteration 8, inertia 1.8714619680326812e+20\n",
      "Iteration 9, inertia 1.8683055088105865e+20\n",
      "Iteration 10, inertia 1.8668319518843306e+20\n",
      "Iteration 11, inertia 1.8653504770661397e+20\n",
      "Iteration 12, inertia 1.863950677086018e+20\n",
      "Iteration 13, inertia 1.862148233961261e+20\n",
      "Iteration 14, inertia 1.861847972109913e+20\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.4507910884988807e+20\n",
      "Iteration 1, inertia 2.1633532671052995e+20\n",
      "Iteration 2, inertia 2.118925591932639e+20\n",
      "Iteration 3, inertia 2.0921811919200738e+20\n",
      "Iteration 4, inertia 2.0662419130177362e+20\n",
      "Iteration 5, inertia 2.0391763139205933e+20\n",
      "Iteration 6, inertia 2.018479758803156e+20\n",
      "Iteration 7, inertia 1.999649456900114e+20\n",
      "Iteration 8, inertia 1.9869188611465527e+20\n",
      "Iteration 9, inertia 1.9785167677290063e+20\n",
      "Iteration 10, inertia 1.9703010631458075e+20\n",
      "Iteration 11, inertia 1.963041106142225e+20\n",
      "Iteration 12, inertia 1.9545824465502313e+20\n",
      "Iteration 13, inertia 1.9466650984238075e+20\n",
      "Iteration 14, inertia 1.9386544399175166e+20\n",
      "Iteration 15, inertia 1.9319522323708274e+20\n",
      "Iteration 16, inertia 1.927538224574248e+20\n",
      "Iteration 17, inertia 1.924547716089936e+20\n",
      "Iteration 18, inertia 1.9199607045986406e+20\n",
      "Iteration 19, inertia 1.9121662006597947e+20\n",
      "Iteration 20, inertia 1.9058900025064574e+20\n",
      "Iteration 21, inertia 1.9009594019431236e+20\n",
      "Iteration 22, inertia 1.8964665895956722e+20\n",
      "Iteration 23, inertia 1.8915665341821053e+20\n",
      "Iteration 24, inertia 1.8889511596198226e+20\n",
      "Iteration 25, inertia 1.886543800948212e+20\n",
      "Iteration 26, inertia 1.8840122912240098e+20\n",
      "Iteration 27, inertia 1.8817252693784845e+20\n",
      "Iteration 28, inertia 1.8784381501660853e+20\n",
      "Iteration 29, inertia 1.877307467335673e+20\n",
      "Converged at iteration 29: center shift 9204537051338.426 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.8709758084988836e+20\n",
      "Iteration 1, inertia 2.1978292368753548e+20\n",
      "Iteration 2, inertia 2.0649230916547792e+20\n",
      "Iteration 3, inertia 1.983063970237701e+20\n",
      "Iteration 4, inertia 1.940058338674999e+20\n",
      "Iteration 5, inertia 1.917051673303497e+20\n",
      "Iteration 6, inertia 1.9051627109531917e+20\n",
      "Iteration 7, inertia 1.8980077534324254e+20\n",
      "Iteration 8, inertia 1.8924672329557937e+20\n",
      "Iteration 9, inertia 1.886894333099811e+20\n",
      "Iteration 10, inertia 1.8784622551623696e+20\n",
      "Iteration 11, inertia 1.8729817136264348e+20\n",
      "Iteration 12, inertia 1.8698471161818025e+20\n",
      "Iteration 13, inertia 1.865881232146492e+20\n",
      "Iteration 14, inertia 1.8645305068000045e+20\n",
      "Iteration 15, inertia 1.86276214655813e+20\n",
      "Iteration 16, inertia 1.8619210374865093e+20\n",
      "Iteration 17, inertia 1.861457133639537e+20\n",
      "Iteration 18, inertia 1.8609467918374778e+20\n",
      "Iteration 19, inertia 1.8607797689459288e+20\n",
      "Iteration 20, inertia 1.8607347319603737e+20\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.213688038498879e+20\n",
      "Iteration 1, inertia 1.9527676148629953e+20\n",
      "Iteration 2, inertia 1.9184701726068792e+20\n",
      "Iteration 3, inertia 1.9022273053391028e+20\n",
      "Iteration 4, inertia 1.892540074017476e+20\n",
      "Iteration 5, inertia 1.8824747282814878e+20\n",
      "Iteration 6, inertia 1.874008231070195e+20\n",
      "Iteration 7, inertia 1.869068810016909e+20\n",
      "Iteration 8, inertia 1.8668607444071694e+20\n",
      "Iteration 9, inertia 1.8660151078553036e+20\n",
      "Iteration 10, inertia 1.8641243825811925e+20\n",
      "Iteration 11, inertia 1.8625923011017323e+20\n",
      "Iteration 12, inertia 1.8624515769436212e+20\n",
      "Iteration 13, inertia 1.862350889038214e+20\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, inertia 2.5614742484988743e+20\n",
      "Iteration 1, inertia 2.0226251315114633e+20\n",
      "Iteration 2, inertia 1.9633352458543707e+20\n",
      "Iteration 3, inertia 1.9384453438883693e+20\n",
      "Iteration 4, inertia 1.9181243305802288e+20\n",
      "Iteration 5, inertia 1.9082338600153635e+20\n",
      "Iteration 6, inertia 1.898029361793986e+20\n",
      "Iteration 7, inertia 1.8885463497504493e+20\n",
      "Iteration 8, inertia 1.88251041258918e+20\n",
      "Iteration 9, inertia 1.8777428281713666e+20\n",
      "Iteration 10, inertia 1.8726356547622712e+20\n",
      "Iteration 11, inertia 1.868288230984382e+20\n",
      "Iteration 12, inertia 1.8650902087439575e+20\n",
      "Iteration 13, inertia 1.8635599196044948e+20\n",
      "Iteration 14, inertia 1.8620074396173605e+20\n",
      "Iteration 15, inertia 1.8613622457556206e+20\n",
      "Iteration 16, inertia 1.8612109882141127e+20\n",
      "Converged at iteration 16: center shift 13286946511162.023 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.7442366484988825e+20\n",
      "Iteration 1, inertia 2.3118239038064335e+20\n",
      "Iteration 2, inertia 2.2137355163648007e+20\n",
      "Iteration 3, inertia 2.1389907726805067e+20\n",
      "Iteration 4, inertia 2.065700151867766e+20\n",
      "Iteration 5, inertia 2.0063092412285904e+20\n",
      "Iteration 6, inertia 1.9649321944961112e+20\n",
      "Iteration 7, inertia 1.9403480187978587e+20\n",
      "Iteration 8, inertia 1.9268479610711002e+20\n",
      "Iteration 9, inertia 1.9167280126324097e+20\n",
      "Iteration 10, inertia 1.9064831800532035e+20\n",
      "Iteration 11, inertia 1.8942413003198503e+20\n",
      "Iteration 12, inertia 1.8840252703499675e+20\n",
      "Iteration 13, inertia 1.8797051982709824e+20\n",
      "Iteration 14, inertia 1.8747780387822354e+20\n",
      "Iteration 15, inertia 1.869375865819623e+20\n",
      "Iteration 16, inertia 1.8669869376654823e+20\n",
      "Iteration 17, inertia 1.865401208782437e+20\n",
      "Iteration 18, inertia 1.8628262869358943e+20\n",
      "Iteration 19, inertia 1.861765735622713e+20\n",
      "Iteration 20, inertia 1.8610468069383025e+20\n",
      "Iteration 21, inertia 1.8601095868284738e+20\n",
      "Iteration 22, inertia 1.8593215445363717e+20\n",
      "Iteration 23, inertia 1.8586268849531378e+20\n",
      "Iteration 24, inertia 1.858367190227278e+20\n",
      "Converged at iteration 24: strict convergence.\n",
      "Training a K-Means model with 12 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.1405163684988813e+20\n",
      "Iteration 1, inertia 1.853191646870698e+20\n",
      "Iteration 2, inertia 1.7795277206511298e+20\n",
      "Iteration 3, inertia 1.73834506481586e+20\n",
      "Iteration 4, inertia 1.710089043097894e+20\n",
      "Iteration 5, inertia 1.6925927365058652e+20\n",
      "Iteration 6, inertia 1.6826362834243076e+20\n",
      "Iteration 7, inertia 1.668841045138365e+20\n",
      "Iteration 8, inertia 1.6549490043952847e+20\n",
      "Iteration 9, inertia 1.6448971633692167e+20\n",
      "Iteration 10, inertia 1.633127275870933e+20\n",
      "Iteration 11, inertia 1.6203713717896918e+20\n",
      "Iteration 12, inertia 1.607214020280221e+20\n",
      "Iteration 13, inertia 1.594684165183022e+20\n",
      "Iteration 14, inertia 1.5791374801617e+20\n",
      "Iteration 15, inertia 1.5640215964241042e+20\n",
      "Iteration 16, inertia 1.552865707314496e+20\n",
      "Iteration 17, inertia 1.5458512106532284e+20\n",
      "Iteration 18, inertia 1.540428175145588e+20\n",
      "Iteration 19, inertia 1.5382730396667177e+20\n",
      "Iteration 20, inertia 1.5356583173713478e+20\n",
      "Iteration 21, inertia 1.5333481492332013e+20\n",
      "Iteration 22, inertia 1.531322370456636e+20\n",
      "Iteration 23, inertia 1.5303285777096068e+20\n",
      "Iteration 24, inertia 1.5286858805360483e+20\n",
      "Iteration 25, inertia 1.5278086443558345e+20\n",
      "Iteration 26, inertia 1.5273665357459803e+20\n",
      "Converged at iteration 26: center shift 5200868053485.816 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.2105454384988674e+20\n",
      "Iteration 1, inertia 1.863441624166402e+20\n",
      "Iteration 2, inertia 1.8018510341473003e+20\n",
      "Iteration 3, inertia 1.7572493896242535e+20\n",
      "Iteration 4, inertia 1.7256499380149957e+20\n",
      "Iteration 5, inertia 1.705697887098114e+20\n",
      "Iteration 6, inertia 1.6852540341380733e+20\n",
      "Iteration 7, inertia 1.6640793998901953e+20\n",
      "Iteration 8, inertia 1.6458673485482574e+20\n",
      "Iteration 9, inertia 1.6272430930077417e+20\n",
      "Iteration 10, inertia 1.605944154792816e+20\n",
      "Iteration 11, inertia 1.584507120749282e+20\n",
      "Iteration 12, inertia 1.5638433858629498e+20\n",
      "Iteration 13, inertia 1.5545084963777036e+20\n",
      "Iteration 14, inertia 1.5489582735107355e+20\n",
      "Iteration 15, inertia 1.5439403842458708e+20\n",
      "Iteration 16, inertia 1.540865217230147e+20\n",
      "Iteration 17, inertia 1.539722925816149e+20\n",
      "Iteration 18, inertia 1.536493633675786e+20\n",
      "Iteration 19, inertia 1.534524578229941e+20\n",
      "Iteration 20, inertia 1.5326118527269732e+20\n",
      "Iteration 21, inertia 1.5296506482004653e+20\n",
      "Iteration 22, inertia 1.5279684610597906e+20\n",
      "Iteration 23, inertia 1.5274142097984068e+20\n",
      "Iteration 24, inertia 1.52679587686213e+20\n",
      "Iteration 25, inertia 1.526010583019822e+20\n",
      "Iteration 26, inertia 1.525878343956783e+20\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.864666189219817e+20\n",
      "Iteration 1, inertia 1.665354122844555e+20\n",
      "Iteration 2, inertia 1.60008174701819e+20\n",
      "Iteration 3, inertia 1.573444089063205e+20\n",
      "Iteration 4, inertia 1.55476699526731e+20\n",
      "Iteration 5, inertia 1.5445899804661055e+20\n",
      "Iteration 6, inertia 1.5398322455396568e+20\n",
      "Iteration 7, inertia 1.5320577359895488e+20\n",
      "Iteration 8, inertia 1.5228798685492403e+20\n",
      "Iteration 9, inertia 1.513361742499309e+20\n",
      "Iteration 10, inertia 1.505881131465154e+20\n",
      "Iteration 11, inertia 1.503286496889723e+20\n",
      "Iteration 12, inertia 1.5018360481070658e+20\n",
      "Iteration 13, inertia 1.4996948054550417e+20\n",
      "Iteration 14, inertia 1.4977420289079643e+20\n",
      "Iteration 15, inertia 1.496278410242569e+20\n",
      "Iteration 16, inertia 1.495452841933682e+20\n",
      "Iteration 17, inertia 1.495127994281846e+20\n",
      "Converged at iteration 17: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.9213953884988703e+20\n",
      "Iteration 1, inertia 1.5559362471191052e+20\n",
      "Iteration 2, inertia 1.5275056581633645e+20\n",
      "Iteration 3, inertia 1.5156545502421097e+20\n",
      "Iteration 4, inertia 1.5075397546702075e+20\n",
      "Iteration 5, inertia 1.5003183967305227e+20\n",
      "Iteration 6, inertia 1.492862358238704e+20\n",
      "Iteration 7, inertia 1.4901101658812452e+20\n",
      "Iteration 8, inertia 1.489347734182225e+20\n",
      "Converged at iteration 8: center shift 13336154850565.053 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.9879561884988742e+20\n",
      "Iteration 1, inertia 1.7621637762956493e+20\n",
      "Iteration 2, inertia 1.7103617057448165e+20\n",
      "Iteration 3, inertia 1.674866948732051e+20\n",
      "Iteration 4, inertia 1.6528737997771768e+20\n",
      "Iteration 5, inertia 1.6331257767046142e+20\n",
      "Iteration 6, inertia 1.6098563850636172e+20\n",
      "Iteration 7, inertia 1.5919309996695165e+20\n",
      "Iteration 8, inertia 1.575637013675714e+20\n",
      "Iteration 9, inertia 1.5604149876626847e+20\n",
      "Iteration 10, inertia 1.54852217550134e+20\n",
      "Iteration 11, inertia 1.5358674728187757e+20\n",
      "Iteration 12, inertia 1.525008784997871e+20\n",
      "Iteration 13, inertia 1.5180228572286645e+20\n",
      "Iteration 14, inertia 1.5137180109244223e+20\n",
      "Iteration 15, inertia 1.510862607979438e+20\n",
      "Iteration 16, inertia 1.5098036209399847e+20\n",
      "Iteration 17, inertia 1.509029347934469e+20\n",
      "Iteration 18, inertia 1.5062900532665516e+20\n",
      "Iteration 19, inertia 1.5036989252818924e+20\n",
      "Iteration 20, inertia 1.501859562840516e+20\n",
      "Iteration 21, inertia 1.501333446189456e+20\n",
      "Iteration 22, inertia 1.498845797288431e+20\n",
      "Iteration 23, inertia 1.4974194509729743e+20\n",
      "Iteration 24, inertia 1.495329250096363e+20\n",
      "Iteration 25, inertia 1.494487004323869e+20\n",
      "Iteration 26, inertia 1.4939852307106785e+20\n",
      "Iteration 27, inertia 1.4935077816921568e+20\n",
      "Iteration 28, inertia 1.4934696529149287e+20\n",
      "Converged at iteration 28: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.9020442884988803e+20\n",
      "Iteration 1, inertia 1.5943803677347594e+20\n",
      "Iteration 2, inertia 1.5500386461758964e+20\n",
      "Iteration 3, inertia 1.53361618811915e+20\n",
      "Iteration 4, inertia 1.5242892913456985e+20\n",
      "Iteration 5, inertia 1.5182246113937254e+20\n",
      "Iteration 6, inertia 1.513107898163015e+20\n",
      "Iteration 7, inertia 1.5093314104726084e+20\n",
      "Iteration 8, inertia 1.5066914237103374e+20\n",
      "Iteration 9, inertia 1.5040125020882167e+20\n",
      "Iteration 10, inertia 1.501385800431978e+20\n",
      "Iteration 11, inertia 1.4973459463940768e+20\n",
      "Iteration 12, inertia 1.495788495647733e+20\n",
      "Iteration 13, inertia 1.494358706586852e+20\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.8107764084988536e+20\n",
      "Iteration 1, inertia 1.6740810006662162e+20\n",
      "Iteration 2, inertia 1.6329526129698626e+20\n",
      "Iteration 3, inertia 1.60825435798438e+20\n",
      "Iteration 4, inertia 1.5917437863020728e+20\n",
      "Iteration 5, inertia 1.578648791494148e+20\n",
      "Iteration 6, inertia 1.566411592104986e+20\n",
      "Iteration 7, inertia 1.5558152220869604e+20\n",
      "Iteration 8, inertia 1.5460057333168367e+20\n",
      "Iteration 9, inertia 1.5395148437052883e+20\n",
      "Iteration 10, inertia 1.5318983675520552e+20\n",
      "Iteration 11, inertia 1.5259009719962357e+20\n",
      "Iteration 12, inertia 1.522398576246414e+20\n",
      "Iteration 13, inertia 1.517967334182821e+20\n",
      "Iteration 14, inertia 1.508215389154761e+20\n",
      "Iteration 15, inertia 1.5027384273172867e+20\n",
      "Iteration 16, inertia 1.499214588953573e+20\n",
      "Iteration 17, inertia 1.49571492821938e+20\n",
      "Iteration 18, inertia 1.493406799714934e+20\n",
      "Iteration 19, inertia 1.492672716735243e+20\n",
      "Iteration 20, inertia 1.492376580590005e+20\n",
      "Iteration 21, inertia 1.4921123077990236e+20\n",
      "Iteration 22, inertia 1.4919972603086052e+20\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.891200858498879e+20\n",
      "Iteration 1, inertia 1.5919869665705106e+20\n",
      "Iteration 2, inertia 1.5504389825044067e+20\n",
      "Iteration 3, inertia 1.5350535972764582e+20\n",
      "Iteration 4, inertia 1.528668170736563e+20\n",
      "Iteration 5, inertia 1.5263156964603134e+20\n",
      "Iteration 6, inertia 1.5252891675594673e+20\n",
      "Iteration 7, inertia 1.5242496810884753e+20\n",
      "Iteration 8, inertia 1.5221829943788274e+20\n",
      "Iteration 9, inertia 1.5204848684033193e+20\n",
      "Iteration 10, inertia 1.517788630010886e+20\n",
      "Iteration 11, inertia 1.5149004453711148e+20\n",
      "Iteration 12, inertia 1.5126421348765195e+20\n",
      "Iteration 13, inertia 1.5106414613594046e+20\n",
      "Iteration 14, inertia 1.508539723525016e+20\n",
      "Iteration 15, inertia 1.5064940797947596e+20\n",
      "Iteration 16, inertia 1.5043657710713458e+20\n",
      "Iteration 17, inertia 1.5025243706033147e+20\n",
      "Iteration 18, inertia 1.502052728663621e+20\n",
      "Iteration 19, inertia 1.5005299246946684e+20\n",
      "Iteration 20, inertia 1.5000051822810907e+20\n",
      "Iteration 21, inertia 1.499162323777992e+20\n",
      "Iteration 22, inertia 1.4981117271259978e+20\n",
      "Iteration 23, inertia 1.4971809794939306e+20\n",
      "Iteration 24, inertia 1.4963767998004625e+20\n",
      "Iteration 25, inertia 1.4957919728154313e+20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 25: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.9280005184988827e+20\n",
      "Iteration 1, inertia 1.7058786408850997e+20\n",
      "Iteration 2, inertia 1.6629535838155814e+20\n",
      "Iteration 3, inertia 1.642581399927402e+20\n",
      "Iteration 4, inertia 1.623245820472555e+20\n",
      "Iteration 5, inertia 1.6096207001134753e+20\n",
      "Iteration 6, inertia 1.5956325076763404e+20\n",
      "Iteration 7, inertia 1.5792712733197995e+20\n",
      "Iteration 8, inertia 1.566975679423952e+20\n",
      "Iteration 9, inertia 1.553789831029925e+20\n",
      "Iteration 10, inertia 1.5457881149164877e+20\n",
      "Iteration 11, inertia 1.5407267613247042e+20\n",
      "Iteration 12, inertia 1.5379633245214166e+20\n",
      "Iteration 13, inertia 1.5357502489774044e+20\n",
      "Iteration 14, inertia 1.5333946171567892e+20\n",
      "Iteration 15, inertia 1.5312210818625543e+20\n",
      "Iteration 16, inertia 1.5299041854873328e+20\n",
      "Iteration 17, inertia 1.5280049474343436e+20\n",
      "Iteration 18, inertia 1.52712771125413e+20\n",
      "Iteration 19, inertia 1.526685602644276e+20\n",
      "Converged at iteration 19: center shift 5200868053490.605 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.1259406684988677e+20\n",
      "Iteration 1, inertia 1.617220564571728e+20\n",
      "Iteration 2, inertia 1.5707549748711886e+20\n",
      "Iteration 3, inertia 1.5527150739974645e+20\n",
      "Iteration 4, inertia 1.5399252596373068e+20\n",
      "Iteration 5, inertia 1.5296435827678796e+20\n",
      "Iteration 6, inertia 1.5203178165900916e+20\n",
      "Iteration 7, inertia 1.5110914707879936e+20\n",
      "Iteration 8, inertia 1.5016538879699832e+20\n",
      "Iteration 9, inertia 1.496757412723147e+20\n",
      "Iteration 10, inertia 1.4941055810582528e+20\n",
      "Iteration 11, inertia 1.4927359606063222e+20\n",
      "Iteration 12, inertia 1.492227346847173e+20\n",
      "Iteration 13, inertia 1.4915006426830347e+20\n",
      "Iteration 14, inertia 1.4913324130737514e+20\n",
      "Iteration 15, inertia 1.4911475177077965e+20\n",
      "Converged at iteration 15: strict convergence.\n",
      "Training a K-Means model with 13 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.7608913684988815e+20\n",
      "Iteration 1, inertia 1.478406467292444e+20\n",
      "Iteration 2, inertia 1.4180861390048274e+20\n",
      "Iteration 3, inertia 1.396270214744576e+20\n",
      "Iteration 4, inertia 1.3825071543781094e+20\n",
      "Iteration 5, inertia 1.374414796502856e+20\n",
      "Iteration 6, inertia 1.3700012759993405e+20\n",
      "Iteration 7, inertia 1.366010275057005e+20\n",
      "Iteration 8, inertia 1.3625956796351824e+20\n",
      "Iteration 9, inertia 1.3585636807105959e+20\n",
      "Iteration 10, inertia 1.3533731508368435e+20\n",
      "Iteration 11, inertia 1.349790734469932e+20\n",
      "Iteration 12, inertia 1.3472730861010367e+20\n",
      "Iteration 13, inertia 1.344300702176982e+20\n",
      "Iteration 14, inertia 1.3415035204145404e+20\n",
      "Iteration 15, inertia 1.3392196585625195e+20\n",
      "Iteration 16, inertia 1.3373058594320776e+20\n",
      "Iteration 17, inertia 1.3344505793184548e+20\n",
      "Iteration 18, inertia 1.3317695815840534e+20\n",
      "Iteration 19, inertia 1.3309324599851056e+20\n",
      "Iteration 20, inertia 1.3305008320829787e+20\n",
      "Iteration 21, inertia 1.330438177177581e+20\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.6544253384988747e+20\n",
      "Iteration 1, inertia 1.4344342566325099e+20\n",
      "Iteration 2, inertia 1.3953370486928625e+20\n",
      "Iteration 3, inertia 1.378960767886817e+20\n",
      "Iteration 4, inertia 1.3632041475345034e+20\n",
      "Iteration 5, inertia 1.345887722968486e+20\n",
      "Iteration 6, inertia 1.3316241611020413e+20\n",
      "Iteration 7, inertia 1.3181605734598117e+20\n",
      "Iteration 8, inertia 1.3024466483492898e+20\n",
      "Iteration 9, inertia 1.288833595012679e+20\n",
      "Iteration 10, inertia 1.2732623919488808e+20\n",
      "Iteration 11, inertia 1.2635154379652155e+20\n",
      "Iteration 12, inertia 1.2589850011015427e+20\n",
      "Iteration 13, inertia 1.2566860630265954e+20\n",
      "Iteration 14, inertia 1.2546831945661533e+20\n",
      "Iteration 15, inertia 1.2537233083236168e+20\n",
      "Iteration 16, inertia 1.2524092855396188e+20\n",
      "Iteration 17, inertia 1.2507402204214628e+20\n",
      "Iteration 18, inertia 1.2496365207896275e+20\n",
      "Iteration 19, inertia 1.2485873798086399e+20\n",
      "Iteration 20, inertia 1.2480253337537507e+20\n",
      "Iteration 21, inertia 1.2477807110450902e+20\n",
      "Iteration 22, inertia 1.2472423692885064e+20\n",
      "Iteration 23, inertia 1.246880847787048e+20\n",
      "Iteration 24, inertia 1.2462314402171159e+20\n",
      "Iteration 25, inertia 1.2448331692336352e+20\n",
      "Iteration 26, inertia 1.2442187362451774e+20\n",
      "Iteration 27, inertia 1.2439893222644684e+20\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.5388293884988834e+20\n",
      "Iteration 1, inertia 1.275599409077731e+20\n",
      "Iteration 2, inertia 1.2538118244358395e+20\n",
      "Iteration 3, inertia 1.2471641207883848e+20\n",
      "Iteration 4, inertia 1.2444950833986888e+20\n",
      "Iteration 5, inertia 1.2430211377802751e+20\n",
      "Iteration 6, inertia 1.2412107322089692e+20\n",
      "Iteration 7, inertia 1.2402260628449049e+20\n",
      "Iteration 8, inertia 1.2389307438426024e+20\n",
      "Iteration 9, inertia 1.2386832390803463e+20\n",
      "Iteration 10, inertia 1.2383837841819861e+20\n",
      "Converged at iteration 10: center shift 5200868053489.311 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.5768793484988816e+20\n",
      "Iteration 1, inertia 1.3517165938450645e+20\n",
      "Iteration 2, inertia 1.3304213473229332e+20\n",
      "Iteration 3, inertia 1.317877532050859e+20\n",
      "Iteration 4, inertia 1.3089824492481203e+20\n",
      "Iteration 5, inertia 1.2976794545101133e+20\n",
      "Iteration 6, inertia 1.2876982448691098e+20\n",
      "Iteration 7, inertia 1.277682972931747e+20\n",
      "Iteration 8, inertia 1.2681329777892914e+20\n",
      "Iteration 9, inertia 1.2644562964716287e+20\n",
      "Iteration 10, inertia 1.2635032066937687e+20\n",
      "Iteration 11, inertia 1.261879305986377e+20\n",
      "Iteration 12, inertia 1.2610486414831601e+20\n",
      "Iteration 13, inertia 1.2601686471184507e+20\n",
      "Iteration 14, inertia 1.2595306607526406e+20\n",
      "Iteration 15, inertia 1.2592063924395518e+20\n",
      "Iteration 16, inertia 1.258252613759257e+20\n",
      "Iteration 17, inertia 1.258086476651879e+20\n",
      "Iteration 18, inertia 1.2578366363390924e+20\n",
      "Iteration 19, inertia 1.2575976068337762e+20\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.6956064884988835e+20\n",
      "Iteration 1, inertia 1.367585488547185e+20\n",
      "Iteration 2, inertia 1.2952997441600381e+20\n",
      "Iteration 3, inertia 1.2684852335186125e+20\n",
      "Iteration 4, inertia 1.2548494200289152e+20\n",
      "Iteration 5, inertia 1.2476013152037267e+20\n",
      "Iteration 6, inertia 1.2440963236080445e+20\n",
      "Iteration 7, inertia 1.2427574001782265e+20\n",
      "Iteration 8, inertia 1.2420325017835704e+20\n",
      "Converged at iteration 8: center shift 5198086583839.957 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.616027808498884e+20\n",
      "Iteration 1, inertia 1.3924594901694534e+20\n",
      "Iteration 2, inertia 1.3580670698278535e+20\n",
      "Iteration 3, inertia 1.3440060512629704e+20\n",
      "Iteration 4, inertia 1.3320641831374543e+20\n",
      "Iteration 5, inertia 1.3230330566401475e+20\n",
      "Iteration 6, inertia 1.3138416734030964e+20\n",
      "Iteration 7, inertia 1.3041402340456907e+20\n",
      "Iteration 8, inertia 1.2955631199888541e+20\n",
      "Iteration 9, inertia 1.2893401344113793e+20\n",
      "Iteration 10, inertia 1.2832929667040379e+20\n",
      "Iteration 11, inertia 1.278672537469622e+20\n",
      "Iteration 12, inertia 1.2752163108793185e+20\n",
      "Iteration 13, inertia 1.2719567795254005e+20\n",
      "Iteration 14, inertia 1.2699003902500987e+20\n",
      "Iteration 15, inertia 1.266906382217912e+20\n",
      "Iteration 16, inertia 1.2645231887543204e+20\n",
      "Iteration 17, inertia 1.262995227161532e+20\n",
      "Iteration 18, inertia 1.261997843473014e+20\n",
      "Iteration 19, inertia 1.2618330381897889e+20\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.440701608498882e+20\n",
      "Iteration 1, inertia 1.2995911551705388e+20\n",
      "Iteration 2, inertia 1.267292035038611e+20\n",
      "Iteration 3, inertia 1.253494153721255e+20\n",
      "Iteration 4, inertia 1.2454975820006833e+20\n",
      "Iteration 5, inertia 1.2421078463342019e+20\n",
      "Iteration 6, inertia 1.2390349597012777e+20\n",
      "Iteration 7, inertia 1.2373470619268048e+20\n",
      "Iteration 8, inertia 1.2358436666843672e+20\n",
      "Iteration 9, inertia 1.2355101035882709e+20\n",
      "Iteration 10, inertia 1.2352106486899112e+20\n",
      "Converged at iteration 10: center shift 5200868053493.648 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.7366222484988818e+20\n",
      "Iteration 1, inertia 1.2906169332299963e+20\n",
      "Iteration 2, inertia 1.2719828181979416e+20\n",
      "Iteration 3, inertia 1.263060484994288e+20\n",
      "Iteration 4, inertia 1.2579430981206327e+20\n",
      "Iteration 5, inertia 1.255469966509641e+20\n",
      "Iteration 6, inertia 1.2552228414431334e+20\n",
      "Iteration 7, inertia 1.2551588351363644e+20\n",
      "Converged at iteration 7: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.64674048888161e+20\n",
      "Iteration 1, inertia 1.4255478437014289e+20\n",
      "Iteration 2, inertia 1.3642285266744066e+20\n",
      "Iteration 3, inertia 1.3386607319509987e+20\n",
      "Iteration 4, inertia 1.3269505355742256e+20\n",
      "Iteration 5, inertia 1.321717594597621e+20\n",
      "Iteration 6, inertia 1.3163881842013515e+20\n",
      "Iteration 7, inertia 1.3114492946349559e+20\n",
      "Iteration 8, inertia 1.3064146155060426e+20\n",
      "Iteration 9, inertia 1.3023029984928947e+20\n",
      "Iteration 10, inertia 1.2986196583590491e+20\n",
      "Iteration 11, inertia 1.2954656972954368e+20\n",
      "Iteration 12, inertia 1.2915911003850975e+20\n",
      "Iteration 13, inertia 1.2900541177456214e+20\n",
      "Iteration 14, inertia 1.287374635377845e+20\n",
      "Iteration 15, inertia 1.2859729294303704e+20\n",
      "Iteration 16, inertia 1.28326217372288e+20\n",
      "Iteration 17, inertia 1.2787503593734871e+20\n",
      "Iteration 18, inertia 1.2763444043246812e+20\n",
      "Iteration 19, inertia 1.2715182743495195e+20\n",
      "Iteration 20, inertia 1.2680473462370707e+20\n",
      "Iteration 21, inertia 1.2661084886293545e+20\n",
      "Iteration 22, inertia 1.2648099310947369e+20\n",
      "Iteration 23, inertia 1.2640713289876413e+20\n",
      "Converged at iteration 23: center shift 11127439946780.285 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.7148578684988817e+20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, inertia 1.413806501725342e+20\n",
      "Iteration 2, inertia 1.3553527613986383e+20\n",
      "Iteration 3, inertia 1.3225288710676834e+20\n",
      "Iteration 4, inertia 1.299301417647573e+20\n",
      "Iteration 5, inertia 1.2845957824990347e+20\n",
      "Iteration 6, inertia 1.2702146620651225e+20\n",
      "Iteration 7, inertia 1.2643977863421105e+20\n",
      "Iteration 8, inertia 1.261668301386174e+20\n",
      "Iteration 9, inertia 1.2598888968066992e+20\n",
      "Iteration 10, inertia 1.2582976163790904e+20\n",
      "Iteration 11, inertia 1.2565804858521546e+20\n",
      "Iteration 12, inertia 1.2547105675705586e+20\n",
      "Iteration 13, inertia 1.2537133568655086e+20\n",
      "Iteration 14, inertia 1.2524587099834162e+20\n",
      "Iteration 15, inertia 1.2520457990327619e+20\n",
      "Iteration 16, inertia 1.2507631615364999e+20\n",
      "Iteration 17, inertia 1.249794402915405e+20\n",
      "Iteration 18, inertia 1.2493304324709699e+20\n",
      "Iteration 19, inertia 1.2485334964945856e+20\n",
      "Iteration 20, inertia 1.2481736274254732e+20\n",
      "Iteration 21, inertia 1.2475523737660495e+20\n",
      "Iteration 22, inertia 1.2473195075071469e+20\n",
      "Iteration 23, inertia 1.246387058322755e+20\n",
      "Iteration 24, inertia 1.2456284372508166e+20\n",
      "Iteration 25, inertia 1.245240994095403e+20\n",
      "Iteration 26, inertia 1.244964762070882e+20\n",
      "Iteration 27, inertia 1.2448743846265695e+20\n",
      "Iteration 28, inertia 1.2446448338725788e+20\n",
      "Iteration 29, inertia 1.2445464634264209e+20\n",
      "Iteration 30, inertia 1.244351909486272e+20\n",
      "Converged at iteration 30: strict convergence.\n",
      "Training a K-Means model with 14 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.4297273684988815e+20\n",
      "Iteration 1, inertia 1.1593106394842936e+20\n",
      "Iteration 2, inertia 1.1271750657107442e+20\n",
      "Iteration 3, inertia 1.1143232956536771e+20\n",
      "Iteration 4, inertia 1.1060926890337118e+20\n",
      "Iteration 5, inertia 1.1031157101368721e+20\n",
      "Iteration 6, inertia 1.1022349581479312e+20\n",
      "Iteration 7, inertia 1.1017135823324874e+20\n",
      "Iteration 8, inertia 1.1013614094086131e+20\n",
      "Iteration 9, inertia 1.100666920242335e+20\n",
      "Iteration 10, inertia 1.0998093973411303e+20\n",
      "Iteration 11, inertia 1.0996620900775422e+20\n",
      "Iteration 12, inertia 1.0994991105586441e+20\n",
      "Converged at iteration 12: center shift 13256552724356.457 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.3963217884988835e+20\n",
      "Iteration 1, inertia 1.2469418585137899e+20\n",
      "Iteration 2, inertia 1.2097509045875894e+20\n",
      "Iteration 3, inertia 1.1895473294198009e+20\n",
      "Iteration 4, inertia 1.1723776852961062e+20\n",
      "Iteration 5, inertia 1.1579218654580366e+20\n",
      "Iteration 6, inertia 1.1476819651258219e+20\n",
      "Iteration 7, inertia 1.1429198498047453e+20\n",
      "Iteration 8, inertia 1.1405352942979411e+20\n",
      "Iteration 9, inertia 1.13830156239797e+20\n",
      "Iteration 10, inertia 1.1366500757573485e+20\n",
      "Iteration 11, inertia 1.133653839713424e+20\n",
      "Iteration 12, inertia 1.1303514069224931e+20\n",
      "Iteration 13, inertia 1.1280329436533332e+20\n",
      "Iteration 14, inertia 1.1265738437188852e+20\n",
      "Iteration 15, inertia 1.1248707891934814e+20\n",
      "Iteration 16, inertia 1.1218429976266803e+20\n",
      "Iteration 17, inertia 1.119006379039352e+20\n",
      "Iteration 18, inertia 1.116812599500366e+20\n",
      "Iteration 19, inertia 1.1159855753055882e+20\n",
      "Iteration 20, inertia 1.114537737554403e+20\n",
      "Iteration 21, inertia 1.1135949111609623e+20\n",
      "Iteration 22, inertia 1.1113517652120932e+20\n",
      "Iteration 23, inertia 1.1083267593909207e+20\n",
      "Iteration 24, inertia 1.1065965874756146e+20\n",
      "Iteration 25, inertia 1.104403249526578e+20\n",
      "Iteration 26, inertia 1.103486292275656e+20\n",
      "Iteration 27, inertia 1.1023210433265856e+20\n",
      "Iteration 28, inertia 1.1014820646460028e+20\n",
      "Iteration 29, inertia 1.1008754541001363e+20\n",
      "Converged at iteration 29: center shift 6874962149964.281 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.5299812584988823e+20\n",
      "Iteration 1, inertia 1.1039520631227213e+20\n",
      "Iteration 2, inertia 1.0714107916718201e+20\n",
      "Iteration 3, inertia 1.0576102445355608e+20\n",
      "Iteration 4, inertia 1.0529479933090511e+20\n",
      "Iteration 5, inertia 1.0496248312260002e+20\n",
      "Iteration 6, inertia 1.0461164789996264e+20\n",
      "Iteration 7, inertia 1.043423185666979e+20\n",
      "Iteration 8, inertia 1.0410695425122515e+20\n",
      "Iteration 9, inertia 1.0397912303259006e+20\n",
      "Iteration 10, inertia 1.0390848580721983e+20\n",
      "Iteration 11, inertia 1.0389154780653286e+20\n",
      "Converged at iteration 11: center shift 2564444308125.452 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.3387864984988744e+20\n",
      "Iteration 1, inertia 1.2630038778145733e+20\n",
      "Iteration 2, inertia 1.2539530568063835e+20\n",
      "Iteration 3, inertia 1.2478964979863521e+20\n",
      "Iteration 4, inertia 1.2430580519613391e+20\n",
      "Iteration 5, inertia 1.2379199877875861e+20\n",
      "Iteration 6, inertia 1.2351125464504708e+20\n",
      "Iteration 7, inertia 1.2306137636792864e+20\n",
      "Iteration 8, inertia 1.2271541795492776e+20\n",
      "Iteration 9, inertia 1.2238029333844838e+20\n",
      "Iteration 10, inertia 1.2204397311242371e+20\n",
      "Iteration 11, inertia 1.215361669335662e+20\n",
      "Iteration 12, inertia 1.2104682221249646e+20\n",
      "Iteration 13, inertia 1.2038745966422127e+20\n",
      "Iteration 14, inertia 1.1986120307118021e+20\n",
      "Iteration 15, inertia 1.1934422386269136e+20\n",
      "Iteration 16, inertia 1.1856525192747473e+20\n",
      "Iteration 17, inertia 1.1794649290041226e+20\n",
      "Iteration 18, inertia 1.1725408179280082e+20\n",
      "Iteration 19, inertia 1.1645054505832184e+20\n",
      "Iteration 20, inertia 1.1576176993433595e+20\n",
      "Iteration 21, inertia 1.1518975089384035e+20\n",
      "Iteration 22, inertia 1.138665078236104e+20\n",
      "Iteration 23, inertia 1.1270260334453871e+20\n",
      "Iteration 24, inertia 1.1193332191408456e+20\n",
      "Iteration 25, inertia 1.1128716870799072e+20\n",
      "Iteration 26, inertia 1.1098285224809275e+20\n",
      "Iteration 27, inertia 1.1086065108871129e+20\n",
      "Iteration 28, inertia 1.1075835050758832e+20\n",
      "Iteration 29, inertia 1.106585285644858e+20\n",
      "Iteration 30, inertia 1.106269042189477e+20\n",
      "Iteration 31, inertia 1.105679452215797e+20\n",
      "Iteration 32, inertia 1.1053113257762808e+20\n",
      "Iteration 33, inertia 1.1050886849269447e+20\n",
      "Converged at iteration 33: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.3848408884988769e+20\n",
      "Iteration 1, inertia 1.2713833869078423e+20\n",
      "Iteration 2, inertia 1.2439611096593714e+20\n",
      "Iteration 3, inertia 1.2271177448862638e+20\n",
      "Iteration 4, inertia 1.220397769596535e+20\n",
      "Iteration 5, inertia 1.2163880019662622e+20\n",
      "Iteration 6, inertia 1.2126256388699447e+20\n",
      "Iteration 7, inertia 1.2066562050580432e+20\n",
      "Iteration 8, inertia 1.2021720643785825e+20\n",
      "Iteration 9, inertia 1.1991120656030004e+20\n",
      "Iteration 10, inertia 1.1982968006794573e+20\n",
      "Iteration 11, inertia 1.1974358827898018e+20\n",
      "Iteration 12, inertia 1.1969919528744318e+20\n",
      "Iteration 13, inertia 1.1960847405492326e+20\n",
      "Iteration 14, inertia 1.1957574880942493e+20\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.4765867384988782e+20\n",
      "Iteration 1, inertia 1.1816946055461308e+20\n",
      "Iteration 2, inertia 1.1439888208597513e+20\n",
      "Iteration 3, inertia 1.1305573959911686e+20\n",
      "Iteration 4, inertia 1.12385851138123e+20\n",
      "Iteration 5, inertia 1.1184875065734657e+20\n",
      "Iteration 6, inertia 1.113937916440639e+20\n",
      "Iteration 7, inertia 1.109656150118686e+20\n",
      "Iteration 8, inertia 1.1063293867921534e+20\n",
      "Iteration 9, inertia 1.1044244424775665e+20\n",
      "Iteration 10, inertia 1.1024498059089386e+20\n",
      "Iteration 11, inertia 1.0991176797538304e+20\n",
      "Iteration 12, inertia 1.097028001869602e+20\n",
      "Iteration 13, inertia 1.095587753207399e+20\n",
      "Iteration 14, inertia 1.0939658747297531e+20\n",
      "Iteration 15, inertia 1.0920170494800973e+20\n",
      "Iteration 16, inertia 1.0897813732638976e+20\n",
      "Iteration 17, inertia 1.0874276197824345e+20\n",
      "Iteration 18, inertia 1.0860812976788346e+20\n",
      "Iteration 19, inertia 1.0850904794716026e+20\n",
      "Iteration 20, inertia 1.0842737487319495e+20\n",
      "Iteration 21, inertia 1.084155901417097e+20\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1995257084988735e+20\n",
      "Iteration 1, inertia 1.1427562432767623e+20\n",
      "Iteration 2, inertia 1.1301578414617847e+20\n",
      "Iteration 3, inertia 1.1231300756599375e+20\n",
      "Iteration 4, inertia 1.116249248341275e+20\n",
      "Iteration 5, inertia 1.1062577850265897e+20\n",
      "Iteration 6, inertia 1.0975695474431885e+20\n",
      "Iteration 7, inertia 1.0920430152647262e+20\n",
      "Iteration 8, inertia 1.0877820152917087e+20\n",
      "Iteration 9, inertia 1.0821421428529132e+20\n",
      "Iteration 10, inertia 1.0776796477234328e+20\n",
      "Iteration 11, inertia 1.0767309980100054e+20\n",
      "Iteration 12, inertia 1.0752893311962464e+20\n",
      "Iteration 13, inertia 1.0744827706930697e+20\n",
      "Iteration 14, inertia 1.0740657206251656e+20\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.4335161884988352e+20\n",
      "Iteration 1, inertia 1.299343854934552e+20\n",
      "Iteration 2, inertia 1.2700826396497835e+20\n",
      "Iteration 3, inertia 1.2507623194862186e+20\n",
      "Iteration 4, inertia 1.2404402101103577e+20\n",
      "Iteration 5, inertia 1.2347778950009668e+20\n",
      "Iteration 6, inertia 1.2319610528721666e+20\n",
      "Iteration 7, inertia 1.2298566692294066e+20\n",
      "Iteration 8, inertia 1.2288862394390762e+20\n",
      "Iteration 9, inertia 1.2280678755830979e+20\n",
      "Iteration 10, inertia 1.2278316810137516e+20\n",
      "Iteration 11, inertia 1.2270955683414532e+20\n",
      "Iteration 12, inertia 1.2269125517452476e+20\n",
      "Iteration 13, inertia 1.2254415868574848e+20\n",
      "Iteration 14, inertia 1.2237104445341314e+20\n",
      "Iteration 15, inertia 1.2213603404333262e+20\n",
      "Iteration 16, inertia 1.219514656980609e+20\n",
      "Iteration 17, inertia 1.2187371992421253e+20\n",
      "Iteration 18, inertia 1.2178660034591783e+20\n",
      "Iteration 19, inertia 1.216525669763513e+20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, inertia 1.2148864652115082e+20\n",
      "Iteration 21, inertia 1.2135625547930893e+20\n",
      "Iteration 22, inertia 1.2130091433820573e+20\n",
      "Iteration 23, inertia 1.2128233889841934e+20\n",
      "Iteration 24, inertia 1.2124310632150529e+20\n",
      "Iteration 25, inertia 1.2120365338317249e+20\n",
      "Iteration 26, inertia 1.211564072051682e+20\n",
      "Iteration 27, inertia 1.211380880276482e+20\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.499140988498861e+20\n",
      "Iteration 1, inertia 1.1193843078182337e+20\n",
      "Iteration 2, inertia 1.0947418817343693e+20\n",
      "Iteration 3, inertia 1.0812056244616005e+20\n",
      "Iteration 4, inertia 1.0719148659632833e+20\n",
      "Iteration 5, inertia 1.0651063794846178e+20\n",
      "Iteration 6, inertia 1.0604022528099813e+20\n",
      "Iteration 7, inertia 1.0567883772323409e+20\n",
      "Iteration 8, inertia 1.0544559945778273e+20\n",
      "Iteration 9, inertia 1.0520235041563322e+20\n",
      "Iteration 10, inertia 1.049754931940804e+20\n",
      "Iteration 11, inertia 1.0479143588356964e+20\n",
      "Iteration 12, inertia 1.0451263974863879e+20\n",
      "Iteration 13, inertia 1.0442761351030057e+20\n",
      "Iteration 14, inertia 1.0440125823933576e+20\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.4267994584988795e+20\n",
      "Iteration 1, inertia 1.2714441063318448e+20\n",
      "Iteration 2, inertia 1.2286955846082144e+20\n",
      "Iteration 3, inertia 1.2021825427559729e+20\n",
      "Iteration 4, inertia 1.1822843240746266e+20\n",
      "Iteration 5, inertia 1.1697043724434291e+20\n",
      "Iteration 6, inertia 1.1614269645763879e+20\n",
      "Iteration 7, inertia 1.1567816236549387e+20\n",
      "Iteration 8, inertia 1.1525517073768805e+20\n",
      "Iteration 9, inertia 1.1502285194721655e+20\n",
      "Iteration 10, inertia 1.1467343797716338e+20\n",
      "Iteration 11, inertia 1.1424512058409373e+20\n",
      "Iteration 12, inertia 1.1387090213694104e+20\n",
      "Iteration 13, inertia 1.1345858776314213e+20\n",
      "Iteration 14, inertia 1.1301178209257115e+20\n",
      "Iteration 15, inertia 1.1271753356139243e+20\n",
      "Iteration 16, inertia 1.1249441668062031e+20\n",
      "Iteration 17, inertia 1.123982965125438e+20\n",
      "Iteration 18, inertia 1.12279723316391e+20\n",
      "Iteration 19, inertia 1.122000296015756e+20\n",
      "Iteration 20, inertia 1.1208601665894747e+20\n",
      "Iteration 21, inertia 1.1198223021068242e+20\n",
      "Iteration 22, inertia 1.1178903474953498e+20\n",
      "Iteration 23, inertia 1.1157253381476473e+20\n",
      "Iteration 24, inertia 1.1135720790214409e+20\n",
      "Iteration 25, inertia 1.111831389632124e+20\n",
      "Iteration 26, inertia 1.111061061732965e+20\n",
      "Iteration 27, inertia 1.1102741867927426e+20\n",
      "Iteration 28, inertia 1.1090663457962469e+20\n",
      "Iteration 29, inertia 1.1087223309351697e+20\n",
      "Iteration 30, inertia 1.1083734316903524e+20\n",
      "Iteration 31, inertia 1.107416404159546e+20\n",
      "Iteration 32, inertia 1.1069271141480174e+20\n",
      "Iteration 33, inertia 1.106417909192474e+20\n",
      "Iteration 34, inertia 1.105523542117674e+20\n",
      "Iteration 35, inertia 1.1051300534733295e+20\n",
      "Iteration 36, inertia 1.104853373695822e+20\n",
      "Converged at iteration 36: strict convergence.\n",
      "Training a K-Means model with 15 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.2123122484988815e+20\n",
      "Iteration 1, inertia 9.874827270806166e+19\n",
      "Iteration 2, inertia 9.632413295344067e+19\n",
      "Iteration 3, inertia 9.551241054473745e+19\n",
      "Iteration 4, inertia 9.488498082070279e+19\n",
      "Iteration 5, inertia 9.465012900049694e+19\n",
      "Iteration 6, inertia 9.455289174476548e+19\n",
      "Iteration 7, inertia 9.454063534655727e+19\n",
      "Iteration 8, inertia 9.450932852839503e+19\n",
      "Iteration 9, inertia 9.444116626626055e+19\n",
      "Iteration 10, inertia 9.435541397614014e+19\n",
      "Iteration 11, inertia 9.434068324978131e+19\n",
      "Iteration 12, inertia 9.43243852978915e+19\n",
      "Converged at iteration 12: center shift 13256552724349.127 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.180843348498881e+20\n",
      "Iteration 1, inertia 1.030628145882613e+20\n",
      "Iteration 2, inertia 1.0111483468586772e+20\n",
      "Iteration 3, inertia 9.977079015592523e+19\n",
      "Iteration 4, inertia 9.896773278574903e+19\n",
      "Iteration 5, inertia 9.842230162572876e+19\n",
      "Iteration 6, inertia 9.805413822109922e+19\n",
      "Iteration 7, inertia 9.776816676181929e+19\n",
      "Iteration 8, inertia 9.755898114906507e+19\n",
      "Iteration 9, inertia 9.721099267265531e+19\n",
      "Iteration 10, inertia 9.670766440950507e+19\n",
      "Iteration 11, inertia 9.627596362364453e+19\n",
      "Iteration 12, inertia 9.593258081401373e+19\n",
      "Iteration 13, inertia 9.5682333210503e+19\n",
      "Iteration 14, inertia 9.549403531372133e+19\n",
      "Iteration 15, inertia 9.520938727527688e+19\n",
      "Iteration 16, inertia 9.504629102848896e+19\n",
      "Iteration 17, inertia 9.497152137121096e+19\n",
      "Iteration 18, inertia 9.492354200515712e+19\n",
      "Iteration 19, inertia 9.490438095511057e+19\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.174514148498882e+20\n",
      "Iteration 1, inertia 1.009712033274564e+20\n",
      "Iteration 2, inertia 9.812016347193301e+19\n",
      "Iteration 3, inertia 9.667381902148025e+19\n",
      "Iteration 4, inertia 9.566452873271892e+19\n",
      "Iteration 5, inertia 9.490300124656409e+19\n",
      "Iteration 6, inertia 9.448112674915805e+19\n",
      "Iteration 7, inertia 9.422928702333691e+19\n",
      "Iteration 8, inertia 9.413078172693861e+19\n",
      "Iteration 9, inertia 9.403403307543282e+19\n",
      "Iteration 10, inertia 9.392217910004531e+19\n",
      "Iteration 11, inertia 9.382090783059485e+19\n",
      "Iteration 12, inertia 9.367064214904609e+19\n",
      "Iteration 13, inertia 9.356599295057578e+19\n",
      "Iteration 14, inertia 9.343875077712103e+19\n",
      "Iteration 15, inertia 9.32802822638314e+19\n",
      "Iteration 16, inertia 9.315521356945885e+19\n",
      "Iteration 17, inertia 9.284376234126257e+19\n",
      "Iteration 18, inertia 9.261112959851612e+19\n",
      "Iteration 19, inertia 9.236592994902172e+19\n",
      "Iteration 20, inertia 9.22619883481266e+19\n",
      "Iteration 21, inertia 9.224947707484593e+19\n",
      "Iteration 22, inertia 9.221284091511464e+19\n",
      "Iteration 23, inertia 9.216860622054462e+19\n",
      "Iteration 24, inertia 9.214449440390971e+19\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1649131884988835e+20\n",
      "Iteration 1, inertia 1.0218929507752557e+20\n",
      "Iteration 2, inertia 9.743528787097253e+19\n",
      "Iteration 3, inertia 9.549324820231356e+19\n",
      "Iteration 4, inertia 9.484814753281209e+19\n",
      "Iteration 5, inertia 9.445238877142422e+19\n",
      "Iteration 6, inertia 9.42002954342262e+19\n",
      "Iteration 7, inertia 9.379225374000582e+19\n",
      "Iteration 8, inertia 9.338013314588289e+19\n",
      "Iteration 9, inertia 9.279517441611599e+19\n",
      "Iteration 10, inertia 9.25647901413359e+19\n",
      "Iteration 11, inertia 9.239463232480143e+19\n",
      "Iteration 12, inertia 9.22691994861616e+19\n",
      "Iteration 13, inertia 9.217158955147436e+19\n",
      "Iteration 14, inertia 9.21185333126028e+19\n",
      "Iteration 15, inertia 9.208257990510754e+19\n",
      "Iteration 16, inertia 9.20713827165138e+19\n",
      "Iteration 17, inertia 9.205720711728033e+19\n",
      "Iteration 18, inertia 9.203940379790968e+19\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1029806084988802e+20\n",
      "Iteration 1, inertia 9.69752793804854e+19\n",
      "Iteration 2, inertia 9.429108992941905e+19\n",
      "Iteration 3, inertia 9.307800794096081e+19\n",
      "Iteration 4, inertia 9.246207937671584e+19\n",
      "Iteration 5, inertia 9.214261492927788e+19\n",
      "Iteration 6, inertia 9.199054568974883e+19\n",
      "Iteration 7, inertia 9.194714609665073e+19\n",
      "Iteration 8, inertia 9.190942637687063e+19\n",
      "Iteration 9, inertia 9.190264761261939e+19\n",
      "Iteration 10, inertia 9.188415485039431e+19\n",
      "Iteration 11, inertia 9.188183158883688e+19\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.0580204084988797e+20\n",
      "Iteration 1, inertia 9.869029220849472e+19\n",
      "Iteration 2, inertia 9.730287961334363e+19\n",
      "Iteration 3, inertia 9.647150443168953e+19\n",
      "Iteration 4, inertia 9.57466288180316e+19\n",
      "Iteration 5, inertia 9.522803418384805e+19\n",
      "Iteration 6, inertia 9.481853087812895e+19\n",
      "Iteration 7, inertia 9.444240467061463e+19\n",
      "Iteration 8, inertia 9.391012642261205e+19\n",
      "Iteration 9, inertia 9.349553946102777e+19\n",
      "Iteration 10, inertia 9.325283961454156e+19\n",
      "Iteration 11, inertia 9.30586492912676e+19\n",
      "Iteration 12, inertia 9.29382948470687e+19\n",
      "Iteration 13, inertia 9.280887355000278e+19\n",
      "Iteration 14, inertia 9.269074315453802e+19\n",
      "Iteration 15, inertia 9.267601242817918e+19\n",
      "Iteration 16, inertia 9.265971447628934e+19\n",
      "Converged at iteration 16: center shift 13256552724349.104 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1118500584988824e+20\n",
      "Iteration 1, inertia 9.727377531088729e+19\n",
      "Iteration 2, inertia 9.448184455975477e+19\n",
      "Iteration 3, inertia 9.35350384625183e+19\n",
      "Iteration 4, inertia 9.283516169418688e+19\n",
      "Iteration 5, inertia 9.23152026480297e+19\n",
      "Iteration 6, inertia 9.191143269202895e+19\n",
      "Iteration 7, inertia 9.171255096961384e+19\n",
      "Iteration 8, inertia 9.155090428484398e+19\n",
      "Iteration 9, inertia 9.150768319336084e+19\n",
      "Iteration 10, inertia 9.14466194659552e+19\n",
      "Iteration 11, inertia 9.142481051176536e+19\n",
      "Iteration 12, inertia 9.140229585187815e+19\n",
      "Iteration 13, inertia 9.133767831559299e+19\n",
      "Iteration 14, inertia 9.125899024191937e+19\n",
      "Iteration 15, inertia 9.119194365280028e+19\n",
      "Iteration 16, inertia 9.107316372564286e+19\n",
      "Iteration 17, inertia 9.091670709689781e+19\n",
      "Iteration 18, inertia 9.087201150658504e+19\n",
      "Iteration 19, inertia 9.083285981927693e+19\n",
      "Iteration 20, inertia 9.077948595978286e+19\n",
      "Iteration 21, inertia 9.065264067505039e+19\n",
      "Iteration 22, inertia 9.058208894606806e+19\n",
      "Iteration 23, inertia 9.05525360327898e+19\n",
      "Converged at iteration 23: center shift 3469729655939.252 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1831579384988816e+20\n",
      "Iteration 1, inertia 1.0067774010415283e+20\n",
      "Iteration 2, inertia 9.728986758597275e+19\n",
      "Iteration 3, inertia 9.566471717073828e+19\n",
      "Iteration 4, inertia 9.471055834810812e+19\n",
      "Iteration 5, inertia 9.40558968966108e+19\n",
      "Iteration 6, inertia 9.359702264868189e+19\n",
      "Iteration 7, inertia 9.330070042809092e+19\n",
      "Iteration 8, inertia 9.322015697260886e+19\n",
      "Iteration 9, inertia 9.31437025638974e+19\n",
      "Iteration 10, inertia 9.299820156643207e+19\n",
      "Iteration 11, inertia 9.285492931237824e+19\n",
      "Iteration 12, inertia 9.277578096761415e+19\n",
      "Iteration 13, inertia 9.275154198899814e+19\n",
      "Iteration 14, inertia 9.271764260256529e+19\n",
      "Iteration 15, inertia 9.27048236838934e+19\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.3725498084988738e+20\n",
      "Iteration 1, inertia 9.710074839383397e+19\n",
      "Iteration 2, inertia 9.272966809898446e+19\n",
      "Iteration 3, inertia 9.165479592667439e+19\n",
      "Iteration 4, inertia 9.123275693224854e+19\n",
      "Iteration 5, inertia 9.107396682506574e+19\n",
      "Iteration 6, inertia 9.092753887688534e+19\n",
      "Iteration 7, inertia 9.07965827260264e+19\n",
      "Converged at iteration 7: center shift 3206716252294.293 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.2590234084988802e+20\n",
      "Iteration 1, inertia 9.635391859516483e+19\n",
      "Iteration 2, inertia 9.362520438323721e+19\n",
      "Iteration 3, inertia 9.284538491980182e+19\n",
      "Iteration 4, inertia 9.266306658668089e+19\n",
      "Iteration 5, inertia 9.260415103759409e+19\n",
      "Iteration 6, inertia 9.257485814765676e+19\n",
      "Iteration 7, inertia 9.255065002906973e+19\n",
      "Iteration 8, inertia 9.252543723032502e+19\n",
      "Converged at iteration 8: strict convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 16 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.252392484988813e+19\n",
      "Iteration 1, inertia 8.171752947066659e+19\n",
      "Iteration 2, inertia 8.0410635323442e+19\n",
      "Iteration 3, inertia 8.01622214205213e+19\n",
      "Iteration 4, inertia 7.991521674430644e+19\n",
      "Iteration 5, inertia 7.976707643201801e+19\n",
      "Iteration 6, inertia 7.974441281964181e+19\n",
      "Iteration 7, inertia 7.97052611323337e+19\n",
      "Iteration 8, inertia 7.967181112982779e+19\n",
      "Iteration 9, inertia 7.958728576943155e+19\n",
      "Iteration 10, inertia 7.951673404044924e+19\n",
      "Iteration 11, inertia 7.94882700429918e+19\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.0669922445453964e+20\n",
      "Iteration 1, inertia 8.822525510955939e+19\n",
      "Iteration 2, inertia 8.555935009983403e+19\n",
      "Iteration 3, inertia 8.430727836931396e+19\n",
      "Iteration 4, inertia 8.373732509024313e+19\n",
      "Iteration 5, inertia 8.357459748973946e+19\n",
      "Iteration 6, inertia 8.34467462718095e+19\n",
      "Iteration 7, inertia 8.33040769594456e+19\n",
      "Iteration 8, inertia 8.314817354491332e+19\n",
      "Iteration 9, inertia 8.292456415921662e+19\n",
      "Iteration 10, inertia 8.279984544974384e+19\n",
      "Iteration 11, inertia 8.261511253226373e+19\n",
      "Iteration 12, inertia 8.243603169292788e+19\n",
      "Iteration 13, inertia 8.225827836409641e+19\n",
      "Iteration 14, inertia 8.216644961432458e+19\n",
      "Iteration 15, inertia 8.202535011275856e+19\n",
      "Iteration 16, inertia 8.18261155863602e+19\n",
      "Iteration 17, inertia 8.154588931718875e+19\n",
      "Iteration 18, inertia 8.137658510678026e+19\n",
      "Iteration 19, inertia 8.122374979165392e+19\n",
      "Iteration 20, inertia 8.108854436980983e+19\n",
      "Iteration 21, inertia 8.105080522254867e+19\n",
      "Iteration 22, inertia 8.077635996414516e+19\n",
      "Iteration 23, inertia 8.068036009463559e+19\n",
      "Iteration 24, inertia 8.058457647134668e+19\n",
      "Iteration 25, inertia 8.04760042918414e+19\n",
      "Iteration 26, inertia 8.040829204522934e+19\n",
      "Iteration 27, inertia 8.035219207592532e+19\n",
      "Converged at iteration 27: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.741660584988821e+19\n",
      "Iteration 1, inertia 8.605315457740333e+19\n",
      "Iteration 2, inertia 8.469616404206577e+19\n",
      "Iteration 3, inertia 8.405814505912748e+19\n",
      "Iteration 4, inertia 8.349248598014388e+19\n",
      "Iteration 5, inertia 8.305871960510978e+19\n",
      "Iteration 6, inertia 8.29378208085196e+19\n",
      "Iteration 7, inertia 8.287204919708923e+19\n",
      "Iteration 8, inertia 8.282936265081674e+19\n",
      "Iteration 9, inertia 8.281623551028055e+19\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.0690493384988536e+20\n",
      "Iteration 1, inertia 8.501777162199831e+19\n",
      "Iteration 2, inertia 8.25700623848458e+19\n",
      "Iteration 3, inertia 8.216465451832762e+19\n",
      "Iteration 4, inertia 8.195079853112938e+19\n",
      "Iteration 5, inertia 8.161470276928197e+19\n",
      "Iteration 6, inertia 8.121943356978414e+19\n",
      "Iteration 7, inertia 8.078567290851656e+19\n",
      "Iteration 8, inertia 8.0621644044496e+19\n",
      "Iteration 9, inertia 8.054794963502042e+19\n",
      "Iteration 10, inertia 8.049160088547357e+19\n",
      "Iteration 11, inertia 8.04708149898784e+19\n",
      "Iteration 12, inertia 8.046489262273908e+19\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1165996062335479e+20\n",
      "Iteration 1, inertia 1.0276641493983879e+20\n",
      "Iteration 2, inertia 1.0052652258579354e+20\n",
      "Iteration 3, inertia 9.964661498023064e+19\n",
      "Iteration 4, inertia 9.914296638647729e+19\n",
      "Iteration 5, inertia 9.873711298883948e+19\n",
      "Iteration 6, inertia 9.83531956535791e+19\n",
      "Iteration 7, inertia 9.796581253010457e+19\n",
      "Iteration 8, inertia 9.734352251297704e+19\n",
      "Iteration 9, inertia 9.690781031681763e+19\n",
      "Iteration 10, inertia 9.651572308768619e+19\n",
      "Iteration 11, inertia 9.61837598104063e+19\n",
      "Iteration 12, inertia 9.591800876435402e+19\n",
      "Iteration 13, inertia 9.541613000337932e+19\n",
      "Iteration 14, inertia 9.520100948724495e+19\n",
      "Iteration 15, inertia 9.501690578400854e+19\n",
      "Iteration 16, inertia 9.475482852373003e+19\n",
      "Iteration 17, inertia 9.458016362631676e+19\n",
      "Iteration 18, inertia 9.420114764625756e+19\n",
      "Iteration 19, inertia 9.368059321116685e+19\n",
      "Iteration 20, inertia 9.317615194246768e+19\n",
      "Iteration 21, inertia 9.287304095659919e+19\n",
      "Iteration 22, inertia 9.2573094395541e+19\n",
      "Iteration 23, inertia 9.228836370085531e+19\n",
      "Iteration 24, inertia 9.183165208149069e+19\n",
      "Iteration 25, inertia 9.157955749097221e+19\n",
      "Iteration 26, inertia 9.135062648629379e+19\n",
      "Iteration 27, inertia 9.111107027526797e+19\n",
      "Iteration 28, inertia 9.089565260462375e+19\n",
      "Iteration 29, inertia 9.083947629858059e+19\n",
      "Iteration 30, inertia 9.070541158490956e+19\n",
      "Iteration 31, inertia 9.064706231622512e+19\n",
      "Iteration 32, inertia 9.060493821861128e+19\n",
      "Iteration 33, inertia 9.056961261521909e+19\n",
      "Converged at iteration 33: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.093172336883159e+20\n",
      "Iteration 1, inertia 8.981238359943953e+19\n",
      "Iteration 2, inertia 8.640775834210281e+19\n",
      "Iteration 3, inertia 8.450889121858671e+19\n",
      "Iteration 4, inertia 8.385965865026771e+19\n",
      "Iteration 5, inertia 8.348703726854177e+19\n",
      "Iteration 6, inertia 8.320786463895133e+19\n",
      "Iteration 7, inertia 8.294490914912458e+19\n",
      "Iteration 8, inertia 8.264105854950746e+19\n",
      "Iteration 9, inertia 8.239326570372982e+19\n",
      "Iteration 10, inertia 8.221092652902813e+19\n",
      "Iteration 11, inertia 8.205853609217316e+19\n",
      "Iteration 12, inertia 8.201840616597217e+19\n",
      "Iteration 13, inertia 8.200564038518142e+19\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1244570884988822e+20\n",
      "Iteration 1, inertia 8.530730819335835e+19\n",
      "Iteration 2, inertia 8.154287733452616e+19\n",
      "Iteration 3, inertia 8.053635072906815e+19\n",
      "Iteration 4, inertia 8.022961005664648e+19\n",
      "Iteration 5, inertia 7.996300584420386e+19\n",
      "Iteration 6, inertia 7.918568574916626e+19\n",
      "Iteration 7, inertia 7.907504477194224e+19\n",
      "Iteration 8, inertia 7.898686025736882e+19\n",
      "Iteration 9, inertia 7.881504766372833e+19\n",
      "Iteration 10, inertia 7.875338230888663e+19\n",
      "Iteration 11, inertia 7.870771739417094e+19\n",
      "Iteration 12, inertia 7.866233304510015e+19\n",
      "Iteration 13, inertia 7.865550855439788e+19\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.1935470684988834e+20\n",
      "Iteration 1, inertia 1.0119916697133189e+20\n",
      "Iteration 2, inertia 9.651330330141906e+19\n",
      "Iteration 3, inertia 9.409670290305155e+19\n",
      "Iteration 4, inertia 9.285896300063811e+19\n",
      "Iteration 5, inertia 9.199903205945373e+19\n",
      "Iteration 6, inertia 9.122983608404489e+19\n",
      "Iteration 7, inertia 9.027490142848473e+19\n",
      "Iteration 8, inertia 8.975151086854403e+19\n",
      "Iteration 9, inertia 8.937694379098227e+19\n",
      "Iteration 10, inertia 8.894537747738785e+19\n",
      "Iteration 11, inertia 8.87002479452212e+19\n",
      "Iteration 12, inertia 8.81650863469438e+19\n",
      "Iteration 13, inertia 8.766322417509404e+19\n",
      "Iteration 14, inertia 8.742534218612759e+19\n",
      "Iteration 15, inertia 8.725520413436418e+19\n",
      "Iteration 16, inertia 8.703572292637752e+19\n",
      "Iteration 17, inertia 8.68691303620944e+19\n",
      "Iteration 18, inertia 8.67755545425759e+19\n",
      "Iteration 19, inertia 8.676134828513994e+19\n",
      "Iteration 20, inertia 8.673608631671362e+19\n",
      "Iteration 21, inertia 8.67269010261979e+19\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.858149284988738e+19\n",
      "Iteration 1, inertia 8.411702811511002e+19\n",
      "Iteration 2, inertia 8.188738374995503e+19\n",
      "Iteration 3, inertia 8.063296785479587e+19\n",
      "Iteration 4, inertia 7.985317767746103e+19\n",
      "Iteration 5, inertia 7.96279054408698e+19\n",
      "Iteration 6, inertia 7.932736944571931e+19\n",
      "Iteration 7, inertia 7.917688758436335e+19\n",
      "Iteration 8, inertia 7.906415841308731e+19\n",
      "Iteration 9, inertia 7.899335532621985e+19\n",
      "Iteration 10, inertia 7.89582464441598e+19\n",
      "Converged at iteration 10: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.750214265387562e+19\n",
      "Iteration 1, inertia 9.071969598082436e+19\n",
      "Iteration 2, inertia 8.985956851002841e+19\n",
      "Iteration 3, inertia 8.931951828723717e+19\n",
      "Iteration 4, inertia 8.890273277614591e+19\n",
      "Iteration 5, inertia 8.87345533510998e+19\n",
      "Iteration 6, inertia 8.859775835216545e+19\n",
      "Iteration 7, inertia 8.8526645149533e+19\n",
      "Iteration 8, inertia 8.849750843225999e+19\n",
      "Converged at iteration 8: center shift 9236337723129.965 within tolerance 13652364605003.256.\n",
      "Training a K-Means model with 17 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.447582484988815e+19\n",
      "Iteration 1, inertia 7.397199342724191e+19\n",
      "Iteration 2, inertia 7.233039803838289e+19\n",
      "Iteration 3, inertia 7.161830812129223e+19\n",
      "Iteration 4, inertia 7.096781971825237e+19\n",
      "Iteration 5, inertia 7.0611692012115255e+19\n",
      "Iteration 6, inertia 7.032078185842725e+19\n",
      "Iteration 7, inertia 7.002886790568274e+19\n",
      "Iteration 8, inertia 6.985502927540038e+19\n",
      "Iteration 9, inertia 6.9625583737812886e+19\n",
      "Iteration 10, inertia 6.943217415871997e+19\n",
      "Iteration 11, inertia 6.92728447159238e+19\n",
      "Iteration 12, inertia 6.91568793304601e+19\n",
      "Iteration 13, inertia 6.8964933293313475e+19\n",
      "Iteration 14, inertia 6.8873905801610576e+19\n",
      "Iteration 15, inertia 6.87263320165762e+19\n",
      "Iteration 16, inertia 6.8625921362975875e+19\n",
      "Iteration 17, inertia 6.857854832504863e+19\n",
      "Iteration 18, inertia 6.856818047783837e+19\n",
      "Iteration 19, inertia 6.856169152342144e+19\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.242592984988818e+19\n",
      "Iteration 1, inertia 7.70100854190988e+19\n",
      "Iteration 2, inertia 7.375008007368658e+19\n",
      "Iteration 3, inertia 7.2254668994592465e+19\n",
      "Iteration 4, inertia 7.181035011909152e+19\n",
      "Iteration 5, inertia 7.147196192720067e+19\n",
      "Iteration 6, inertia 7.120634313318473e+19\n",
      "Iteration 7, inertia 7.110675333642349e+19\n",
      "Iteration 8, inertia 7.107320967856137e+19\n",
      "Iteration 9, inertia 7.101400373598359e+19\n",
      "Iteration 10, inertia 7.0983164635201585e+19\n",
      "Converged at iteration 10: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.924944084988805e+19\n",
      "Iteration 1, inertia 7.644555458610661e+19\n",
      "Iteration 2, inertia 7.315070194167629e+19\n",
      "Iteration 3, inertia 7.197153283875167e+19\n",
      "Iteration 4, inertia 7.139632143668037e+19\n",
      "Iteration 5, inertia 7.104492551144311e+19\n",
      "Iteration 6, inertia 7.060059909293487e+19\n",
      "Iteration 7, inertia 7.031123854842802e+19\n",
      "Iteration 8, inertia 7.0137092858698736e+19\n",
      "Iteration 9, inertia 6.996411188635974e+19\n",
      "Iteration 10, inertia 6.985007604234542e+19\n",
      "Iteration 11, inertia 6.974500198130659e+19\n",
      "Iteration 12, inertia 6.959462150352539e+19\n",
      "Iteration 13, inertia 6.91509731141678e+19\n",
      "Iteration 14, inertia 6.906005486187915e+19\n",
      "Iteration 15, inertia 6.903093175584989e+19\n",
      "Iteration 16, inertia 6.896395389649342e+19\n",
      "Iteration 17, inertia 6.895568115044289e+19\n",
      "Iteration 18, inertia 6.8953363051170365e+19\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.341434884988835e+19\n",
      "Iteration 1, inertia 7.960214197842289e+19\n",
      "Iteration 2, inertia 7.708486574138602e+19\n",
      "Iteration 3, inertia 7.590282206048458e+19\n",
      "Iteration 4, inertia 7.502532236671392e+19\n",
      "Iteration 5, inertia 7.440254912336504e+19\n",
      "Iteration 6, inertia 7.380573868473996e+19\n",
      "Iteration 7, inertia 7.351969710401167e+19\n",
      "Iteration 8, inertia 7.323806197709352e+19\n",
      "Iteration 9, inertia 7.297896376902974e+19\n",
      "Iteration 10, inertia 7.273608622050951e+19\n",
      "Iteration 11, inertia 7.261759829103085e+19\n",
      "Iteration 12, inertia 7.256688619554102e+19\n",
      "Iteration 13, inertia 7.254881946864197e+19\n",
      "Iteration 14, inertia 7.2528714498634105e+19\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1.0478416084988743e+20\n",
      "Iteration 1, inertia 8.684497095601884e+19\n",
      "Iteration 2, inertia 8.428540988523574e+19\n",
      "Iteration 3, inertia 8.306905662277822e+19\n",
      "Iteration 4, inertia 8.225091948510763e+19\n",
      "Iteration 5, inertia 8.156321407578012e+19\n",
      "Iteration 6, inertia 8.075807798908784e+19\n",
      "Iteration 7, inertia 7.998721259158482e+19\n",
      "Iteration 8, inertia 7.909536368678255e+19\n",
      "Iteration 9, inertia 7.85833148876747e+19\n",
      "Iteration 10, inertia 7.81189573259143e+19\n",
      "Iteration 11, inertia 7.777973882904786e+19\n",
      "Iteration 12, inertia 7.748569202315713e+19\n",
      "Iteration 13, inertia 7.712815223731582e+19\n",
      "Iteration 14, inertia 7.696010599381467e+19\n",
      "Iteration 15, inertia 7.6877563182907e+19\n",
      "Iteration 16, inertia 7.679806089120545e+19\n",
      "Iteration 17, inertia 7.678212530447483e+19\n",
      "Iteration 18, inertia 7.677448510998559e+19\n",
      "Iteration 19, inertia 7.676871350473795e+19\n",
      "Converged at iteration 19: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.389192484988743e+19\n",
      "Iteration 1, inertia 8.181584973212208e+19\n",
      "Iteration 2, inertia 7.940560496127957e+19\n",
      "Iteration 3, inertia 7.849766010536908e+19\n",
      "Iteration 4, inertia 7.811290427295118e+19\n",
      "Iteration 5, inertia 7.774830482382538e+19\n",
      "Iteration 6, inertia 7.747956519615432e+19\n",
      "Iteration 7, inertia 7.731961914468128e+19\n",
      "Iteration 8, inertia 7.725133479112638e+19\n",
      "Iteration 9, inertia 7.721593152942383e+19\n",
      "Iteration 10, inertia 7.714732038970714e+19\n",
      "Iteration 11, inertia 7.708453833240484e+19\n",
      "Iteration 12, inertia 7.694475596690509e+19\n",
      "Iteration 13, inertia 7.682006382934014e+19\n",
      "Iteration 14, inertia 7.66627966739308e+19\n",
      "Iteration 15, inertia 7.638626108284112e+19\n",
      "Iteration 16, inertia 7.607072720727518e+19\n",
      "Iteration 17, inertia 7.589003117129227e+19\n",
      "Iteration 18, inertia 7.574973433703172e+19\n",
      "Iteration 19, inertia 7.558824978455262e+19\n",
      "Iteration 20, inertia 7.545666173879519e+19\n",
      "Iteration 21, inertia 7.539080648531154e+19\n",
      "Iteration 22, inertia 7.53325238305615e+19\n",
      "Iteration 23, inertia 7.528968352733146e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, inertia 7.522176415603084e+19\n",
      "Iteration 25, inertia 7.517435175174187e+19\n",
      "Iteration 26, inertia 7.51426388656676e+19\n",
      "Iteration 27, inertia 7.511763960252247e+19\n",
      "Converged at iteration 27: center shift 3252179480086.23 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.318774888816101e+19\n",
      "Iteration 1, inertia 7.947396732284115e+19\n",
      "Iteration 2, inertia 7.566130091303323e+19\n",
      "Iteration 3, inertia 7.426941719325776e+19\n",
      "Iteration 4, inertia 7.354725905866251e+19\n",
      "Iteration 5, inertia 7.316256700354609e+19\n",
      "Iteration 6, inertia 7.304498683558092e+19\n",
      "Iteration 7, inertia 7.297832766840551e+19\n",
      "Iteration 8, inertia 7.288954741639642e+19\n",
      "Iteration 9, inertia 7.2822403661188645e+19\n",
      "Iteration 10, inertia 7.2799800127056945e+19\n",
      "Iteration 11, inertia 7.2752640861798105e+19\n",
      "Iteration 12, inertia 7.270136082923195e+19\n",
      "Converged at iteration 12: center shift 13492573861529.941 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.095996884988795e+19\n",
      "Iteration 1, inertia 7.848946972488393e+19\n",
      "Iteration 2, inertia 7.744990196257043e+19\n",
      "Iteration 3, inertia 7.713724587816968e+19\n",
      "Iteration 4, inertia 7.670665442000388e+19\n",
      "Iteration 5, inertia 7.625801846651429e+19\n",
      "Iteration 6, inertia 7.6128739665056e+19\n",
      "Iteration 7, inertia 7.608135349085179e+19\n",
      "Iteration 8, inertia 7.605732448231698e+19\n",
      "Iteration 9, inertia 7.603136150111434e+19\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.697802484988722e+19\n",
      "Iteration 1, inertia 7.699678491297241e+19\n",
      "Iteration 2, inertia 7.44672836404089e+19\n",
      "Iteration 3, inertia 7.348739923255371e+19\n",
      "Iteration 4, inertia 7.298040791263784e+19\n",
      "Iteration 5, inertia 7.249391601358129e+19\n",
      "Iteration 6, inertia 7.194779666401252e+19\n",
      "Iteration 7, inertia 7.1490813540902445e+19\n",
      "Iteration 8, inertia 7.109294585811801e+19\n",
      "Iteration 9, inertia 7.068187431481063e+19\n",
      "Iteration 10, inertia 7.0082588860119106e+19\n",
      "Iteration 11, inertia 6.973713439895809e+19\n",
      "Iteration 12, inertia 6.9626586149981135e+19\n",
      "Iteration 13, inertia 6.9554882522390995e+19\n",
      "Iteration 14, inertia 6.94552397073522e+19\n",
      "Iteration 15, inertia 6.94430868119033e+19\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.56836138498883e+19\n",
      "Iteration 1, inertia 7.543155214292704e+19\n",
      "Iteration 2, inertia 7.347808550568256e+19\n",
      "Iteration 3, inertia 7.247424424062517e+19\n",
      "Iteration 4, inertia 7.193623047104463e+19\n",
      "Iteration 5, inertia 7.147731121517133e+19\n",
      "Iteration 6, inertia 7.137516926345854e+19\n",
      "Iteration 7, inertia 7.124862498894593e+19\n",
      "Iteration 8, inertia 7.11630770267798e+19\n",
      "Iteration 9, inertia 7.0991206628841595e+19\n",
      "Iteration 10, inertia 7.0873496750901395e+19\n",
      "Iteration 11, inertia 7.079886986473559e+19\n",
      "Iteration 12, inertia 7.071128317899923e+19\n",
      "Iteration 13, inertia 7.069433997651141e+19\n",
      "Iteration 14, inertia 7.067247834637776e+19\n",
      "Converged at iteration 14: strict convergence.\n",
      "Training a K-Means model with 18 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.724962484988815e+19\n",
      "Iteration 1, inertia 6.9467505474630255e+19\n",
      "Iteration 2, inertia 6.837549616933542e+19\n",
      "Iteration 3, inertia 6.76936155208433e+19\n",
      "Iteration 4, inertia 6.70474672934743e+19\n",
      "Iteration 5, inertia 6.66692244117162e+19\n",
      "Iteration 6, inertia 6.63908759183169e+19\n",
      "Iteration 7, inertia 6.620617411651706e+19\n",
      "Iteration 8, inertia 6.613394247103493e+19\n",
      "Iteration 9, inertia 6.601195147590834e+19\n",
      "Iteration 10, inertia 6.587661590774856e+19\n",
      "Iteration 11, inertia 6.579741860509634e+19\n",
      "Iteration 12, inertia 6.5767611912320025e+19\n",
      "Iteration 13, inertia 6.5732761098642e+19\n",
      "Iteration 14, inertia 6.571238581761786e+19\n",
      "Iteration 15, inertia 6.561446147459219e+19\n",
      "Iteration 16, inertia 6.553012153768708e+19\n",
      "Iteration 17, inertia 6.5472193014052766e+19\n",
      "Iteration 18, inertia 6.5373655595849286e+19\n",
      "Iteration 19, inertia 6.5323061009244094e+19\n",
      "Iteration 20, inertia 6.5262046303072535e+19\n",
      "Iteration 21, inertia 6.522362176830565e+19\n",
      "Iteration 22, inertia 6.5216632061803086e+19\n",
      "Converged at iteration 22: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.65319858498868e+19\n",
      "Iteration 1, inertia 7.017511793796329e+19\n",
      "Iteration 2, inertia 6.902285948806168e+19\n",
      "Iteration 3, inertia 6.852936917647324e+19\n",
      "Iteration 4, inertia 6.80403793376937e+19\n",
      "Iteration 5, inertia 6.78192347396522e+19\n",
      "Iteration 6, inertia 6.771140126438465e+19\n",
      "Iteration 7, inertia 6.758602920216522e+19\n",
      "Iteration 8, inertia 6.757313457191582e+19\n",
      "Iteration 9, inertia 6.756586895241841e+19\n",
      "Converged at iteration 9: center shift 5820668431150.402 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.5977805849888e+19\n",
      "Iteration 1, inertia 7.333671260783192e+19\n",
      "Iteration 2, inertia 7.0835927915483685e+19\n",
      "Iteration 3, inertia 6.983347611526648e+19\n",
      "Iteration 4, inertia 6.9318449894949044e+19\n",
      "Iteration 5, inertia 6.891467595799718e+19\n",
      "Iteration 6, inertia 6.870408713418177e+19\n",
      "Iteration 7, inertia 6.856950156371729e+19\n",
      "Iteration 8, inertia 6.839070923790713e+19\n",
      "Iteration 9, inertia 6.808969116061176e+19\n",
      "Iteration 10, inertia 6.786057608807176e+19\n",
      "Iteration 11, inertia 6.74212134210321e+19\n",
      "Iteration 12, inertia 6.68655275276194e+19\n",
      "Iteration 13, inertia 6.640717333003106e+19\n",
      "Iteration 14, inertia 6.604457205632041e+19\n",
      "Iteration 15, inertia 6.580718685689846e+19\n",
      "Iteration 16, inertia 6.570642703655321e+19\n",
      "Iteration 17, inertia 6.558053998430578e+19\n",
      "Iteration 18, inertia 6.548117321102831e+19\n",
      "Iteration 19, inertia 6.529123467051559e+19\n",
      "Iteration 20, inertia 6.4635352008223646e+19\n",
      "Iteration 21, inertia 6.434851424594015e+19\n",
      "Iteration 22, inertia 6.4283685531345535e+19\n",
      "Iteration 23, inertia 6.42252081475529e+19\n",
      "Iteration 24, inertia 6.4169905657236955e+19\n",
      "Iteration 25, inertia 6.393405117538749e+19\n",
      "Iteration 26, inertia 6.385748740206511e+19\n",
      "Iteration 27, inertia 6.375274313629802e+19\n",
      "Iteration 28, inertia 6.366097233998167e+19\n",
      "Iteration 29, inertia 6.358483397743328e+19\n",
      "Iteration 30, inertia 6.353909261422938e+19\n",
      "Iteration 31, inertia 6.351871545715869e+19\n",
      "Converged at iteration 31: center shift 11406343900192.186 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.091638884988825e+19\n",
      "Iteration 1, inertia 7.210708186265852e+19\n",
      "Iteration 2, inertia 7.069619641318239e+19\n",
      "Iteration 3, inertia 6.976318718363609e+19\n",
      "Iteration 4, inertia 6.8985134601230066e+19\n",
      "Iteration 5, inertia 6.82974854489613e+19\n",
      "Iteration 6, inertia 6.784168710226781e+19\n",
      "Iteration 7, inertia 6.73904831647141e+19\n",
      "Iteration 8, inertia 6.6897663440325345e+19\n",
      "Iteration 9, inertia 6.653191086036964e+19\n",
      "Iteration 10, inertia 6.632364267893431e+19\n",
      "Iteration 11, inertia 6.613095670471749e+19\n",
      "Iteration 12, inertia 6.60268982986182e+19\n",
      "Iteration 13, inertia 6.5995751631635325e+19\n",
      "Iteration 14, inertia 6.596632805397325e+19\n",
      "Iteration 15, inertia 6.593992530255419e+19\n",
      "Iteration 16, inertia 6.588946614347375e+19\n",
      "Iteration 17, inertia 6.583532727470457e+19\n",
      "Iteration 18, inertia 6.574062690089116e+19\n",
      "Iteration 19, inertia 6.5620009313133355e+19\n",
      "Iteration 20, inertia 6.5482339813710455e+19\n",
      "Iteration 21, inertia 6.531900026280353e+19\n",
      "Iteration 22, inertia 6.510605875390208e+19\n",
      "Iteration 23, inertia 6.489816239377915e+19\n",
      "Iteration 24, inertia 6.46863548310098e+19\n",
      "Iteration 25, inertia 6.447015891757341e+19\n",
      "Iteration 26, inertia 6.423195937335713e+19\n",
      "Iteration 27, inertia 6.404742996096564e+19\n",
      "Iteration 28, inertia 6.395595579293208e+19\n",
      "Iteration 29, inertia 6.38707108204884e+19\n",
      "Iteration 30, inertia 6.371271510229498e+19\n",
      "Iteration 31, inertia 6.351655053652684e+19\n",
      "Iteration 32, inertia 6.339101691966562e+19\n",
      "Iteration 33, inertia 6.330487580250319e+19\n",
      "Iteration 34, inertia 6.3167829137728e+19\n",
      "Iteration 35, inertia 6.30217536658133e+19\n",
      "Iteration 36, inertia 6.288960884649033e+19\n",
      "Iteration 37, inertia 6.28358283050311e+19\n",
      "Iteration 38, inertia 6.2774864909360955e+19\n",
      "Iteration 39, inertia 6.267771407233167e+19\n",
      "Iteration 40, inertia 6.257138553648149e+19\n",
      "Iteration 41, inertia 6.247131561578215e+19\n",
      "Iteration 42, inertia 6.244117430198271e+19\n",
      "Iteration 43, inertia 6.240345458220261e+19\n",
      "Iteration 44, inertia 6.239667581795139e+19\n",
      "Iteration 45, inertia 6.237413750225563e+19\n",
      "Iteration 46, inertia 6.235225414927917e+19\n",
      "Iteration 47, inertia 6.232766144698147e+19\n",
      "Converged at iteration 47: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.555974584988837e+19\n",
      "Iteration 1, inertia 6.811433892232325e+19\n",
      "Iteration 2, inertia 6.519934765820875e+19\n",
      "Iteration 3, inertia 6.44877207188592e+19\n",
      "Iteration 4, inertia 6.34113525050416e+19\n",
      "Iteration 5, inertia 6.30951755365127e+19\n",
      "Iteration 6, inertia 6.290436141092631e+19\n",
      "Iteration 7, inertia 6.283040183083412e+19\n",
      "Iteration 8, inertia 6.278558734590745e+19\n",
      "Iteration 9, inertia 6.273649085076277e+19\n",
      "Iteration 10, inertia 6.272029832628373e+19\n",
      "Converged at iteration 10: center shift 6035143521494.215 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.198028884988802e+19\n",
      "Iteration 1, inertia 6.454025577256412e+19\n",
      "Iteration 2, inertia 6.335073179520311e+19\n",
      "Iteration 3, inertia 6.284852971572042e+19\n",
      "Iteration 4, inertia 6.249761685674082e+19\n",
      "Iteration 5, inertia 6.225911887186084e+19\n",
      "Iteration 6, inertia 6.210373789894387e+19\n",
      "Iteration 7, inertia 6.2049221541222285e+19\n",
      "Iteration 8, inertia 6.2039203325126566e+19\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, inertia 7.573566484988807e+19\n",
      "Iteration 1, inertia 6.674163704595102e+19\n",
      "Iteration 2, inertia 6.549242502658176e+19\n",
      "Iteration 3, inertia 6.4898789998104674e+19\n",
      "Iteration 4, inertia 6.456753776921494e+19\n",
      "Iteration 5, inertia 6.438893566477195e+19\n",
      "Iteration 6, inertia 6.432621786601026e+19\n",
      "Iteration 7, inertia 6.429444069862195e+19\n",
      "Iteration 8, inertia 6.42485287740519e+19\n",
      "Iteration 9, inertia 6.4227316422056714e+19\n",
      "Iteration 10, inertia 6.4219252788914864e+19\n",
      "Iteration 11, inertia 6.42136160468412e+19\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 9.115129784988837e+19\n",
      "Iteration 1, inertia 7.638143185554142e+19\n",
      "Iteration 2, inertia 7.269430801703928e+19\n",
      "Iteration 3, inertia 7.039738002596145e+19\n",
      "Iteration 4, inertia 6.870875781062724e+19\n",
      "Iteration 5, inertia 6.791492608413702e+19\n",
      "Iteration 6, inertia 6.76592992254772e+19\n",
      "Iteration 7, inertia 6.739164094651311e+19\n",
      "Iteration 8, inertia 6.679249837815846e+19\n",
      "Iteration 9, inertia 6.644761527182008e+19\n",
      "Iteration 10, inertia 6.633056665387397e+19\n",
      "Iteration 11, inertia 6.625219392632996e+19\n",
      "Iteration 12, inertia 6.608660592852742e+19\n",
      "Iteration 13, inertia 6.602494057368577e+19\n",
      "Iteration 14, inertia 6.598828481419393e+19\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.484936584988815e+19\n",
      "Iteration 1, inertia 6.985443762813102e+19\n",
      "Iteration 2, inertia 6.7789881780870414e+19\n",
      "Iteration 3, inertia 6.659239078613585e+19\n",
      "Iteration 4, inertia 6.6036029551339635e+19\n",
      "Iteration 5, inertia 6.581055328941828e+19\n",
      "Iteration 6, inertia 6.5775292483057074e+19\n",
      "Iteration 7, inertia 6.574144159137453e+19\n",
      "Iteration 8, inertia 6.573776414244445e+19\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.756148484988812e+19\n",
      "Iteration 1, inertia 7.33790586712658e+19\n",
      "Iteration 2, inertia 7.17940454245474e+19\n",
      "Iteration 3, inertia 7.127018580550232e+19\n",
      "Iteration 4, inertia 7.105379350978328e+19\n",
      "Iteration 5, inertia 7.095514813296719e+19\n",
      "Iteration 6, inertia 7.0879562926892835e+19\n",
      "Iteration 7, inertia 7.08415120544359e+19\n",
      "Iteration 8, inertia 7.079193180603646e+19\n",
      "Iteration 9, inertia 7.071432055694622e+19\n",
      "Iteration 10, inertia 7.060690744139797e+19\n",
      "Iteration 11, inertia 7.054438143526328e+19\n",
      "Iteration 12, inertia 7.053023318446074e+19\n",
      "Converged at iteration 12: center shift 7111721448984.815 within tolerance 13652364605003.256.\n",
      "Training a K-Means model with 19 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.213492484988814e+19\n",
      "Iteration 1, inertia 6.476923786484885e+19\n",
      "Iteration 2, inertia 6.333708950649146e+19\n",
      "Iteration 3, inertia 6.2331767002749e+19\n",
      "Iteration 4, inertia 6.139558918293526e+19\n",
      "Iteration 5, inertia 6.071417175115536e+19\n",
      "Iteration 6, inertia 6.02136115797812e+19\n",
      "Iteration 7, inertia 5.9907366698917446e+19\n",
      "Iteration 8, inertia 5.97006133326477e+19\n",
      "Iteration 9, inertia 5.953958124317154e+19\n",
      "Iteration 10, inertia 5.934208602813637e+19\n",
      "Iteration 11, inertia 5.928050409298793e+19\n",
      "Iteration 12, inertia 5.926152975899475e+19\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.662889884988457e+19\n",
      "Iteration 1, inertia 7.082527582686435e+19\n",
      "Iteration 2, inertia 6.834017772215147e+19\n",
      "Iteration 3, inertia 6.713261990796629e+19\n",
      "Iteration 4, inertia 6.6177036261453914e+19\n",
      "Iteration 5, inertia 6.5322168600499765e+19\n",
      "Iteration 6, inertia 6.453237685532431e+19\n",
      "Iteration 7, inertia 6.392398247963042e+19\n",
      "Iteration 8, inertia 6.345033144688083e+19\n",
      "Iteration 9, inertia 6.313902337356813e+19\n",
      "Iteration 10, inertia 6.271627041524347e+19\n",
      "Iteration 11, inertia 6.228895930795387e+19\n",
      "Iteration 12, inertia 6.1873820790381355e+19\n",
      "Iteration 13, inertia 6.1440167749193155e+19\n",
      "Iteration 14, inertia 6.111969961378901e+19\n",
      "Iteration 15, inertia 6.092082239449833e+19\n",
      "Iteration 16, inertia 6.065605583449999e+19\n",
      "Iteration 17, inertia 6.045851587425776e+19\n",
      "Iteration 18, inertia 6.031508330105449e+19\n",
      "Iteration 19, inertia 6.022803740218422e+19\n",
      "Iteration 20, inertia 6.013383891821478e+19\n",
      "Iteration 21, inertia 6.0009011811169804e+19\n",
      "Iteration 22, inertia 5.992198561888339e+19\n",
      "Iteration 23, inertia 5.987813540519758e+19\n",
      "Iteration 24, inertia 5.985195602646231e+19\n",
      "Converged at iteration 24: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.054973384988473e+19\n",
      "Iteration 1, inertia 5.927934795075155e+19\n",
      "Iteration 2, inertia 5.79980188152974e+19\n",
      "Iteration 3, inertia 5.692576810395618e+19\n",
      "Iteration 4, inertia 5.659074010576594e+19\n",
      "Iteration 5, inertia 5.646016247668365e+19\n",
      "Iteration 6, inertia 5.631717257935597e+19\n",
      "Iteration 7, inertia 5.62958642252482e+19\n",
      "Iteration 8, inertia 5.6281299762077024e+19\n",
      "Iteration 9, inertia 5.625423283988621e+19\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.6192665849888195e+19\n",
      "Iteration 1, inertia 5.948557992292403e+19\n",
      "Iteration 2, inertia 5.802495055877638e+19\n",
      "Iteration 3, inertia 5.745398779896424e+19\n",
      "Iteration 4, inertia 5.729676194603952e+19\n",
      "Iteration 5, inertia 5.719804384218179e+19\n",
      "Iteration 6, inertia 5.708114541409406e+19\n",
      "Iteration 7, inertia 5.6970949371988435e+19\n",
      "Iteration 8, inertia 5.684868999161929e+19\n",
      "Iteration 9, inertia 5.680335865415015e+19\n",
      "Iteration 10, inertia 5.67352522375515e+19\n",
      "Iteration 11, inertia 5.668360615442055e+19\n",
      "Iteration 12, inertia 5.662508966597348e+19\n",
      "Iteration 13, inertia 5.662009163066416e+19\n",
      "Converged at iteration 13: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.8820763849887154e+19\n",
      "Iteration 1, inertia 5.765394852611377e+19\n",
      "Iteration 2, inertia 5.659108773023913e+19\n",
      "Iteration 3, inertia 5.6300021119850955e+19\n",
      "Iteration 4, inertia 5.610408866512033e+19\n",
      "Iteration 5, inertia 5.600470319692334e+19\n",
      "Iteration 6, inertia 5.5924989271640056e+19\n",
      "Iteration 7, inertia 5.590203052977657e+19\n",
      "Iteration 8, inertia 5.589972987113051e+19\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 8.192080984988818e+19\n",
      "Iteration 1, inertia 6.710546963178788e+19\n",
      "Iteration 2, inertia 6.508460656959113e+19\n",
      "Iteration 3, inertia 6.428999573121646e+19\n",
      "Iteration 4, inertia 6.376267542113188e+19\n",
      "Iteration 5, inertia 6.330301519050247e+19\n",
      "Iteration 6, inertia 6.296398743935677e+19\n",
      "Iteration 7, inertia 6.278452903492772e+19\n",
      "Iteration 8, inertia 6.265513739262279e+19\n",
      "Iteration 9, inertia 6.254663760665176e+19\n",
      "Iteration 10, inertia 6.241135901759348e+19\n",
      "Iteration 11, inertia 6.232522810710016e+19\n",
      "Iteration 12, inertia 6.222908070091961e+19\n",
      "Iteration 13, inertia 6.2156135439170535e+19\n",
      "Iteration 14, inertia 6.2075003481114485e+19\n",
      "Iteration 15, inertia 6.200733192840286e+19\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.626234984988715e+19\n",
      "Iteration 1, inertia 6.605649058999295e+19\n",
      "Iteration 2, inertia 6.366193688889283e+19\n",
      "Iteration 3, inertia 6.264395219608207e+19\n",
      "Iteration 4, inertia 6.179631964778601e+19\n",
      "Iteration 5, inertia 6.090127097466373e+19\n",
      "Iteration 6, inertia 6.03539685817682e+19\n",
      "Iteration 7, inertia 6.001352961754833e+19\n",
      "Iteration 8, inertia 5.969887013110566e+19\n",
      "Iteration 9, inertia 5.958818587285492e+19\n",
      "Iteration 10, inertia 5.9196666910729585e+19\n",
      "Iteration 11, inertia 5.875174557616867e+19\n",
      "Iteration 12, inertia 5.841918490221955e+19\n",
      "Iteration 13, inertia 5.833703631507277e+19\n",
      "Iteration 14, inertia 5.831973799683218e+19\n",
      "Iteration 15, inertia 5.829875747957388e+19\n",
      "Iteration 16, inertia 5.82950800306438e+19\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.986545384988837e+19\n",
      "Iteration 1, inertia 6.386764911545449e+19\n",
      "Iteration 2, inertia 6.154765066400894e+19\n",
      "Iteration 3, inertia 6.0436379592159986e+19\n",
      "Iteration 4, inertia 5.982447684977921e+19\n",
      "Iteration 5, inertia 5.9168819850730594e+19\n",
      "Iteration 6, inertia 5.871983452880219e+19\n",
      "Iteration 7, inertia 5.8157991817140265e+19\n",
      "Iteration 8, inertia 5.774972434817551e+19\n",
      "Iteration 9, inertia 5.750590711677403e+19\n",
      "Iteration 10, inertia 5.7286023553771725e+19\n",
      "Iteration 11, inertia 5.7245363711798804e+19\n",
      "Iteration 12, inertia 5.722250565367898e+19\n",
      "Iteration 13, inertia 5.7200328935779795e+19\n",
      "Iteration 14, inertia 5.716933246973444e+19\n",
      "Iteration 15, inertia 5.716602661308803e+19\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.376414084988825e+19\n",
      "Iteration 1, inertia 6.1925258944816275e+19\n",
      "Iteration 2, inertia 5.899899151365211e+19\n",
      "Iteration 3, inertia 5.757455898539914e+19\n",
      "Iteration 4, inertia 5.685811046562981e+19\n",
      "Iteration 5, inertia 5.6647655938682044e+19\n",
      "Iteration 6, inertia 5.658153174812066e+19\n",
      "Iteration 7, inertia 5.64178162303112e+19\n",
      "Iteration 8, inertia 5.594914049453366e+19\n",
      "Iteration 9, inertia 5.57533914313926e+19\n",
      "Iteration 10, inertia 5.5712145134338425e+19\n",
      "Converged at iteration 10: center shift 8535750952406.238 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.779705684988831e+19\n",
      "Iteration 1, inertia 6.469235771993588e+19\n",
      "Iteration 2, inertia 6.261348392644525e+19\n",
      "Iteration 3, inertia 6.127790458913484e+19\n",
      "Iteration 4, inertia 6.0356548894003724e+19\n",
      "Iteration 5, inertia 5.972605442908846e+19\n",
      "Iteration 6, inertia 5.937190758321645e+19\n",
      "Iteration 7, inertia 5.916649495208343e+19\n",
      "Iteration 8, inertia 5.9083017243244765e+19\n",
      "Iteration 9, inertia 5.90183869171522e+19\n",
      "Iteration 10, inertia 5.894770722371255e+19\n",
      "Iteration 11, inertia 5.880073475604598e+19\n",
      "Iteration 12, inertia 5.878881027039968e+19\n",
      "Converged at iteration 12: center shift 3404974596194.1733 within tolerance 13652364605003.256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Means model with 20 clusters! \n",
      "\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.511332484988814e+19\n",
      "Iteration 1, inertia 5.76173479765299e+19\n",
      "Iteration 2, inertia 5.694347088213772e+19\n",
      "Iteration 3, inertia 5.649677020050973e+19\n",
      "Iteration 4, inertia 5.596719893723694e+19\n",
      "Iteration 5, inertia 5.568878202280713e+19\n",
      "Iteration 6, inertia 5.545266907865361e+19\n",
      "Iteration 7, inertia 5.531864443618702e+19\n",
      "Iteration 8, inertia 5.521192812001451e+19\n",
      "Iteration 9, inertia 5.5109394044674e+19\n",
      "Iteration 10, inertia 5.496994238640922e+19\n",
      "Iteration 11, inertia 5.48961084168925e+19\n",
      "Iteration 12, inertia 5.482156572100428e+19\n",
      "Iteration 13, inertia 5.475747961317307e+19\n",
      "Iteration 14, inertia 5.473319509461055e+19\n",
      "Iteration 15, inertia 5.471185329947044e+19\n",
      "Iteration 16, inertia 5.466561917134152e+19\n",
      "Iteration 17, inertia 5.462708744697761e+19\n",
      "Iteration 18, inertia 5.457891780163765e+19\n",
      "Iteration 19, inertia 5.45313800275065e+19\n",
      "Iteration 20, inertia 5.451775074411658e+19\n",
      "Converged at iteration 20: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.23735938498879e+19\n",
      "Iteration 1, inertia 5.879341414136997e+19\n",
      "Iteration 2, inertia 5.605435271790141e+19\n",
      "Iteration 3, inertia 5.456025247579544e+19\n",
      "Iteration 4, inertia 5.3807594995163914e+19\n",
      "Iteration 5, inertia 5.3499418918979625e+19\n",
      "Iteration 6, inertia 5.316935090701214e+19\n",
      "Iteration 7, inertia 5.297527700051905e+19\n",
      "Iteration 8, inertia 5.290145383068897e+19\n",
      "Iteration 9, inertia 5.284845215669807e+19\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.589401061198166e+19\n",
      "Iteration 1, inertia 5.599032017650625e+19\n",
      "Iteration 2, inertia 5.3514464839782646e+19\n",
      "Iteration 3, inertia 5.242523606498198e+19\n",
      "Iteration 4, inertia 5.159966230034887e+19\n",
      "Iteration 5, inertia 5.118102990027935e+19\n",
      "Iteration 6, inertia 5.099051144613457e+19\n",
      "Iteration 7, inertia 5.094131091818725e+19\n",
      "Iteration 8, inertia 5.08448046802315e+19\n",
      "Iteration 9, inertia 5.080682685939598e+19\n",
      "Iteration 10, inertia 5.076408729339654e+19\n",
      "Iteration 11, inertia 5.075042273629381e+19\n",
      "Converged at iteration 11: center shift 8557324206268.949 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.5175693849887965e+19\n",
      "Iteration 1, inertia 5.337517325902474e+19\n",
      "Iteration 2, inertia 5.19916187995009e+19\n",
      "Iteration 3, inertia 5.150556384072201e+19\n",
      "Iteration 4, inertia 5.126833029673452e+19\n",
      "Iteration 5, inertia 5.099396479126453e+19\n",
      "Iteration 6, inertia 5.086218789266643e+19\n",
      "Iteration 7, inertia 5.083009576772099e+19\n",
      "Iteration 8, inertia 5.08050918341226e+19\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.3693327895957225e+19\n",
      "Iteration 1, inertia 6.006002413107552e+19\n",
      "Iteration 2, inertia 5.883725665930877e+19\n",
      "Iteration 3, inertia 5.827341685176106e+19\n",
      "Iteration 4, inertia 5.808123278870315e+19\n",
      "Iteration 5, inertia 5.777350237118742e+19\n",
      "Iteration 6, inertia 5.752081128532345e+19\n",
      "Iteration 7, inertia 5.7359671753116746e+19\n",
      "Iteration 8, inertia 5.725069219707446e+19\n",
      "Iteration 9, inertia 5.708175353092641e+19\n",
      "Iteration 10, inertia 5.699285264701959e+19\n",
      "Iteration 11, inertia 5.686052205167839e+19\n",
      "Iteration 12, inertia 5.675635869450833e+19\n",
      "Iteration 13, inertia 5.662573695519315e+19\n",
      "Iteration 14, inertia 5.659174652988157e+19\n",
      "Iteration 15, inertia 5.6509205168596845e+19\n",
      "Iteration 16, inertia 5.643576693694471e+19\n",
      "Iteration 17, inertia 5.6419131819172176e+19\n",
      "Iteration 18, inertia 5.641049106084649e+19\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.018410084988832e+19\n",
      "Iteration 1, inertia 5.383796930032796e+19\n",
      "Iteration 2, inertia 5.26858108676006e+19\n",
      "Iteration 3, inertia 5.22093163525987e+19\n",
      "Iteration 4, inertia 5.190788777526512e+19\n",
      "Iteration 5, inertia 5.160239147865853e+19\n",
      "Iteration 6, inertia 5.148521633120885e+19\n",
      "Iteration 7, inertia 5.142181167105112e+19\n",
      "Iteration 8, inertia 5.13792797291809e+19\n",
      "Iteration 9, inertia 5.1344350527770714e+19\n",
      "Iteration 10, inertia 5.131168693309644e+19\n",
      "Iteration 11, inertia 5.127228585717471e+19\n",
      "Iteration 12, inertia 5.118760311679725e+19\n",
      "Iteration 13, inertia 5.111919918545881e+19\n",
      "Iteration 14, inertia 5.10707683847305e+19\n",
      "Iteration 15, inertia 5.102878299450253e+19\n",
      "Iteration 16, inertia 5.0969945454577385e+19\n",
      "Iteration 17, inertia 5.092909967826738e+19\n",
      "Iteration 18, inertia 5.090629471511961e+19\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.398433884988831e+19\n",
      "Iteration 1, inertia 5.635073274559891e+19\n",
      "Iteration 2, inertia 5.470330429048341e+19\n",
      "Iteration 3, inertia 5.3825108425668755e+19\n",
      "Iteration 4, inertia 5.3423212650070606e+19\n",
      "Iteration 5, inertia 5.3199868951412376e+19\n",
      "Iteration 6, inertia 5.2973553733441044e+19\n",
      "Iteration 7, inertia 5.286941296499986e+19\n",
      "Iteration 8, inertia 5.280801042571306e+19\n",
      "Iteration 9, inertia 5.277685147468918e+19\n",
      "Iteration 10, inertia 5.2721235613771874e+19\n",
      "Iteration 11, inertia 5.26946346167883e+19\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.645289384988824e+19\n",
      "Iteration 1, inertia 5.55245683750189e+19\n",
      "Iteration 2, inertia 5.413019538158273e+19\n",
      "Iteration 3, inertia 5.359412188834479e+19\n",
      "Iteration 4, inertia 5.278671723210237e+19\n",
      "Iteration 5, inertia 5.269363814661929e+19\n",
      "Iteration 6, inertia 5.265448920430573e+19\n",
      "Iteration 7, inertia 5.264256115689573e+19\n",
      "Converged at iteration 7: center shift 5197106165778.728 within tolerance 13652364605003.256.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 6.177174984988779e+19\n",
      "Iteration 1, inertia 5.418770315192666e+19\n",
      "Iteration 2, inertia 5.32679824144994e+19\n",
      "Iteration 3, inertia 5.27611464724925e+19\n",
      "Iteration 4, inertia 5.236889727485181e+19\n",
      "Iteration 5, inertia 5.180662809783598e+19\n",
      "Iteration 6, inertia 5.151651157998442e+19\n",
      "Iteration 7, inertia 5.1358013059038495e+19\n",
      "Iteration 8, inertia 5.129678311712867e+19\n",
      "Iteration 9, inertia 5.127544681502685e+19\n",
      "Iteration 10, inertia 5.121275512099381e+19\n",
      "Iteration 11, inertia 5.118977254794208e+19\n",
      "Iteration 12, inertia 5.118598004335086e+19\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 7.774808584988765e+19\n",
      "Iteration 1, inertia 5.45738963664356e+19\n",
      "Iteration 2, inertia 5.293256496967463e+19\n",
      "Iteration 3, inertia 5.22672055656238e+19\n",
      "Iteration 4, inertia 5.175269272837771e+19\n",
      "Iteration 5, inertia 5.1567598166297625e+19\n",
      "Iteration 6, inertia 5.1343468019345826e+19\n",
      "Iteration 7, inertia 5.124864597546709e+19\n",
      "Iteration 8, inertia 5.123106504668448e+19\n",
      "Converged at iteration 8: strict convergence.\n"
     ]
    }
   ],
   "source": [
    "K = range(2, 21)\n",
    "\n",
    "inertia = []\n",
    "silhouette = []\n",
    "\n",
    "for k in K:\n",
    "    print(\"Training a K-Means model with {} clusters! \".format(k))\n",
    "    print()\n",
    "    kmeans = KMeans(n_clusters=k,\n",
    "                    random_state=1234,\n",
    "                    verbose=1)\n",
    "    kmeans.fit(No_bankrupcies)\n",
    "    \n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette.append(silhouette_score(No_bankrupcies, kmeans.predict(No_bankrupcies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bc7abe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Silhouette Method showing the optimal k')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "ax[0].plot(K, inertia, 'bx-')\n",
    "ax[0].set_xlabel('k')\n",
    "ax[0].set_ylabel('inertia')\n",
    "ax[0].set_xticks(np.arange(min(K), max(K)+1, 1.0))\n",
    "ax[0].set_title('Elbow Method showing the optimal k')\n",
    "ax[1].plot(K, silhouette, 'bx-')\n",
    "ax[1].set_xlabel('k')\n",
    "ax[1].set_ylabel('silhouette score')\n",
    "ax[1].set_xticks(np.arange(min(K), max(K)+1, 1.0))\n",
    "ax[1].set_title('Silhouette Method showing the optimal k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bedee1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(random_state=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=8, random_state=1)\n",
    "kmeans.fit(No_bankrupcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a1f701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1200\n",
       "1     665\n",
       "2     390\n",
       "3    1016\n",
       "4     239\n",
       "5     621\n",
       "6     394\n",
       "7     754\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = kmeans.predict(No_bankrupcies)\n",
    "\n",
    "elem_in_cluster = pd.Series(clusters).value_counts().sort_index() # Number of values in each cluster\n",
    "elem_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d935479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_weight = []\n",
    "for j in range(len(elem_in_cluster)):\n",
    "    weight = elem_in_cluster[j]/len(No_bankrupcies)\n",
    "    clusters_weight.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a88f98c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>Continuous interest rate (after tax)</th>\n",
       "      <th>Cash flow rate</th>\n",
       "      <th>Net Value Per Share (B)</th>\n",
       "      <th>Net Value Per Share (A)</th>\n",
       "      <th>Net Value Per Share (C)</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
       "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
       "      <th>Operating Profit Growth Rate</th>\n",
       "      <th>Regular Net Profit Growth Rate</th>\n",
       "      <th>Continuous Net Profit Growth Rate</th>\n",
       "      <th>Total Asset Growth Rate</th>\n",
       "      <th>Total Asset Return Growth Rate Ratio</th>\n",
       "      <th>Cash Reinvestment %</th>\n",
       "      <th>Interest Expense Ratio</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Long-term fund suitability ratio (A)</th>\n",
       "      <th>Borrowing dependency</th>\n",
       "      <th>Operating profit/Paid-in capital</th>\n",
       "      <th>Net profit before tax/Paid-in capital</th>\n",
       "      <th>Inventory and accounts receivable/Net value</th>\n",
       "      <th>Operating profit per person</th>\n",
       "      <th>Working Capital to Total Assets</th>\n",
       "      <th>Quick Assets/Total Assets</th>\n",
       "      <th>Cash/Total Assets</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Operating Funds to Liability</th>\n",
       "      <th>Inventory/Working Capital</th>\n",
       "      <th>Current Liabilities/Liability</th>\n",
       "      <th>Working Capital/Equity</th>\n",
       "      <th>Current Liabilities/Equity</th>\n",
       "      <th>Retained Earnings to Total Assets</th>\n",
       "      <th>Total income/Total expense</th>\n",
       "      <th>Total expense/Assets</th>\n",
       "      <th>Working capitcal Turnover Rate</th>\n",
       "      <th>Cash Flow to Sales</th>\n",
       "      <th>Current Liability to Equity</th>\n",
       "      <th>Equity to Long-term Liability</th>\n",
       "      <th>Cash Flow to Total Assets</th>\n",
       "      <th>Cash Flow to Liability</th>\n",
       "      <th>CFO to Assets</th>\n",
       "      <th>Cash Flow to Equity</th>\n",
       "      <th>Current Liability to Current Assets</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Equity to Liability</th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>0.517184</td>\n",
       "      <td>0.595181</td>\n",
       "      <td>0.561647</td>\n",
       "      <td>0.603756</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>0.797462</td>\n",
       "      <td>0.809370</td>\n",
       "      <td>0.303594</td>\n",
       "      <td>0.781611</td>\n",
       "      <td>0.475932</td>\n",
       "      <td>0.192954</td>\n",
       "      <td>0.192954</td>\n",
       "      <td>0.192954</td>\n",
       "      <td>0.230595</td>\n",
       "      <td>0.192815</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.689523</td>\n",
       "      <td>0.217599</td>\n",
       "      <td>6.640000e+08</td>\n",
       "      <td>0.264216</td>\n",
       "      <td>0.394127</td>\n",
       "      <td>0.631164</td>\n",
       "      <td>0.088430</td>\n",
       "      <td>0.911570</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.371884</td>\n",
       "      <td>0.108139</td>\n",
       "      <td>0.191878</td>\n",
       "      <td>0.396305</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>0.751356</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>0.057453</td>\n",
       "      <td>0.367288</td>\n",
       "      <td>0.280184</td>\n",
       "      <td>0.603629</td>\n",
       "      <td>0.731964</td>\n",
       "      <td>0.328345</td>\n",
       "      <td>0.951720</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.052945</td>\n",
       "      <td>0.593915</td>\n",
       "      <td>0.671555</td>\n",
       "      <td>0.328345</td>\n",
       "      <td>0.115561</td>\n",
       "      <td>0.574051</td>\n",
       "      <td>0.439802</td>\n",
       "      <td>0.642290</td>\n",
       "      <td>0.305384</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.830270</td>\n",
       "      <td>0.623729</td>\n",
       "      <td>0.603755</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>0.277612</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.565597</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>0.530249</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.570962</td>\n",
       "      <td>0.615301</td>\n",
       "      <td>0.998959</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.809339</td>\n",
       "      <td>0.303588</td>\n",
       "      <td>0.781579</td>\n",
       "      <td>0.476161</td>\n",
       "      <td>0.173992</td>\n",
       "      <td>0.173992</td>\n",
       "      <td>0.173992</td>\n",
       "      <td>0.215184</td>\n",
       "      <td>0.171349</td>\n",
       "      <td>0.022574</td>\n",
       "      <td>0.848154</td>\n",
       "      <td>0.689767</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>7.200000e+09</td>\n",
       "      <td>0.265032</td>\n",
       "      <td>0.387663</td>\n",
       "      <td>0.630612</td>\n",
       "      <td>0.047042</td>\n",
       "      <td>0.952958</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.095111</td>\n",
       "      <td>0.170357</td>\n",
       "      <td>0.395876</td>\n",
       "      <td>0.391895</td>\n",
       "      <td>0.921106</td>\n",
       "      <td>0.787858</td>\n",
       "      <td>0.131380</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>0.376574</td>\n",
       "      <td>0.276975</td>\n",
       "      <td>0.806740</td>\n",
       "      <td>0.740325</td>\n",
       "      <td>0.327503</td>\n",
       "      <td>0.922722</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>0.594134</td>\n",
       "      <td>0.671511</td>\n",
       "      <td>0.327503</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.583495</td>\n",
       "      <td>0.428952</td>\n",
       "      <td>0.622632</td>\n",
       "      <td>0.307950</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>0.800269</td>\n",
       "      <td>0.626044</td>\n",
       "      <td>0.615297</td>\n",
       "      <td>0.840279</td>\n",
       "      <td>0.276037</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>0.081620</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>0.514649</td>\n",
       "      <td>0.579427</td>\n",
       "      <td>0.564484</td>\n",
       "      <td>0.602581</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>0.797441</td>\n",
       "      <td>0.809358</td>\n",
       "      <td>0.303537</td>\n",
       "      <td>0.781620</td>\n",
       "      <td>0.464811</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.242980</td>\n",
       "      <td>0.195601</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.848002</td>\n",
       "      <td>0.689322</td>\n",
       "      <td>0.217585</td>\n",
       "      <td>5.960000e+09</td>\n",
       "      <td>0.263702</td>\n",
       "      <td>0.382307</td>\n",
       "      <td>0.630643</td>\n",
       "      <td>0.121293</td>\n",
       "      <td>0.878707</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.119954</td>\n",
       "      <td>0.194691</td>\n",
       "      <td>0.406797</td>\n",
       "      <td>0.402535</td>\n",
       "      <td>0.849637</td>\n",
       "      <td>0.563323</td>\n",
       "      <td>0.164218</td>\n",
       "      <td>0.119202</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.277259</td>\n",
       "      <td>0.929519</td>\n",
       "      <td>0.738858</td>\n",
       "      <td>0.331511</td>\n",
       "      <td>0.948818</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.047951</td>\n",
       "      <td>0.593944</td>\n",
       "      <td>0.671568</td>\n",
       "      <td>0.331511</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.625360</td>\n",
       "      <td>0.455607</td>\n",
       "      <td>0.602408</td>\n",
       "      <td>0.311788</td>\n",
       "      <td>0.024939</td>\n",
       "      <td>0.825276</td>\n",
       "      <td>0.623844</td>\n",
       "      <td>0.602577</td>\n",
       "      <td>0.842345</td>\n",
       "      <td>0.279389</td>\n",
       "      <td>0.026799</td>\n",
       "      <td>0.565198</td>\n",
       "      <td>0.030795</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>0.505484</td>\n",
       "      <td>0.557894</td>\n",
       "      <td>0.548156</td>\n",
       "      <td>0.663940</td>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.797888</td>\n",
       "      <td>0.809640</td>\n",
       "      <td>0.303736</td>\n",
       "      <td>0.781946</td>\n",
       "      <td>0.473439</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.227664</td>\n",
       "      <td>0.184228</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.848005</td>\n",
       "      <td>0.689324</td>\n",
       "      <td>0.217585</td>\n",
       "      <td>6.690000e+09</td>\n",
       "      <td>0.263834</td>\n",
       "      <td>0.381897</td>\n",
       "      <td>0.630650</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.945642</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.369760</td>\n",
       "      <td>0.106958</td>\n",
       "      <td>0.183244</td>\n",
       "      <td>0.394266</td>\n",
       "      <td>0.403903</td>\n",
       "      <td>0.826426</td>\n",
       "      <td>0.409265</td>\n",
       "      <td>0.221474</td>\n",
       "      <td>0.036908</td>\n",
       "      <td>0.363156</td>\n",
       "      <td>0.276975</td>\n",
       "      <td>0.615800</td>\n",
       "      <td>0.735769</td>\n",
       "      <td>0.327374</td>\n",
       "      <td>0.938185</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>0.594194</td>\n",
       "      <td>0.671591</td>\n",
       "      <td>0.327374</td>\n",
       "      <td>0.111261</td>\n",
       "      <td>0.649374</td>\n",
       "      <td>0.462153</td>\n",
       "      <td>0.606112</td>\n",
       "      <td>0.315305</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>0.811530</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.663940</td>\n",
       "      <td>0.840928</td>\n",
       "      <td>0.276277</td>\n",
       "      <td>0.026799</td>\n",
       "      <td>0.565197</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>0.504168</td>\n",
       "      <td>0.561928</td>\n",
       "      <td>0.549708</td>\n",
       "      <td>0.613738</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>0.797431</td>\n",
       "      <td>0.809345</td>\n",
       "      <td>0.303426</td>\n",
       "      <td>0.781605</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.174413</td>\n",
       "      <td>0.174413</td>\n",
       "      <td>0.174413</td>\n",
       "      <td>0.223598</td>\n",
       "      <td>0.179257</td>\n",
       "      <td>0.022115</td>\n",
       "      <td>0.848049</td>\n",
       "      <td>0.689431</td>\n",
       "      <td>0.217597</td>\n",
       "      <td>6.720000e+09</td>\n",
       "      <td>0.264053</td>\n",
       "      <td>0.389469</td>\n",
       "      <td>0.631966</td>\n",
       "      <td>0.154881</td>\n",
       "      <td>0.845119</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.378281</td>\n",
       "      <td>0.116453</td>\n",
       "      <td>0.178275</td>\n",
       "      <td>0.409453</td>\n",
       "      <td>0.396373</td>\n",
       "      <td>0.844396</td>\n",
       "      <td>0.623509</td>\n",
       "      <td>0.081422</td>\n",
       "      <td>0.149663</td>\n",
       "      <td>0.347908</td>\n",
       "      <td>0.277307</td>\n",
       "      <td>0.920141</td>\n",
       "      <td>0.740035</td>\n",
       "      <td>0.334448</td>\n",
       "      <td>0.935566</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.068622</td>\n",
       "      <td>0.593963</td>\n",
       "      <td>0.671572</td>\n",
       "      <td>0.334448</td>\n",
       "      <td>0.112961</td>\n",
       "      <td>0.641425</td>\n",
       "      <td>0.459027</td>\n",
       "      <td>0.600923</td>\n",
       "      <td>0.314273</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.808609</td>\n",
       "      <td>0.623801</td>\n",
       "      <td>0.613738</td>\n",
       "      <td>0.841269</td>\n",
       "      <td>0.282046</td>\n",
       "      <td>0.027258</td>\n",
       "      <td>0.566703</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.609454</td>\n",
       "      <td>0.602048</td>\n",
       "      <td>0.999060</td>\n",
       "      <td>0.797657</td>\n",
       "      <td>0.809519</td>\n",
       "      <td>0.303802</td>\n",
       "      <td>0.781793</td>\n",
       "      <td>0.456043</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.247424</td>\n",
       "      <td>0.219703</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.848523</td>\n",
       "      <td>0.690747</td>\n",
       "      <td>0.217740</td>\n",
       "      <td>9.550000e+09</td>\n",
       "      <td>0.265918</td>\n",
       "      <td>0.367814</td>\n",
       "      <td>0.630852</td>\n",
       "      <td>0.085437</td>\n",
       "      <td>0.914563</td>\n",
       "      <td>0.006786</td>\n",
       "      <td>0.372422</td>\n",
       "      <td>0.110167</td>\n",
       "      <td>0.201259</td>\n",
       "      <td>0.400994</td>\n",
       "      <td>0.436184</td>\n",
       "      <td>0.816919</td>\n",
       "      <td>0.298744</td>\n",
       "      <td>0.036543</td>\n",
       "      <td>0.069614</td>\n",
       "      <td>0.328416</td>\n",
       "      <td>0.277342</td>\n",
       "      <td>0.758802</td>\n",
       "      <td>0.735780</td>\n",
       "      <td>0.328766</td>\n",
       "      <td>0.946399</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.593968</td>\n",
       "      <td>0.671580</td>\n",
       "      <td>0.328766</td>\n",
       "      <td>0.112060</td>\n",
       "      <td>0.654665</td>\n",
       "      <td>0.462703</td>\n",
       "      <td>0.530002</td>\n",
       "      <td>0.316109</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.841345</td>\n",
       "      <td>0.624089</td>\n",
       "      <td>0.602044</td>\n",
       "      <td>0.843005</td>\n",
       "      <td>0.277477</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>0.565444</td>\n",
       "      <td>0.044851</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.506752</td>\n",
       "      <td>0.563399</td>\n",
       "      <td>0.563360</td>\n",
       "      <td>0.604599</td>\n",
       "      <td>0.998995</td>\n",
       "      <td>0.797593</td>\n",
       "      <td>0.809508</td>\n",
       "      <td>0.303826</td>\n",
       "      <td>0.781807</td>\n",
       "      <td>0.480659</td>\n",
       "      <td>0.179133</td>\n",
       "      <td>0.179133</td>\n",
       "      <td>0.179133</td>\n",
       "      <td>0.223315</td>\n",
       "      <td>0.176094</td>\n",
       "      <td>0.022332</td>\n",
       "      <td>0.848212</td>\n",
       "      <td>0.690410</td>\n",
       "      <td>0.217719</td>\n",
       "      <td>6.670000e+09</td>\n",
       "      <td>0.264434</td>\n",
       "      <td>0.380099</td>\n",
       "      <td>0.630623</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>0.973668</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.369654</td>\n",
       "      <td>0.096316</td>\n",
       "      <td>0.174391</td>\n",
       "      <td>0.395112</td>\n",
       "      <td>0.392777</td>\n",
       "      <td>0.802107</td>\n",
       "      <td>0.223893</td>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.018016</td>\n",
       "      <td>0.374684</td>\n",
       "      <td>0.277047</td>\n",
       "      <td>0.581476</td>\n",
       "      <td>0.734264</td>\n",
       "      <td>0.326699</td>\n",
       "      <td>0.937618</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.011049</td>\n",
       "      <td>0.594009</td>\n",
       "      <td>0.671575</td>\n",
       "      <td>0.326699</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.644949</td>\n",
       "      <td>0.460999</td>\n",
       "      <td>0.597414</td>\n",
       "      <td>0.314774</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.814818</td>\n",
       "      <td>0.625298</td>\n",
       "      <td>0.604595</td>\n",
       "      <td>0.841004</td>\n",
       "      <td>0.275430</td>\n",
       "      <td>0.026794</td>\n",
       "      <td>0.565173</td>\n",
       "      <td>0.139579</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510</th>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.537942</td>\n",
       "      <td>0.546228</td>\n",
       "      <td>0.598618</td>\n",
       "      <td>0.998993</td>\n",
       "      <td>0.797407</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>0.303504</td>\n",
       "      <td>0.781581</td>\n",
       "      <td>0.464134</td>\n",
       "      <td>0.191901</td>\n",
       "      <td>0.191901</td>\n",
       "      <td>0.191901</td>\n",
       "      <td>0.216791</td>\n",
       "      <td>0.172855</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.847981</td>\n",
       "      <td>0.689186</td>\n",
       "      <td>0.217569</td>\n",
       "      <td>6.690000e+09</td>\n",
       "      <td>0.263616</td>\n",
       "      <td>0.377799</td>\n",
       "      <td>0.631126</td>\n",
       "      <td>0.108746</td>\n",
       "      <td>0.891254</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.371420</td>\n",
       "      <td>0.099818</td>\n",
       "      <td>0.171872</td>\n",
       "      <td>0.400561</td>\n",
       "      <td>0.392924</td>\n",
       "      <td>0.799142</td>\n",
       "      <td>0.317485</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>0.075222</td>\n",
       "      <td>0.346042</td>\n",
       "      <td>0.277294</td>\n",
       "      <td>0.648311</td>\n",
       "      <td>0.735095</td>\n",
       "      <td>0.329321</td>\n",
       "      <td>0.939319</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.010702</td>\n",
       "      <td>0.593957</td>\n",
       "      <td>0.671572</td>\n",
       "      <td>0.329321</td>\n",
       "      <td>0.115051</td>\n",
       "      <td>0.642970</td>\n",
       "      <td>0.459284</td>\n",
       "      <td>0.583741</td>\n",
       "      <td>0.314572</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.799665</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.598617</td>\n",
       "      <td>0.840313</td>\n",
       "      <td>0.278637</td>\n",
       "      <td>0.026924</td>\n",
       "      <td>0.565720</td>\n",
       "      <td>0.034702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>0.493004</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>0.598553</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>0.797423</td>\n",
       "      <td>0.809338</td>\n",
       "      <td>0.303543</td>\n",
       "      <td>0.781589</td>\n",
       "      <td>0.458264</td>\n",
       "      <td>0.193081</td>\n",
       "      <td>0.193081</td>\n",
       "      <td>0.193081</td>\n",
       "      <td>0.226057</td>\n",
       "      <td>0.186639</td>\n",
       "      <td>0.022120</td>\n",
       "      <td>0.848051</td>\n",
       "      <td>0.690247</td>\n",
       "      <td>0.217607</td>\n",
       "      <td>7.410000e+09</td>\n",
       "      <td>0.264360</td>\n",
       "      <td>0.362198</td>\n",
       "      <td>0.630797</td>\n",
       "      <td>0.148442</td>\n",
       "      <td>0.851558</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.371939</td>\n",
       "      <td>0.108953</td>\n",
       "      <td>0.185687</td>\n",
       "      <td>0.409880</td>\n",
       "      <td>0.399471</td>\n",
       "      <td>0.859163</td>\n",
       "      <td>0.740799</td>\n",
       "      <td>0.164228</td>\n",
       "      <td>0.132765</td>\n",
       "      <td>0.332666</td>\n",
       "      <td>0.277119</td>\n",
       "      <td>0.849657</td>\n",
       "      <td>0.740908</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.937350</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.026386</td>\n",
       "      <td>0.593949</td>\n",
       "      <td>0.671566</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.113265</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.455040</td>\n",
       "      <td>0.526209</td>\n",
       "      <td>0.309958</td>\n",
       "      <td>0.025129</td>\n",
       "      <td>0.813044</td>\n",
       "      <td>0.624115</td>\n",
       "      <td>0.598553</td>\n",
       "      <td>0.841628</td>\n",
       "      <td>0.281446</td>\n",
       "      <td>0.026838</td>\n",
       "      <td>0.565371</td>\n",
       "      <td>0.024543</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>0.504558</td>\n",
       "      <td>0.558820</td>\n",
       "      <td>0.549708</td>\n",
       "      <td>0.606588</td>\n",
       "      <td>0.999107</td>\n",
       "      <td>0.797647</td>\n",
       "      <td>0.809499</td>\n",
       "      <td>0.303685</td>\n",
       "      <td>0.781777</td>\n",
       "      <td>0.480023</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0.222558</td>\n",
       "      <td>0.178128</td>\n",
       "      <td>0.022064</td>\n",
       "      <td>0.848015</td>\n",
       "      <td>0.689280</td>\n",
       "      <td>0.217591</td>\n",
       "      <td>6.070000e+07</td>\n",
       "      <td>0.263664</td>\n",
       "      <td>0.373999</td>\n",
       "      <td>0.630690</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>0.984612</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.370001</td>\n",
       "      <td>0.101015</td>\n",
       "      <td>0.177174</td>\n",
       "      <td>0.396286</td>\n",
       "      <td>0.401574</td>\n",
       "      <td>0.876058</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.246222</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.388888</td>\n",
       "      <td>0.277095</td>\n",
       "      <td>0.859788</td>\n",
       "      <td>0.737410</td>\n",
       "      <td>0.326650</td>\n",
       "      <td>0.946818</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.594165</td>\n",
       "      <td>0.671688</td>\n",
       "      <td>0.326650</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.715709</td>\n",
       "      <td>0.555735</td>\n",
       "      <td>0.594555</td>\n",
       "      <td>0.321760</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.812539</td>\n",
       "      <td>0.841360</td>\n",
       "      <td>0.606588</td>\n",
       "      <td>0.840855</td>\n",
       "      <td>0.275145</td>\n",
       "      <td>0.026810</td>\n",
       "      <td>0.565246</td>\n",
       "      <td>0.219073</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5279 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "2542                                           0.517184          \n",
       "1809                                           0.530249          \n",
       "4322                                           0.514649          \n",
       "3933                                           0.505484          \n",
       "1784                                           0.504168          \n",
       "...                                                 ...          \n",
       "1337                                           0.569688          \n",
       "406                                            0.506752          \n",
       "5510                                           0.493151          \n",
       "2191                                           0.493004          \n",
       "2671                                           0.504558          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "2542                                 0.595181   \n",
       "1809                                 0.538432   \n",
       "4322                                 0.579427   \n",
       "3933                                 0.557894   \n",
       "1784                                 0.561928   \n",
       "...                                       ...   \n",
       "1337                                 0.625600   \n",
       "406                                  0.563399   \n",
       "5510                                 0.537942   \n",
       "2191                                 0.562800   \n",
       "2671                                 0.558820   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "2542                                           0.561647    \n",
       "1809                                           0.570962    \n",
       "4322                                           0.564484    \n",
       "3933                                           0.548156    \n",
       "1784                                           0.549708    \n",
       "...                                                 ...    \n",
       "1337                                           0.609454    \n",
       "406                                            0.563360    \n",
       "5510                                           0.546228    \n",
       "2191                                           0.539804    \n",
       "2671                                           0.549708    \n",
       "\n",
       "       Operating Gross Margin   Operating Profit Rate  \\\n",
       "2542                 0.603756                0.998997   \n",
       "1809                 0.615301                0.998959   \n",
       "4322                 0.602581                0.999006   \n",
       "3933                 0.663940                0.999284   \n",
       "1784                 0.613738                0.999051   \n",
       "...                       ...                     ...   \n",
       "1337                 0.602048                0.999060   \n",
       "406                  0.604599                0.998995   \n",
       "5510                 0.598618                0.998993   \n",
       "2191                 0.598553                0.998988   \n",
       "2671                 0.606588                0.999107   \n",
       "\n",
       "       Pre-tax net Interest Rate   After-tax net Interest Rate  \\\n",
       "2542                    0.797462                      0.809370   \n",
       "1809                    0.797414                      0.809339   \n",
       "4322                    0.797441                      0.809358   \n",
       "3933                    0.797888                      0.809640   \n",
       "1784                    0.797431                      0.809345   \n",
       "...                          ...                           ...   \n",
       "1337                    0.797657                      0.809519   \n",
       "406                     0.797593                      0.809508   \n",
       "5510                    0.797407                      0.809325   \n",
       "2191                    0.797423                      0.809338   \n",
       "2671                    0.797647                      0.809499   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  \\\n",
       "2542                                      0.303594   \n",
       "1809                                      0.303588   \n",
       "4322                                      0.303537   \n",
       "3933                                      0.303736   \n",
       "1784                                      0.303426   \n",
       "...                                            ...   \n",
       "1337                                      0.303802   \n",
       "406                                       0.303826   \n",
       "5510                                      0.303504   \n",
       "2191                                      0.303543   \n",
       "2671                                      0.303685   \n",
       "\n",
       "       Continuous interest rate (after tax)   Cash flow rate  \\\n",
       "2542                               0.781611         0.475932   \n",
       "1809                               0.781579         0.476161   \n",
       "4322                               0.781620         0.464811   \n",
       "3933                               0.781946         0.473439   \n",
       "1784                               0.781605         0.464009   \n",
       "...                                     ...              ...   \n",
       "1337                               0.781793         0.456043   \n",
       "406                                0.781807         0.480659   \n",
       "5510                               0.781581         0.464134   \n",
       "2191                               0.781589         0.458264   \n",
       "2671                               0.781777         0.480023   \n",
       "\n",
       "       Net Value Per Share (B)   Net Value Per Share (A)  \\\n",
       "2542                  0.192954                  0.192954   \n",
       "1809                  0.173992                  0.173992   \n",
       "4322                  0.206565                  0.206565   \n",
       "3933                  0.217564                  0.217564   \n",
       "1784                  0.174413                  0.174413   \n",
       "...                        ...                       ...   \n",
       "1337                  0.198053                  0.198053   \n",
       "406                   0.179133                  0.179133   \n",
       "5510                  0.191901                  0.191901   \n",
       "2191                  0.193081                  0.193081   \n",
       "2671                  0.188825                  0.188825   \n",
       "\n",
       "       Net Value Per Share (C)   Persistent EPS in the Last Four Seasons  \\\n",
       "2542                  0.192954                                  0.230595   \n",
       "1809                  0.173992                                  0.215184   \n",
       "4322                  0.206565                                  0.242980   \n",
       "3933                  0.217564                                  0.227664   \n",
       "1784                  0.174413                                  0.223598   \n",
       "...                        ...                                       ...   \n",
       "1337                  0.198053                                  0.247424   \n",
       "406                   0.179133                                  0.223315   \n",
       "5510                  0.191901                                  0.216791   \n",
       "2191                  0.193081                                  0.226057   \n",
       "2671                  0.188825                                  0.222558   \n",
       "\n",
       "       Per Share Net profit before tax (Yuan ¥)  \\\n",
       "2542                                   0.192815   \n",
       "1809                                   0.171349   \n",
       "4322                                   0.195601   \n",
       "3933                                   0.184228   \n",
       "1784                                   0.179257   \n",
       "...                                         ...   \n",
       "1337                                   0.219703   \n",
       "406                                    0.176094   \n",
       "5510                                   0.172855   \n",
       "2191                                   0.186639   \n",
       "2671                                   0.178128   \n",
       "\n",
       "       Realized Sales Gross Profit Growth Rate   Operating Profit Growth Rate  \\\n",
       "2542                                  0.022095                       0.848005   \n",
       "1809                                  0.022574                       0.848154   \n",
       "4322                                  0.022119                       0.848002   \n",
       "3933                                  0.022086                       0.848005   \n",
       "1784                                  0.022115                       0.848049   \n",
       "...                                        ...                            ...   \n",
       "1337                                  0.025418                       0.848523   \n",
       "406                                   0.022332                       0.848212   \n",
       "5510                                  0.022040                       0.847981   \n",
       "2191                                  0.022120                       0.848051   \n",
       "2671                                  0.022064                       0.848015   \n",
       "\n",
       "       Regular Net Profit Growth Rate   Continuous Net Profit Growth Rate  \\\n",
       "2542                         0.689523                            0.217599   \n",
       "1809                         0.689767                            0.217631   \n",
       "4322                         0.689322                            0.217585   \n",
       "3933                         0.689324                            0.217585   \n",
       "1784                         0.689431                            0.217597   \n",
       "...                               ...                                 ...   \n",
       "1337                         0.690747                            0.217740   \n",
       "406                          0.690410                            0.217719   \n",
       "5510                         0.689186                            0.217569   \n",
       "2191                         0.690247                            0.217607   \n",
       "2671                         0.689280                            0.217591   \n",
       "\n",
       "       Total Asset Growth Rate   Total Asset Return Growth Rate Ratio  \\\n",
       "2542              6.640000e+08                               0.264216   \n",
       "1809              7.200000e+09                               0.265032   \n",
       "4322              5.960000e+09                               0.263702   \n",
       "3933              6.690000e+09                               0.263834   \n",
       "1784              6.720000e+09                               0.264053   \n",
       "...                        ...                                    ...   \n",
       "1337              9.550000e+09                               0.265918   \n",
       "406               6.670000e+09                               0.264434   \n",
       "5510              6.690000e+09                               0.263616   \n",
       "2191              7.410000e+09                               0.264360   \n",
       "2671              6.070000e+07                               0.263664   \n",
       "\n",
       "       Cash Reinvestment %   Interest Expense Ratio   Debt ratio %  \\\n",
       "2542              0.394127                 0.631164       0.088430   \n",
       "1809              0.387663                 0.630612       0.047042   \n",
       "4322              0.382307                 0.630643       0.121293   \n",
       "3933              0.381897                 0.630650       0.054358   \n",
       "1784              0.389469                 0.631966       0.154881   \n",
       "...                    ...                      ...            ...   \n",
       "1337              0.367814                 0.630852       0.085437   \n",
       "406               0.380099                 0.630623       0.026332   \n",
       "5510              0.377799                 0.631126       0.108746   \n",
       "2191              0.362198                 0.630797       0.148442   \n",
       "2671              0.373999                 0.630690       0.015388   \n",
       "\n",
       "       Net worth/Assets   Long-term fund suitability ratio (A)  \\\n",
       "2542           0.911570                               0.005192   \n",
       "1809           0.952958                               0.005991   \n",
       "4322           0.878707                               0.010607   \n",
       "3933           0.945642                               0.006713   \n",
       "1784           0.845119                               0.007876   \n",
       "...                 ...                                    ...   \n",
       "1337           0.914563                               0.006786   \n",
       "406            0.973668                               0.005631   \n",
       "5510           0.891254                               0.005502   \n",
       "2191           0.851558                               0.005683   \n",
       "2671           0.984612                               0.006578   \n",
       "\n",
       "       Borrowing dependency   Operating profit/Paid-in capital  \\\n",
       "2542               0.371884                           0.108139   \n",
       "1809               0.369637                           0.095111   \n",
       "4322               0.369637                           0.119954   \n",
       "3933               0.369760                           0.106958   \n",
       "1784               0.378281                           0.116453   \n",
       "...                     ...                                ...   \n",
       "1337               0.372422                           0.110167   \n",
       "406                0.369654                           0.096316   \n",
       "5510               0.371420                           0.099818   \n",
       "2191               0.371939                           0.108953   \n",
       "2671               0.370001                           0.101015   \n",
       "\n",
       "       Net profit before tax/Paid-in capital  \\\n",
       "2542                                0.191878   \n",
       "1809                                0.170357   \n",
       "4322                                0.194691   \n",
       "3933                                0.183244   \n",
       "1784                                0.178275   \n",
       "...                                      ...   \n",
       "1337                                0.201259   \n",
       "406                                 0.174391   \n",
       "5510                                0.171872   \n",
       "2191                                0.185687   \n",
       "2671                                0.177174   \n",
       "\n",
       "       Inventory and accounts receivable/Net value  \\\n",
       "2542                                      0.396305   \n",
       "1809                                      0.395876   \n",
       "4322                                      0.406797   \n",
       "3933                                      0.394266   \n",
       "1784                                      0.409453   \n",
       "...                                            ...   \n",
       "1337                                      0.400994   \n",
       "406                                       0.395112   \n",
       "5510                                      0.400561   \n",
       "2191                                      0.409880   \n",
       "2671                                      0.396286   \n",
       "\n",
       "       Operating profit per person   Working Capital to Total Assets  \\\n",
       "2542                      0.401404                          0.751356   \n",
       "1809                      0.391895                          0.921106   \n",
       "4322                      0.402535                          0.849637   \n",
       "3933                      0.403903                          0.826426   \n",
       "1784                      0.396373                          0.844396   \n",
       "...                            ...                               ...   \n",
       "1337                      0.436184                          0.816919   \n",
       "406                       0.392777                          0.802107   \n",
       "5510                      0.392924                          0.799142   \n",
       "2191                      0.399471                          0.859163   \n",
       "2671                      0.401574                          0.876058   \n",
       "\n",
       "       Quick Assets/Total Assets   Cash/Total Assets  \\\n",
       "2542                    0.171034            0.019934   \n",
       "1809                    0.787858            0.131380   \n",
       "4322                    0.563323            0.164218   \n",
       "3933                    0.409265            0.221474   \n",
       "1784                    0.623509            0.081422   \n",
       "...                          ...                 ...   \n",
       "1337                    0.298744            0.036543   \n",
       "406                     0.223893            0.041121   \n",
       "5510                    0.317485            0.009554   \n",
       "2191                    0.740799            0.164228   \n",
       "2671                    0.415983            0.246222   \n",
       "\n",
       "       Current Liability to Assets   Operating Funds to Liability  \\\n",
       "2542                      0.057453                       0.367288   \n",
       "1809                      0.042138                       0.376574   \n",
       "4322                      0.119202                       0.350225   \n",
       "3933                      0.036908                       0.363156   \n",
       "1784                      0.149663                       0.347908   \n",
       "...                            ...                            ...   \n",
       "1337                      0.069614                       0.328416   \n",
       "406                       0.018016                       0.374684   \n",
       "5510                      0.075222                       0.346042   \n",
       "2191                      0.132765                       0.332666   \n",
       "2671                      0.016984                       0.388888   \n",
       "\n",
       "       Inventory/Working Capital   Current Liabilities/Liability  \\\n",
       "2542                    0.280184                        0.603629   \n",
       "1809                    0.276975                        0.806740   \n",
       "4322                    0.277259                        0.929519   \n",
       "3933                    0.276975                        0.615800   \n",
       "1784                    0.277307                        0.920141   \n",
       "...                          ...                             ...   \n",
       "1337                    0.277342                        0.758802   \n",
       "406                     0.277047                        0.581476   \n",
       "5510                    0.277294                        0.648311   \n",
       "2191                    0.277119                        0.849657   \n",
       "2671                    0.277095                        0.859788   \n",
       "\n",
       "       Working Capital/Equity   Current Liabilities/Equity  \\\n",
       "2542                 0.731964                     0.328345   \n",
       "1809                 0.740325                     0.327503   \n",
       "4322                 0.738858                     0.331511   \n",
       "3933                 0.735769                     0.327374   \n",
       "1784                 0.740035                     0.334448   \n",
       "...                       ...                          ...   \n",
       "1337                 0.735780                     0.328766   \n",
       "406                  0.734264                     0.326699   \n",
       "5510                 0.735095                     0.329321   \n",
       "2191                 0.740908                     0.333200   \n",
       "2671                 0.737410                     0.326650   \n",
       "\n",
       "       Retained Earnings to Total Assets   Total income/Total expense  \\\n",
       "2542                            0.951720                     0.002341   \n",
       "1809                            0.922722                     0.002253   \n",
       "4322                            0.948818                     0.002334   \n",
       "3933                            0.938185                     0.002321   \n",
       "1784                            0.935566                     0.002261   \n",
       "...                                  ...                          ...   \n",
       "1337                            0.946399                     0.003123   \n",
       "406                             0.937618                     0.002522   \n",
       "5510                            0.939319                     0.002289   \n",
       "2191                            0.937350                     0.002341   \n",
       "2671                            0.946818                     0.002488   \n",
       "\n",
       "       Total expense/Assets   Working capitcal Turnover Rate  \\\n",
       "2542               0.052945                         0.593915   \n",
       "1809               0.032573                         0.594134   \n",
       "4322               0.047951                         0.593944   \n",
       "3933               0.029146                         0.594194   \n",
       "1784               0.068622                         0.593963   \n",
       "...                     ...                              ...   \n",
       "1337               0.008681                         0.593968   \n",
       "406                0.011049                         0.594009   \n",
       "5510               0.010702                         0.593957   \n",
       "2191               0.026386                         0.593949   \n",
       "2671               0.010999                         0.594165   \n",
       "\n",
       "       Cash Flow to Sales   Current Liability to Equity  \\\n",
       "2542             0.671555                      0.328345   \n",
       "1809             0.671511                      0.327503   \n",
       "4322             0.671568                      0.331511   \n",
       "3933             0.671591                      0.327374   \n",
       "1784             0.671572                      0.334448   \n",
       "...                   ...                           ...   \n",
       "1337             0.671580                      0.328766   \n",
       "406              0.671575                      0.326699   \n",
       "5510             0.671572                      0.329321   \n",
       "2191             0.671566                      0.333200   \n",
       "2671             0.671688                      0.326650   \n",
       "\n",
       "       Equity to Long-term Liability   Cash Flow to Total Assets  \\\n",
       "2542                        0.115561                    0.574051   \n",
       "1809                        0.110933                    0.583495   \n",
       "4322                        0.110933                    0.625360   \n",
       "3933                        0.111261                    0.649374   \n",
       "1784                        0.112961                    0.641425   \n",
       "...                              ...                         ...   \n",
       "1337                        0.112060                    0.654665   \n",
       "406                         0.110933                    0.644949   \n",
       "5510                        0.115051                    0.642970   \n",
       "2191                        0.113265                    0.618267   \n",
       "2671                        0.110933                    0.715709   \n",
       "\n",
       "       Cash Flow to Liability   CFO to Assets   Cash Flow to Equity  \\\n",
       "2542                 0.439802        0.642290              0.305384   \n",
       "1809                 0.428952        0.622632              0.307950   \n",
       "4322                 0.455607        0.602408              0.311788   \n",
       "3933                 0.462153        0.606112              0.315305   \n",
       "1784                 0.459027        0.600923              0.314273   \n",
       "...                       ...             ...                   ...   \n",
       "1337                 0.462703        0.530002              0.316109   \n",
       "406                  0.460999        0.597414              0.314774   \n",
       "5510                 0.459284        0.583741              0.314572   \n",
       "2191                 0.455040        0.526209              0.309958   \n",
       "2671                 0.555735        0.594555              0.321760   \n",
       "\n",
       "       Current Liability to Current Assets   Net Income to Total Assets  \\\n",
       "2542                              0.047557                     0.830270   \n",
       "1809                              0.008567                     0.800269   \n",
       "4322                              0.024939                     0.825276   \n",
       "3933                              0.014551                     0.811530   \n",
       "1784                              0.028316                     0.808609   \n",
       "...                                    ...                          ...   \n",
       "1337                              0.023299                     0.841345   \n",
       "406                               0.011483                     0.814818   \n",
       "5510                              0.027990                     0.799665   \n",
       "2191                              0.025129                     0.813044   \n",
       "2671                              0.005068                     0.812539   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "2542             0.623729                0.603755   \n",
       "1809             0.626044                0.615297   \n",
       "4322             0.623844                0.602577   \n",
       "3933             0.624060                0.663940   \n",
       "1784             0.623801                0.613738   \n",
       "...                   ...                     ...   \n",
       "1337             0.624089                0.602044   \n",
       "406              0.625298                0.604595   \n",
       "5510             0.623397                0.598617   \n",
       "2191             0.624115                0.598553   \n",
       "2671             0.841360                0.606588   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "2542                             0.842314              0.277612   \n",
       "1809                             0.840279              0.276037   \n",
       "4322                             0.842345              0.279389   \n",
       "3933                             0.840928              0.276277   \n",
       "1784                             0.841269              0.282046   \n",
       "...                                   ...                   ...   \n",
       "1337                             0.843005              0.277477   \n",
       "406                              0.841004              0.275430   \n",
       "5510                             0.840313              0.278637   \n",
       "2191                             0.841628              0.281446   \n",
       "2671                             0.840855              0.275145   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "2542                             0.026892   \n",
       "1809                             0.026791   \n",
       "4322                             0.026799   \n",
       "3933                             0.026799   \n",
       "1784                             0.027258   \n",
       "...                                   ...   \n",
       "1337                             0.026855   \n",
       "406                              0.026794   \n",
       "5510                             0.026924   \n",
       "2191                             0.026838   \n",
       "2671                             0.026810   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)  \\\n",
       "2542                                           0.565597     \n",
       "1809                                           0.565158     \n",
       "4322                                           0.565198     \n",
       "3933                                           0.565197     \n",
       "1784                                           0.566703     \n",
       "...                                                 ...     \n",
       "1337                                           0.565444     \n",
       "406                                            0.565173     \n",
       "5510                                           0.565720     \n",
       "2191                                           0.565371     \n",
       "2671                                           0.565246     \n",
       "\n",
       "       Equity to Liability  Bankrupt?  cluster  \n",
       "2542              0.043263          0        6  \n",
       "1809              0.081620          0        3  \n",
       "4322              0.030795          0        7  \n",
       "3933              0.070907          0        0  \n",
       "1784              0.023375          0        0  \n",
       "...                    ...        ...      ...  \n",
       "1337              0.044851          0        2  \n",
       "406               0.139579          0        0  \n",
       "5510              0.034702          0        0  \n",
       "2191              0.024543          0        3  \n",
       "2671              0.219073          0        1  \n",
       "\n",
       "[5279 rows x 62 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "No_bankrupcies[\"cluster\"] = clusters\n",
    "No_bankrupcies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f0592c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_No_bankrupcies = pd.DataFrame(columns=No_bankrupcies.columns, dtype=float)\n",
    " \n",
    "for cluster, weight in enumerate(clusters_weight):\n",
    "    new_No_bankrupcies = pd.concat([new_No_bankrupcies, No_bankrupcies[No_bankrupcies[\"cluster\"]==cluster].sample(round(300*weight))], axis=0)\n",
    "    \n",
    "new_No_bankrupcies.drop(columns=['cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "073490ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "final1_df = pd.concat([new_No_bankrupcies, upsample], axis=0)\n",
    "final1_df = final1_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21d0641b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 61)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e14d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = final1_df['Bankrupt?'].astype('int')\n",
    "X_train = final1_df.drop(['Bankrupt?'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661c99d",
   "metadata": {},
   "source": [
    "# Predicting with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "604cad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model n_neighbors=2 is: 0.88\n",
      "The kappa of the model n_neighbors=2 is: 0.75\n",
      "The accuracy of the model n_neighbors=5 is: 0.81\n",
      "The kappa of the model n_neighbors=5 is: 0.62\n",
      "The accuracy of the model n_neighbors=8 is: 0.76\n",
      "The kappa of the model n_neighbors=8 is: 0.51\n",
      "The accuracy of the model n_neighbors=11 is: 0.73\n",
      "The kappa of the model n_neighbors=11 is: 0.47\n"
     ]
    }
   ],
   "source": [
    "# entrenar distintos modelos con distintos valores de k\n",
    "K = range(2, 14, 3)\n",
    "accuracies = []\n",
    "models = []\n",
    "\n",
    "for k in K:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k) \n",
    "    knn.fit(X_train, y_train)\n",
    "    models.append(knn)\n",
    "    ypred_train = knn.predict(X_train)\n",
    "    accuracies.append(accuracy_score(y_train, ypred_train))\n",
    "    print(\"The accuracy of the model n_neighbors={} is: {:.2f}\".format(k, accuracy_score(y_train, ypred_train)))\n",
    "    print(\"The kappa of the model n_neighbors={} is: {:.2f}\".format(k, cohen_kappa_score(y_train, ypred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "278eebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model n_neighbors=2 is: 0.77\n",
      "The kappa score of the model n_neighbors=2 is: 0.06\n",
      "The accuracy of the model n_neighbors=5 is: 0.65\n",
      "The kappa score of the model n_neighbors=5 is: 0.05\n",
      "The accuracy of the model n_neighbors=8 is: 0.73\n",
      "The kappa score of the model n_neighbors=8 is: 0.08\n",
      "The accuracy of the model n_neighbors=11 is: 0.70\n",
      "The kappa score of the model n_neighbors=11 is: 0.07\n"
     ]
    }
   ],
   "source": [
    "K = range(2, 14, 3)\n",
    "#accuracies = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    #knn = KNeighborsClassifier(n_neighbors=k) \n",
    "    knn = models[i]\n",
    "    ypred_test = knn.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, ypred_test))\n",
    "    print(\"The accuracy of the model n_neighbors={} is: {:.2f}\".format(list(K)[i], accuracy_score(y_test, ypred_test)))\n",
    "    print(\"The kappa score of the model n_neighbors={} is: {:.2f}\".format(list(K)[i],cohen_kappa_score(y_test, ypred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80f961f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2) \n",
    "knn.fit(X_train, y_train)\n",
    "ypred_train = knn.predict(X_train)\n",
    "ypred_test = knn.predict(X_test)\n",
    "display(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9aa8ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[282,  57],\n",
       "       [ 18, 243]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(confusion_matrix(ypred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0ad5f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1024,   24],\n",
       "       [ 296,   20]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(confusion_matrix(ypred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28443f5",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8353638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the Random Forest in the TRAIN set is 0.92\n",
      "The accuracy for the Random Forest in the TEST set is 0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    309\n",
       "0    291\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[271,  29],\n",
       "       [ 20, 280]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8366666666666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1153\n",
       "1     211\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1150,  170],\n",
       "       [   3,   41]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.28330883335560597"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=4,\n",
    "                             min_samples_split=6,\n",
    "                             min_samples_leaf =3,\n",
    "                             max_samples=0.8, random_state=8)\n",
    "                            \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"The accuracy for the Random Forest in the TRAIN set is {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"The accuracy for the Random Forest in the TEST set is {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2b0bb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "max_depth_choices= [2, 3, 5,7,9] \n",
    "min_samples_split_choices = [2,4,6,8,9]  \n",
    "min_samples_leaf_choices = [1,3] \n",
    "max_samples=[0.8,0.5]\n",
    "#n_jobs = [-1]\n",
    "\n",
    "grid = {'max_depth': max_depth_choices,\n",
    "        'min_samples_split': min_samples_split_choices,\n",
    "        'min_samples_leaf': min_samples_leaf_choices,\n",
    "        'max_samples':max_samples}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = grid, cv = 5) \n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96fef021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'max_samples': 0.8,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "767875ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the Random Forest in the TRAIN set is 0.89\n",
      "The accuracy for the Random Forest in the TEST set is 0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    314\n",
       "0    286\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[260,  40],\n",
       "       [ 26, 274]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1144\n",
       "1     220\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1141,  179],\n",
       "       [   3,   41]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2714359504132231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(**grid_search.best_params_, random_state =8)\n",
    "                          \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"The accuracy for the Random Forest in the TRAIN set is {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"The accuracy for the Random Forest in the TEST set is {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111ee80",
   "metadata": {},
   "source": [
    "### Grid Search Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11d5effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_kap = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "grid1 = GridSearchCV(estimator=clf, param_grid=grid, scoring=coh_kap, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55aa5f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166666666666666"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1.fit(X_train, y_train)\n",
    "grid1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1852cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5,\n",
       " 'max_samples': 0.5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 4}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26265b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the Random Forest in the TRAIN set is 0.95\n",
      "The accuracy for the Random Forest in the TEST set is 0.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    319\n",
       "0    281\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[276,  24],\n",
       "       [  5, 295]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9033333333333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1161\n",
       "1     203\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1158,  162],\n",
       "       [   3,   41]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.294580057923045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(**grid1.best_params_, random_state =8)\n",
    "                          \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"The accuracy for the Random Forest in the TRAIN set is {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"The accuracy for the Random Forest in the TEST set is {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(y.value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3913b",
   "metadata": {},
   "source": [
    "## Feature Extraction-SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12509063",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5551a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.dependence_plot(' Borrowing dependency', shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a05b1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x684 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60332121",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da8614df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f2ac396",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cl = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fdb823d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73d7ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the XGB in the TRAIN set is 1.00\n",
      "The accuracy for the XGB in the TEST set is 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    300\n",
       "1    300\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[300,   0],\n",
       "       [  0, 300]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1192\n",
       "1     172\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1188,  132],\n",
       "       [   4,   40]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3362720403022669"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The accuracy for the XGB in the TRAIN set is {:.2f}\".format(xgb_cl.score(X_train, y_train)))\n",
    "print(\"The accuracy for the XGB in the TEST set is {:.2f}\".format(xgb_cl.score(X_test, y_test)))\n",
    "\n",
    "y_pred = pd.Series(xgb_cl.predict(X_train))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = pd.Series(xgb_cl.predict(X_test))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b27be1",
   "metadata": {},
   "source": [
    "Learning_rate: also called eta, it specifies how quickly the model fits the residual errors by using additional base learners.typical values: 0.01–0.2\n",
    "\n",
    "Gamma, reg_alpha, reg_lambda: these 3 parameters specify the values for 3 types of regularization done by XGBoost - minimum loss reduction to create a new split, L1 reg on leaf weights, L2 reg leaf weights respectively.Typical values for gamma: 0 - 0.5 but highly dependent on the data. Typical values for reg_alpha and reg_lambda: 0 - 1 is a good starting point but again, depends on the data.\n",
    "\n",
    "Max_depth - how deep the tree's decision nodes can go. Must be a positive integer. typical values: 1–10\n",
    "\n",
    "Subsample - fraction of the training set that can be used to train each tree. If this value is low, it may lead to underfitting or if it is too high, it may lead to overfitting. typical values: 0.5–0.9\n",
    "\n",
    "Colsample_bytree- fraction of the features that can be used to train each tree. A large value means almost all features can be used to build the decision tree. typical values: 0.5–0.9\n",
    "\n",
    "The above are the main hyperparameters people often tune. It is perfectly OK if you don’t understand them all completely (like me) but you can refer to this post which gives a thorough overview of how each of the above parameters works and how to tune them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b44ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.2],\n",
    "    \"gamma\": [0, 0.5, 1],\n",
    "    \"reg_lambda\": [0, 0.5, 1, 10],\n",
    "    \"scale_pos_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.5],\n",
    "    \"reg_alpha\": [0, 0.5, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1c051e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "# Init Grid Search\n",
    "grid_cv = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring=\"roc_auc\")\n",
    "\n",
    "# Fit\n",
    "_ = grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37b171da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9237333333333334"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f841bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'gamma': 1,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 7,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 0,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10cbc322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the XGB in the TRAIN set is 0.99\n",
      "The accuracy for the XGB in the TEST set is 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    303\n",
       "0    297\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[295,   5],\n",
       "       [  2, 298]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9766666666666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1192\n",
       "1     172\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1189,  131],\n",
       "       [   3,   41]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3460327455919394"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_cl = xgb.XGBClassifier(\n",
    "    **grid_cv.best_params_,\n",
    "    objective=\"binary:logistic\")\n",
    "_ = final_cl.fit(X_train, y_train)\n",
    "preds = final_cl.predict(X_test)\n",
    "\n",
    "print(\"The accuracy for the XGB in the TRAIN set is {:.2f}\".format(final_cl.score(X_train, y_train)))\n",
    "print(\"The accuracy for the XGB in the TEST set is {:.2f}\".format(final_cl.score(X_test, y_test)))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_train))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_test))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04db66",
   "metadata": {},
   "source": [
    "### Grid Search-Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70508a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_kap = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "grid2 = GridSearchCV(estimator=xgb_cl, param_grid=grid, scoring=coh_kap, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "044a26ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:05:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6333333333333334"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.fit(X_train, y_train)\n",
    "grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc35303b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5,\n",
       " 'max_samples': 0.8,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ba6ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"max_samples\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "The accuracy for the XGB in the TRAIN set is 1.00\n",
      "The accuracy for the XGB in the TEST set is 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    300\n",
       "1    300\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[300,   0],\n",
       "       [  0, 300]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1196\n",
       "1     168\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1191,  129],\n",
       "       [   5,   39]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3338678640153946"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_cl = xgb.XGBClassifier(\n",
    "    **grid2.best_params_,\n",
    "    objective=\"binary:logistic\")\n",
    "_ = final_cl.fit(X_train, y_train)\n",
    "preds = final_cl.predict(X_test)\n",
    "\n",
    "print(\"The accuracy for the XGB in the TRAIN set is {:.2f}\".format(final_cl.score(X_train, y_train)))\n",
    "print(\"The accuracy for the XGB in the TEST set is {:.2f}\".format(final_cl.score(X_test, y_test)))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_train))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_train, y_pred))\n",
    "display(cohen_kappa_score(y_train, y_pred))\n",
    "\n",
    "y_pred = pd.Series(final_cl.predict(X_test))\n",
    "display(pd.DataFrame(y_pred).value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))\n",
    "display(cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc1c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e025f688",
   "metadata": {},
   "source": [
    "## Feature extraction-SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0862ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(final_cl)\n",
    "shap_values = explainer.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470aa899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35438ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.dependence_plot(' Borrowing dependency', shap_values[0], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35107792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x684 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d19bf",
   "metadata": {},
   "source": [
    "# K-fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(shape=(4, 12))\n",
    "\n",
    "pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos CV-10 a todos los modelos para tner una mejor estimacion del accuracy y menor varianza\n",
    "nfolds = 10\n",
    "models = [knn, clf, xgb_cl]\n",
    "cv_scores = np.zeros((len(models), nfolds))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    scores = cross_val_score(model, X, y, scoring='roc', cv=nfolds)\n",
    "    cv_scores[i] = scores\n",
    "    \n",
    "cv_df = pd.DataFrame(cv_scores, index=[models])\n",
    "cv_df['mean_score'] = cv_df.mean(1)\n",
    "cv_df['std_score'] = cv_df.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea41b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
